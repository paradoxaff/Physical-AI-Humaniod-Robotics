"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[728],{7772:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"modules/module-03-isaac/isaac-navigation","title":"Chapter 3 - Isaac Navigation","description":"Learning Objectives","source":"@site/docs/modules/module-03-isaac/isaac-navigation.mdx","sourceDirName":"modules/module-03-isaac","slug":"/modules/module-03-isaac/isaac-navigation","permalink":"/docs/modules/module-03-isaac/isaac-navigation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-03-isaac/isaac-navigation.mdx","tags":[],"version":"current","frontMatter":{"title":"Chapter 3 - Isaac Navigation","sidebar_label":"Isaac Navigation"},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Manipulation","permalink":"/docs/modules/module-03-isaac/isaac-manipulation"},"next":{"title":"Isaac ROS","permalink":"/docs/modules/module-03-isaac/isaac-ros"}}');var t=a(4848),s=a(8453);const o={title:"Chapter 3 - Isaac Navigation",sidebar_label:"Isaac Navigation"},r="Chapter 3: Isaac Navigation",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Physical AI Concept",id:"physical-ai-concept",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Isaac Navigation Architecture",id:"isaac-navigation-architecture",level:3},{value:"Isaac Navigation Planning Pipeline",id:"isaac-navigation-planning-pipeline",level:3},{value:"Tools &amp; Software",id:"tools--software",level:2},{value:"Code / Configuration Examples",id:"code--configuration-examples",level:2},{value:"Isaac Navigation Core Node",id:"isaac-navigation-core-node",level:3},{value:"Isaac Visual SLAM Integration Node",id:"isaac-visual-slam-integration-node",level:3},{value:"Isaac Dynamic Obstacle Avoidance Node",id:"isaac-dynamic-obstacle-avoidance-node",level:3},{value:"Practical Lab / Simulation",id:"practical-lab--simulation",level:2},{value:"Lab Exercise 1: Isaac Navigation Installation and Setup",id:"lab-exercise-1-isaac-navigation-installation-and-setup",level:3},{value:"Lab Exercise 2: Global Path Planning",id:"lab-exercise-2-global-path-planning",level:3},{value:"Lab Exercise 3: Visual SLAM Integration",id:"lab-exercise-3-visual-slam-integration",level:3},{value:"Lab Exercise 4: Dynamic Obstacle Avoidance",id:"lab-exercise-4-dynamic-obstacle-avoidance",level:3},{value:"Lab Exercise 5: Performance Optimization",id:"lab-exercise-5-performance-optimization",level:3},{value:"Lab Exercise 6: Real-world Validation",id:"lab-exercise-6-real-world-validation",level:3},{value:"Real-World Mapping",id:"real-world-mapping",level:2},{value:"Industrial Applications",id:"industrial-applications",level:3},{value:"Research Applications",id:"research-applications",level:3},{value:"Key Success Factors",id:"key-success-factors",level:3},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"chapter-3-isaac-navigation",children:"Chapter 3: Isaac Navigation"})}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Implement GPU-accelerated navigation algorithms using Isaac Navigation"}),"\n",(0,t.jsx)(e.li,{children:"Integrate Isaac Navigation with perception systems for robust localization"}),"\n",(0,t.jsx)(e.li,{children:"Configure navigation parameters for different robot platforms and environments"}),"\n",(0,t.jsx)(e.li,{children:"Deploy navigation systems on NVIDIA hardware with optimized performance"}),"\n",(0,t.jsx)(e.li,{children:"Implement advanced navigation features like dynamic obstacle avoidance"}),"\n",(0,t.jsx)(e.li,{children:"Validate navigation performance in simulation and real-world scenarios"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"physical-ai-concept",children:"Physical AI Concept"}),"\n",(0,t.jsx)(e.p,{children:"Isaac Navigation represents NVIDIA's specialized navigation stack optimized for AI-powered mobile robots, leveraging GPU acceleration for real-time path planning, localization, and obstacle avoidance. For Physical AI systems, navigation is a fundamental capability that enables robots to move autonomously in physical spaces while avoiding obstacles and reaching desired destinations. Isaac Navigation provides the computational foundation for safe, efficient, and robust navigation in complex environments."}),"\n",(0,t.jsx)(e.p,{children:"Key aspects of Isaac Navigation in Physical AI:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"GPU-Accelerated Planning"}),": High-performance path planning algorithms"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Perception Integration"}),": Tight coupling with perception systems for robust localization"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Real-time Performance"}),": Optimized for deterministic, low-latency operation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dynamic Obstacle Handling"}),": Advanced algorithms for moving obstacle avoidance"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Multi-sensor Fusion"}),": Integration of various sensor modalities for navigation"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(e.h3,{id:"isaac-navigation-architecture",children:"Isaac Navigation Architecture"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"Isaac Navigation Architecture:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Application Layer                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502   High-Level    \u2502  \u2502   Behavior      \u2502  \u2502   Mission       \u2502    \u2502\n\u2502  \u2502   Commands      \u2502  \u2502   Control       \u2502  \u2502   Planning      \u2502    \u2502\n\u2502  \u2502  \u2022 Goal         \u2502  \u2502  \u2022 State        \u2502  \u2502  \u2022 Task         \u2502    \u2502\n\u2502  \u2502    Management   \u2502  \u2502    Machines     \u2502  \u2502    Sequencing   \u2502    \u2502\n\u2502  \u2502  \u2022 Route        \u2502  \u2502  \u2022 Recovery     \u2502  \u2502  \u2022 Resource     \u2502    \u2502\n\u2502  \u2502    Planning     \u2502  \u2502    Behaviors    \u2502  \u2502    Allocation   \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502              \u2502                    \u2502                    \u2502           \u2502\n\u2502              \u25bc                    \u25bc                    \u25bc           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                  Isaac Navigation Core                      \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502\n\u2502  \u2502  \u2502  Global     \u2502  \u2502  Local       \u2502  \u2502  Controller \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  Planner    \u2502  \u2502  Planner     \u2502  \u2502  Manager   \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 A*       \u2502  \u2502  \u2022 DWA       \u2502  \u2502  \u2022 Path     \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Dijkstra \u2502  \u2502  \u2022 TEB       \u2502  \u2502    Following \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 RRT      \u2502  \u2502  \u2022 MPC       \u2502  \u2502  \u2022 Velocity \u2502        \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    Control   \u2502        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                     \u2502                            \u2502\n\u2502                                     \u25bc                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              GPU Computing Layer                            \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502\n\u2502  \u2502  \u2502   CUDA      \u2502  \u2502   TensorRT  \u2502  \u2502   cuDNN     \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502   Core      \u2502  \u2502   Inference \u2502  \u2502   Deep      \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Parallel \u2502  \u2502  \u2022 Model    \u2502  \u2502    Learning \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Processing\u2502 \u2502    Optimization\u2502\u2502    Primitives\u2502       \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                     \u2502                            \u2502\n\u2502                                     \u25bc                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Sensor Integration                           \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502\n\u2502  \u2502  \u2502  Perception \u2502  \u2502  Localization\u2502  \u2502  Mapping    \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Object   \u2502  \u2502  \u2022 AMCL      \u2502  \u2502  \u2022 Occupancy \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Detection \u2502  \u2502  \u2022 Particle  \u2502  \u2502    Grid     \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Tracking \u2502  \u2502    Filter    \u2502  \u2502  \u2022 Costmap   \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Segmentation\u2502\u2502  \u2022 UKF      \u2502  \u2502  \u2022 3D Maps  \u2502        \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(e.h3,{id:"isaac-navigation-planning-pipeline",children:"Isaac Navigation Planning Pipeline"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"Navigation Planning Pipeline:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Goal Input    \u2502\u2500\u2500\u2500\u25b6\u2502  Global         \u2502\u2500\u2500\u2500\u25b6\u2502  Local          \u2502\n\u2502   \u2022 Position    \u2502    \u2502  Path Planning  \u2502    \u2502  Path Planning  \u2502\n\u2502   \u2022 Orientation \u2502    \u2502  \u2022 A* Algorithm \u2502    \u2502  \u2022 Trajectory   \u2502\n\u2502   \u2022 Constraints \u2502    \u2502  \u2022 Cost Maps   \u2502    \u2502    Optimization \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502  \u2022 Obstacles   \u2502    \u2502  \u2022 Kinodynamic  \u2502\n         \u2502               \u2502  \u2022 Recovery    \u2502    \u2502    Constraints \u2502\n         \u25bc               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502                       \u2502\n\u2502   Map Input     \u2502              \u25bc                       \u25bc\n\u2502   \u2022 Static      \u2502\u2500\u2500\u2500\u25b6\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Map         \u2502    \u2502  Path          \u2502\u2500\u2500\u2500\u25b6\u2502  Controller     \u2502\n\u2502   \u2022 Costmap     \u2502    \u2502  Smoothing &   \u2502    \u2502  \u2022 Velocity     \u2502\n\u2502   \u2022 Semantic    \u2502    \u2502  Optimization  \u2502    \u2502    Commands     \u2502\n\u2502     Map         \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502  \u2022 Safety       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502    Overrides    \u2502\n         \u2502                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25bc                                               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Sensor        \u2502\u2500\u2500\u2500\u25b6\u2502  Obstacle       \u2502\u2500\u2500\u2500\u25b6\u2502  Robot          \u2502\n\u2502   Fusion        \u2502    \u2502  Avoidance      \u2502    \u2502  Actuation      \u2502\n\u2502   \u2022 LIDAR       \u2502    \u2502  \u2022 Dynamic      \u2502    \u2502  \u2022 Wheel        \u2502\n\u2502   \u2022 Camera      \u2502    \u2502    Obstacles    \u2502    \u2502    Commands     \u2502\n\u2502   \u2022 IMU         \u2502    \u2502  \u2022 Recovery     \u2502    \u2502  \u2022 Motor        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502    Behaviors    \u2502    \u2502    Control      \u2502\n                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(e.h2,{id:"tools--software",children:"Tools & Software"}),"\n",(0,t.jsx)(e.p,{children:"This chapter uses:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Isaac ROS Navigation"})," - GPU-accelerated navigation stack"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Isaac ROS Visual SLAM"})," - Visual simultaneous localization and mapping"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Isaac ROS Occupancy Grids"})," - GPU-accelerated mapping"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"CUDA & TensorRT"})," - GPU computing and AI acceleration"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"NVIDIA Jetson"})," - Edge AI computing platforms for navigation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"ROS 2 Navigation2"})," - Standard navigation framework integration"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Cartographer"})," - SLAM algorithm (optional integration)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"AMCL"})," - Adaptive Monte Carlo Localization"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"code--configuration-examples",children:"Code / Configuration Examples"}),"\n",(0,t.jsx)(e.h3,{id:"isaac-navigation-core-node",children:"Isaac Navigation Core Node"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Twist, Point\nfrom nav_msgs.msg import Odometry, Path, OccupancyGrid\nfrom sensor_msgs.msg import LaserScan, PointCloud2\nfrom visualization_msgs.msg import Marker, MarkerArray\nfrom std_msgs.msg import Header\nimport numpy as np\nimport math\nfrom scipy.spatial import distance\nfrom tf2_ros import TransformException\nfrom tf2_ros.buffer import Buffer\nfrom tf2_ros.transform_listener import TransformListener\nimport time\n\nclass IsaacNavigationNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_navigation_node\')\n\n        # TF2 setup for coordinate transformations\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Publishers and subscribers\n        self.odom_sub = self.create_subscription(\n            Odometry, \'/odom\', self.odom_callback, 10\n        )\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10\n        )\n        self.goal_sub = self.create_subscription(\n            PoseStamped, \'/move_base_simple/goal\', self.goal_callback, 10\n        )\n        self.map_sub = self.create_subscription(\n            OccupancyGrid, \'/map\', self.map_callback, 10\n        )\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.global_path_pub = self.create_publisher(Path, \'/navigation/global_plan\', 10)\n        self.local_path_pub = self.create_publisher(Path, \'/navigation/local_plan\', 10)\n        self.velocity_pub = self.create_publisher(Twist, \'/navigation/velocity\', 10)\n\n        # Navigation state\n        self.current_pose = None\n        self.current_twist = None\n        self.goal_pose = None\n        self.scan_data = None\n        self.map_data = None\n        self.navigation_active = False\n        self.global_path = []\n        self.local_path = []\n\n        # Navigation parameters\n        self.linear_vel_max = 0.5\n        self.angular_vel_max = 0.8\n        self.safe_distance = 0.5\n        self.arrival_threshold = 0.3\n        self.path_resolution = 0.1  # meters\n        self.control_frequency = 10.0  # Hz\n        self.planning_frequency = 2.0  # Hz\n\n        # Robot parameters\n        self.robot_radius = 0.3  # meters\n        self.max_acceleration = 0.5  # m/s^2\n        self.max_deceleration = 1.0  # m/s^2\n\n        # Timers for navigation control\n        self.control_timer = self.create_timer(1.0/self.control_frequency, self.control_loop)\n        self.planning_timer = self.create_timer(1.0/self.planning_frequency, self.planning_loop)\n\n        # Path planning parameters\n        self.path_smoothing_factor = 0.3\n        self.inflation_radius = 0.5  # meters for obstacle inflation\n\n        self.get_logger().info(\'Isaac navigation node initialized\')\n\n    def odom_callback(self, msg):\n        """Update current robot pose and twist from odometry"""\n        self.current_pose = msg.pose.pose\n        self.current_twist = msg.twist.twist\n\n    def scan_callback(self, msg):\n        """Update laser scan data for obstacle detection"""\n        self.scan_data = msg\n\n    def map_callback(self, msg):\n        """Update map data for global planning"""\n        self.map_data = msg\n\n    def goal_callback(self, msg):\n        """Set new navigation goal and start navigation"""\n        self.goal_pose = msg.pose\n        self.navigation_active = True\n        self.get_logger().info(f\'New goal set: ({msg.pose.position.x:.2f}, {msg.pose.position.y:.2f})\')\n\n        # Plan initial path\n        self.plan_global_path()\n\n    def plan_global_path(self):\n        """Plan global path from current pose to goal"""\n        if not self.current_pose or not self.goal_pose or not self.map_data:\n            return\n\n        # Extract current and goal positions\n        start_pos = (self.current_pose.position.x, self.current_pose.position.y)\n        goal_pos = (self.goal_pose.position.x, self.goal_pose.position.y)\n\n        # Simple path planning using A* on the occupancy grid\n        # In a real implementation, this would use a more sophisticated planner\n        path = self.a_star_pathfinding(start_pos, goal_pos)\n\n        if path:\n            # Smooth the path\n            smoothed_path = self.smooth_path(path)\n            self.global_path = smoothed_path\n            self.publish_global_path()\n            self.get_logger().info(f\'Global path planned with {len(path)} waypoints\')\n        else:\n            self.get_logger().warn(\'Could not find a path to the goal\')\n            self.navigation_active = False\n\n    def a_star_pathfinding(self, start, goal):\n        """Simple A* pathfinding on occupancy grid (simplified implementation)"""\n        if not self.map_data:\n            return None\n\n        # Convert world coordinates to grid coordinates\n        def world_to_grid(world_x, world_y):\n            grid_x = int((world_x - self.map_data.info.origin.position.x) / self.map_data.info.resolution)\n            grid_y = int((world_y - self.map_data.info.origin.position.y) / self.map_data.info.resolution)\n            return grid_x, grid_y\n\n        # Convert grid coordinates to world coordinates\n        def grid_to_world(grid_x, grid_y):\n            world_x = grid_x * self.map_data.info.resolution + self.map_data.info.origin.position.x\n            world_y = grid_y * self.map_data.info.resolution + self.map_data.info.origin.position.y\n            return world_x, world_y\n\n        start_grid = world_to_grid(start[0], start[1])\n        goal_grid = world_to_grid(goal[0], goal[1])\n\n        # Check if start and goal are within map bounds\n        if (start_grid[0] < 0 or start_grid[0] >= self.map_data.info.width or\n            start_grid[1] < 0 or start_grid[1] >= self.map_data.info.height or\n            goal_grid[0] < 0 or goal_grid[0] >= self.map_data.info.width or\n            goal_grid[1] < 0 or goal_grid[1] >= self.map_data.info.height):\n            return None\n\n        # For this simplified example, we\'ll return a straight line\n        # A real A* implementation would be more complex\n        path = []\n        steps = max(abs(goal_grid[0] - start_grid[0]), abs(goal_grid[1] - start_grid[1]))\n        if steps == 0:\n            steps = 1\n\n        for i in range(steps + 1):\n            t = i / steps\n            x = start_grid[0] + int(t * (goal_grid[0] - start_grid[0]))\n            y = start_grid[1] + int(t * (goal_grid[1] - start_grid[1]))\n            world_x, world_y = grid_to_world(x, y)\n            path.append((world_x, world_y))\n\n        return path\n\n    def smooth_path(self, path):\n        """Smooth the path using a simple algorithm"""\n        if len(path) < 3:\n            return path\n\n        smoothed_path = [path[0]]  # Start with first point\n\n        for i in range(1, len(path) - 1):\n            # Calculate smoothing based on neighboring points\n            prev_point = np.array(path[i-1])\n            curr_point = np.array(path[i])\n            next_point = np.array(path[i+1])\n\n            # Apply smoothing factor\n            smoothed_point = ((1 - self.path_smoothing_factor) * curr_point +\n                             self.path_smoothing_factor * 0.5 * (prev_point + next_point))\n            smoothed_path.append(tuple(smoothed_point))\n\n        smoothed_path.append(path[-1])  # End with last point\n        return smoothed_path\n\n    def publish_global_path(self):\n        """Publish the global path"""\n        if not self.global_path:\n            return\n\n        path_msg = Path()\n        path_msg.header.stamp = self.get_clock().now().to_msg()\n        path_msg.header.frame_id = \'map\'\n\n        for point in self.global_path:\n            pose_stamped = PoseStamped()\n            pose_stamped.header.frame_id = \'map\'\n            pose_stamped.pose.position.x = point[0]\n            pose_stamped.pose.position.y = point[1]\n            pose_stamped.pose.position.z = 0.0\n            path_msg.poses.append(pose_stamped)\n\n        self.global_path_pub.publish(path_msg)\n\n    def planning_loop(self):\n        """Periodic path planning and replanning"""\n        if not self.navigation_active:\n            return\n\n        # Check if we need to replan (e.g., if path is invalid or blocked)\n        if self.is_path_blocked():\n            self.get_logger().info(\'Replanning path due to obstacles\')\n            self.plan_global_path()\n\n    def is_path_blocked(self):\n        """Check if the current path is blocked by obstacles"""\n        if not self.scan_data or not self.global_path:\n            return False\n\n        # Check laser scan for obstacles along the path\n        # This is a simplified check - in reality, this would be more sophisticated\n        front_scan = self.scan_data.ranges[len(self.scan_data.ranges)//2 - 30 : len(self.scan_data.ranges)//2 + 30]\n        valid_ranges = [r for r in front_scan if not (math.isnan(r) or math.isinf(r))]\n\n        if valid_ranges:\n            min_distance = min(valid_ranges)\n            return min_distance < self.safe_distance\n\n        return False\n\n    def control_loop(self):\n        """Main navigation control loop"""\n        if not self.navigation_active or not self.current_pose or not self.goal_pose:\n            if self.navigation_active:\n                self.stop_robot()\n            return\n\n        # Check for obstacles\n        if self.scan_data and self.is_path_blocked():\n            self.stop_robot()\n            self.get_logger().warn(\'Path blocked by obstacle, stopping\')\n            return\n\n        # Calculate direction to next waypoint\n        next_waypoint = self.get_next_waypoint()\n        if not next_waypoint:\n            self.stop_robot()\n            self.navigation_active = False\n            self.get_logger().info(\'Reached goal\')\n            return\n\n        # Calculate control commands\n        cmd_vel = self.calculate_velocity_command(next_waypoint)\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.velocity_pub.publish(cmd_vel)\n\n    def get_next_waypoint(self):\n        """Get the next waypoint along the global path"""\n        if not self.global_path or not self.current_pose:\n            return None\n\n        current_pos = (self.current_pose.position.x, self.current_pose.position.y)\n\n        # Find the closest point on the path\n        min_dist = float(\'inf\')\n        closest_idx = 0\n\n        for i, point in enumerate(self.global_path):\n            dist = math.sqrt((point[0] - current_pos[0])**2 + (point[1] - current_pos[1])**2)\n            if dist < min_dist:\n                min_dist = dist\n                closest_idx = i\n\n        # Return the next point after the closest one\n        next_idx = min(closest_idx + 1, len(self.global_path) - 1)\n        return self.global_path[next_idx]\n\n    def calculate_velocity_command(self, target_waypoint):\n        """Calculate velocity command to reach target waypoint"""\n        cmd = Twist()\n\n        if not self.current_pose:\n            return cmd\n\n        # Calculate direction to target\n        current_pos = (self.current_pose.position.x, self.current_pose.position.y)\n        dx = target_waypoint[0] - current_pos[0]\n        dy = target_waypoint[1] - current_pos[1]\n        distance_to_target = math.sqrt(dx*dx + dy*dy)\n\n        # Check if reached target\n        if distance_to_target < self.arrival_threshold:\n            return cmd  # Zero velocity\n\n        # Calculate heading to target\n        target_heading = math.atan2(dy, dx)\n\n        # Get current orientation\n        current_yaw = self.get_yaw_from_quaternion(self.current_pose.orientation)\n\n        # Calculate angular error\n        angle_error = self.normalize_angle(target_heading - current_yaw)\n\n        # Proportional controller for angular velocity\n        cmd.angular.z = max(-self.angular_vel_max, min(self.angular_vel_max, angle_error * 2.0))\n\n        # Move forward if roughly aligned with target\n        if abs(angle_error) < 0.5:  # 0.5 radians = ~28 degrees\n            # Scale linear velocity based on distance to target and angle error\n            linear_scale = max(0.1, min(1.0, distance_to_target / 2.0))\n            cmd.linear.x = self.linear_vel_max * linear_scale\n\n        # Apply safety limits based on obstacle detection\n        if self.scan_data:\n            cmd = self.apply_safety_limits(cmd)\n\n        return cmd\n\n    def apply_safety_limits(self, cmd):\n        """Apply safety limits based on obstacle detection"""\n        # Check for obstacles in front\n        front_scan = self.scan_data.ranges[len(self.scan_data.ranges)//2 - 15 : len(self.scan_data.ranges)//2 + 15]\n        valid_ranges = [r for r in front_scan if not (math.isnan(r) or math.isinf(r))]\n\n        if valid_ranges:\n            min_distance = min(valid_ranges)\n            if min_distance < self.safe_distance:\n                # Reduce velocity based on proximity to obstacles\n                speed_factor = min_distance / self.safe_distance\n                cmd.linear.x *= speed_factor\n                cmd.angular.z *= speed_factor\n\n        return cmd\n\n    def stop_robot(self):\n        """Stop robot movement"""\n        cmd = Twist()\n        self.cmd_vel_pub.publish(cmd)\n\n    def get_yaw_from_quaternion(self, quaternion):\n        """Extract yaw angle from quaternion"""\n        siny_cosp = 2 * (quaternion.w * quaternion.z + quaternion.x * quaternion.y)\n        cosy_cosp = 1 - 2 * (quaternion.y * quaternion.y + quaternion.z * quaternion.z)\n        return math.atan2(siny_cosp, cosy_cosp)\n\n    def normalize_angle(self, angle):\n        """Normalize angle to [-pi, pi] range"""\n        while angle > math.pi:\n            angle -= 2.0 * math.pi\n        while angle < -math.pi:\n            angle += 2.0 * math.pi\n        return angle\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacNavigationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"isaac-visual-slam-integration-node",children:"Isaac Visual SLAM Integration Node"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, Imu\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import Pose, Point, Quaternion\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nimport math\nfrom scipy.spatial.transform import Rotation as R\n\nclass IsaacVisualSLAMNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_visual_slam_node\')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.left_image_sub = self.create_subscription(\n            Image, \'/stereo/left/image_rect_color\', self.left_image_callback, 10\n        )\n        self.right_image_sub = self.create_subscription(\n            Image, \'/stereo/right/image_rect_color\', self.right_image_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, \'/imu/data\', self.imu_callback, 10\n        )\n        self.odom_pub = self.create_publisher(Odometry, \'/visual_slam/odometry\', 10)\n\n        # Visual SLAM parameters\n        self.feature_detector = cv2.ORB_create(nfeatures=1000)\n        self.descriptor_matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n\n        # Camera parameters (will be set from camera_info)\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n\n        # SLAM state\n        self.previous_frame = None\n        self.previous_keypoints = None\n        self.previous_descriptors = None\n        self.current_position = np.array([0.0, 0.0, 0.0])\n        self.current_orientation = np.array([0.0, 0.0, 0.0, 1.0])  # quaternion\n        self.frame_count = 0\n\n        # IMU integration\n        self.imu_data = None\n        self.imu_orientation = None\n\n        # Timer for processing\n        self.process_timer = self.create_timer(0.1, self.process_frame)  # 10 Hz\n\n        self.get_logger().info(\'Isaac visual SLAM node initialized\')\n\n    def left_image_callback(self, msg):\n        """Process left camera image for visual SLAM"""\n        try:\n            self.current_frame = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\n            self.frame_header = msg.header\n        except Exception as e:\n            self.get_logger().error(f\'Error processing left image: {e}\')\n\n    def right_image_callback(self, msg):\n        """Process right camera image (for stereo depth if needed)"""\n        try:\n            self.right_frame = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\n        except Exception as e:\n            self.get_logger().error(f\'Error processing right image: {e}\')\n\n    def imu_callback(self, msg):\n        """Process IMU data for orientation estimation"""\n        # Extract orientation from IMU (if available)\n        self.imu_orientation = np.array([\n            msg.orientation.x,\n            msg.orientation.y,\n            msg.orientation.z,\n            msg.orientation.w\n        ])\n\n        # Store angular velocity and linear acceleration for integration\n        self.imu_data = {\n            \'angular_velocity\': np.array([\n                msg.angular_velocity.x,\n                msg.angular_velocity.y,\n                msg.angular_velocity.z\n            ]),\n            \'linear_acceleration\': np.array([\n                msg.linear_acceleration.x,\n                msg.linear_acceleration.y,\n                msg.linear_acceleration.z\n            ])\n        }\n\n    def process_frame(self):\n        """Process current frame for visual SLAM"""\n        if self.current_frame is None or self.previous_frame is None:\n            # Store first frame as reference\n            if self.current_frame is not None:\n                self.previous_frame = self.current_frame.copy()\n                self.previous_keypoints, self.previous_descriptors = self.extract_features(self.current_frame)\n            return\n\n        # Extract features from current frame\n        current_keypoints, current_descriptors = self.extract_features(self.current_frame)\n\n        if current_descriptors is None or self.previous_descriptors is None:\n            return\n\n        # Match features between frames\n        matches = self.match_features(self.previous_descriptors, current_descriptors)\n\n        if len(matches) >= 10:  # Need minimum matches for reliable estimation\n            # Estimate motion between frames\n            motion = self.estimate_motion(matches, self.previous_keypoints, current_keypoints)\n\n            if motion is not None:\n                # Update pose based on estimated motion\n                self.update_pose(motion)\n\n                # Publish odometry\n                self.publish_odometry()\n\n        # Update previous frame for next iteration\n        self.previous_frame = self.current_frame.copy()\n        self.previous_keypoints = current_keypoints\n        self.previous_descriptors = current_descriptors\n\n    def extract_features(self, frame):\n        """Extract ORB features from frame"""\n        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        keypoints = self.feature_detector.detect(gray, None)\n        keypoints, descriptors = self.feature_detector.compute(gray, keypoints)\n        return keypoints, descriptors\n\n    def match_features(self, desc1, desc2):\n        """Match features between two frames"""\n        if desc1 is None or desc2 is None:\n            return []\n\n        matches = self.descriptor_matcher.knnMatch(desc1, desc2, k=2)\n\n        # Apply Lowe\'s ratio test\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance < 0.7 * n.distance:\n                    good_matches.append(m)\n\n        return good_matches\n\n    def estimate_motion(self, matches, kp1, kp2):\n        """Estimate motion between two frames using matched keypoints"""\n        if len(matches) < 10:\n            return None\n\n        # Extract matched keypoints\n        src_points = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n        dst_points = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n        # Compute homography using RANSAC\n        homography, mask = cv2.findHomography(src_points, dst_points, cv2.RANSAC, 5.0)\n\n        if homography is None:\n            return None\n\n        # Extract translation and rotation from homography\n        # This is a simplified approach - real visual SLAM uses more sophisticated methods\n        h = homography\n        translation = np.array([h[0, 2], h[1, 2], 0.0])  # x, y translation (z assumed to be 0)\n\n        # Extract rotation (simplified)\n        rotation_matrix = np.array([[h[0, 0], h[0, 1], 0],\n                                    [h[1, 0], h[1, 1], 0],\n                                    [0, 0, 1]])\n\n        # Convert to axis-angle and then to quaternion\n        r = R.from_matrix(rotation_matrix[:3, :3])\n        rotation_quat = r.as_quat()\n\n        return {\n            \'translation\': translation,\n            \'rotation\': rotation_quat\n        }\n\n    def update_pose(self, motion):\n        """Update current pose based on estimated motion"""\n        # Apply translation to position\n        self.current_position += motion[\'translation\'] * 0.1  # Scale factor for realistic movement\n\n        # Apply rotation to orientation\n        motion_quat = motion[\'rotation\']\n        current_rot = R.from_quat(self.current_orientation)\n        motion_rot = R.from_quat(motion_quat)\n        new_rot = current_rot * motion_rot\n        self.current_orientation = new_rot.as_quat()\n\n        # Optionally integrate with IMU data for better orientation estimate\n        if self.imu_orientation is not None:\n            # Simple complementary filter (in practice, use more sophisticated fusion)\n            alpha = 0.8  # Weight for visual estimate\n            visual_quat = self.current_orientation\n            imu_quat = self.imu_orientation\n\n            # This is a simplified fusion - real implementation would be more complex\n            self.current_orientation = visual_quat  # Use visual estimate for now\n\n    def publish_odometry(self):\n        """Publish odometry message with estimated pose"""\n        odom_msg = Odometry()\n        odom_msg.header = self.frame_header\n        odom_msg.child_frame_id = \'base_link\'\n\n        # Set position\n        odom_msg.pose.pose.position.x = float(self.current_position[0])\n        odom_msg.pose.pose.position.y = float(self.current_position[1])\n        odom_msg.pose.pose.position.z = float(self.current_position[2])\n\n        # Set orientation\n        odom_msg.pose.pose.orientation.x = float(self.current_orientation[0])\n        odom_msg.pose.pose.orientation.y = float(self.current_orientation[1])\n        odom_msg.pose.pose.orientation.z = float(self.current_orientation[2])\n        odom_msg.pose.pose.orientation.w = float(self.current_orientation[3])\n\n        # Set zero velocities (would come from differentiation in real system)\n        odom_msg.twist.twist.linear.x = 0.0\n        odom_msg.twist.twist.linear.y = 0.0\n        odom_msg.twist.twist.linear.z = 0.0\n        odom_msg.twist.twist.angular.x = 0.0\n        odom_msg.twist.twist.angular.y = 0.0\n        odom_msg.twist.twist.angular.z = 0.0\n\n        self.odom_pub.publish(odom_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacVisualSLAMNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"isaac-dynamic-obstacle-avoidance-node",children:"Isaac Dynamic Obstacle Avoidance Node"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, PointCloud2\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom visualization_msgs.msg import Marker, MarkerArray\nfrom std_msgs.msg import ColorRGBA\nimport numpy as np\nimport math\nfrom scipy.spatial.distance import cdist\n\nclass IsaacObstacleAvoidanceNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_obstacle_avoidance_node\')\n\n        # Publishers and subscribers\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10\n        )\n        self.velocity_sub = self.create_subscription(\n            Twist, \'/cmd_vel\', self.velocity_callback, 10\n        )\n        self.avoidance_pub = self.create_publisher(Twist, \'/avoidance/cmd_vel\', 10)\n        self.debug_pub = self.create_publisher(MarkerArray, \'/avoidance/debug\', 10)\n\n        # Obstacle avoidance parameters\n        self.safe_distance = 0.8  # meters\n        self.too_close_distance = 0.4  # meters\n        self.max_angular_velocity = 1.0  # rad/s\n        self.max_linear_velocity = 0.5  # m/s\n        self.linear_slowdown_distance = 1.5  # meters\n        self.angular_speedup_factor = 2.0\n\n        # Robot state\n        self.scan_data = None\n        self.desired_velocity = Twist()\n        self.current_velocity = Twist()\n\n        # Dynamic obstacle detection parameters\n        self.obstacle_threshold = 0.3  # meters for obstacle detection\n        self.velocity_threshold = 0.05  # m/s for static vs dynamic\n        self.min_obstacle_points = 5  # minimum points to consider as obstacle\n\n        # Timer for obstacle avoidance\n        self.avoidance_timer = self.create_timer(0.05, self.avoidance_control)  # 20 Hz\n\n        self.get_logger().info(\'Isaac obstacle avoidance node initialized\')\n\n    def scan_callback(self, msg):\n        """Process laser scan data for obstacle detection"""\n        self.scan_data = msg\n\n    def velocity_callback(self, msg):\n        """Update desired velocity from navigation stack"""\n        self.desired_velocity = msg\n\n    def avoidance_control(self):\n        """Main obstacle avoidance control loop"""\n        if not self.scan_data:\n            return\n\n        # Detect obstacles and classify as static or dynamic\n        obstacles = self.detect_obstacles()\n\n        # Determine if we need to avoid obstacles\n        avoidance_needed = self.check_obstacle_proximity(obstacles)\n\n        if avoidance_needed:\n            # Calculate avoidance commands\n            avoidance_cmd = self.calculate_avoidance_commands(obstacles)\n            self.avoidance_pub.publish(avoidance_cmd)\n        else:\n            # Pass through desired velocity if no obstacles\n            self.avoidance_pub.publish(self.desired_velocity)\n\n        # Publish debug visualization\n        self.publish_debug_markers(obstacles)\n\n    def detect_obstacles(self):\n        """Detect obstacles from laser scan data"""\n        if not self.scan_data:\n            return []\n\n        obstacles = []\n        ranges = np.array(self.scan_data.ranges)\n        angles = np.linspace(\n            self.scan_data.angle_min,\n            self.scan_data.angle_max,\n            len(ranges)\n        )\n\n        # Convert to Cartesian coordinates\n        valid_indices = np.isfinite(ranges)\n        x_coords = ranges[valid_indices] * np.cos(angles[valid_indices])\n        y_coords = ranges[valid_indices] * np.sin(angles[valid_indices])\n\n        points = np.column_stack((x_coords, y_coords))\n\n        if len(points) == 0:\n            return []\n\n        # Simple clustering to group points into obstacles\n        obstacle_clusters = self.cluster_points(points)\n\n        for cluster in obstacle_clusters:\n            if len(cluster) >= self.min_obstacle_points:\n                # Calculate cluster center and bounding box\n                center = np.mean(cluster, axis=0)\n                min_point = np.min(cluster, axis=0)\n                max_point = np.max(cluster, axis=0)\n                size = max_point - min_point\n\n                obstacle = {\n                    \'center\': center,\n                    \'points\': cluster,\n                    \'size\': size,\n                    \'is_dynamic\': self.is_dynamic_obstacle(cluster)\n                }\n                obstacles.append(obstacle)\n\n        return obstacles\n\n    def cluster_points(self, points):\n        """Simple clustering of points using distance threshold"""\n        if len(points) == 0:\n            return []\n\n        clusters = []\n        used = np.zeros(len(points), dtype=bool)\n\n        for i, point in enumerate(points):\n            if used[i]:\n                continue\n\n            cluster = [point]\n            used[i] = True\n\n            # Find nearby points\n            for j, other_point in enumerate(points):\n                if used[j]:\n                    continue\n\n                dist = np.linalg.norm(point - other_point)\n                if dist < self.obstacle_threshold:\n                    cluster.append(other_point)\n                    used[j] = True\n\n            clusters.append(np.array(cluster))\n\n        return clusters\n\n    def is_dynamic_obstacle(self, points):\n        """Check if obstacle is dynamic based on previous positions (simplified)"""\n        # In a real implementation, this would track obstacles over time\n        # to determine if they are moving independently\n        # For this example, we\'ll assume all obstacles are static\n        return False\n\n    def check_obstacle_proximity(self, obstacles):\n        """Check if any obstacles are in the robot\'s path"""\n        if not self.scan_data:\n            return False\n\n        # Check if there are obstacles in front of the robot\n        front_scan_start = len(self.scan_data.ranges) // 2 - 30\n        front_scan_end = len(self.scan_data.ranges) // 2 + 30\n        front_ranges = self.scan_data.ranges[front_scan_start:front_scan_end]\n\n        valid_ranges = [r for r in front_ranges if not (math.isnan(r) or math.isinf(r))]\n\n        if valid_ranges:\n            min_distance = min(valid_ranges)\n            return min_distance < self.safe_distance\n\n        return False\n\n    def calculate_avoidance_commands(self, obstacles):\n        """Calculate avoidance commands based on obstacle positions"""\n        cmd = Twist()\n\n        if not self.scan_data:\n            return cmd\n\n        # Get desired velocity as baseline\n        cmd.linear.x = self.desired_velocity.linear.x\n        cmd.angular.z = self.desired_velocity.angular.z\n\n        # Find the closest obstacle in front\n        front_scan_start = len(self.scan_data.ranges) // 2 - 30\n        front_scan_end = len(self.scan_data.ranges) // 2 + 30\n        front_ranges = self.scan_data.ranges[front_scan_start:front_scan_end]\n        front_angles = np.linspace(\n            self.scan_data.angle_min + front_scan_start * self.scan_data.angle_increment,\n            self.scan_data.angle_min + front_scan_end * self.scan_data.angle_increment,\n            len(front_ranges)\n        )\n\n        valid_indices = [i for i, r in enumerate(front_ranges) if not (math.isnan(r) or math.isinf(r))]\n        if not valid_indices:\n            return cmd\n\n        closest_idx = min(valid_indices, key=lambda i: front_ranges[i])\n        closest_distance = front_ranges[closest_idx]\n        closest_angle = front_angles[closest_idx]\n\n        # Adjust velocity based on obstacle proximity\n        if closest_distance < self.too_close_distance:\n            # Stop if too close\n            cmd.linear.x = 0.0\n            # Turn away from obstacle\n            cmd.angular.z = math.copysign(self.max_angular_velocity, -closest_angle)\n        elif closest_distance < self.linear_slowdown_distance:\n            # Slow down and turn slightly\n            slowdown_factor = closest_distance / self.linear_slowdown_distance\n            cmd.linear.x *= slowdown_factor\n            cmd.angular.z += math.copysign(0.3, -closest_angle)\n\n        # Limit velocities\n        cmd.linear.x = max(-self.max_linear_velocity, min(self.max_linear_velocity, cmd.linear.x))\n        cmd.angular.z = max(-self.max_angular_velocity, min(self.max_angular_velocity, cmd.angular.z))\n\n        return cmd\n\n    def publish_debug_markers(self, obstacles):\n        """Publish debug markers for visualization"""\n        marker_array = MarkerArray()\n\n        # Clear old markers\n        clear_marker = Marker()\n        clear_marker.header.frame_id = \'base_link\'\n        clear_marker.header.stamp = self.get_clock().now().to_msg()\n        clear_marker.ns = \'obstacles\'\n        clear_marker.id = 0\n        clear_marker.action = Marker.DELETEALL\n        marker_array.markers.append(clear_marker)\n\n        # Add obstacle markers\n        for i, obstacle in enumerate(obstacles):\n            marker = Marker()\n            marker.header.frame_id = \'base_link\'\n            marker.header.stamp = self.get_clock().now().to_msg()\n            marker.ns = \'obstacles\'\n            marker.id = i + 1\n            marker.type = Marker.SPHERE\n            marker.action = Marker.ADD\n\n            marker.pose.position.x = float(obstacle[\'center\'][0])\n            marker.pose.position.y = float(obstacle[\'center\'][1])\n            marker.pose.position.z = 0.0\n\n            marker.pose.orientation.w = 1.0\n\n            # Size based on obstacle size\n            avg_size = np.mean(obstacle[\'size\'])\n            marker.scale.x = max(0.2, avg_size)\n            marker.scale.y = max(0.2, avg_size)\n            marker.scale.z = 0.2\n\n            # Color based on dynamic/static\n            if obstacle[\'is_dynamic\']:\n                marker.color = ColorRGBA(r=1.0, g=0.0, b=0.0, a=0.8)  # Red for dynamic\n            else:\n                marker.color = ColorRGBA(r=1.0, g=1.0, b=0.0, a=0.8)  # Yellow for static\n\n            marker_array.markers.append(marker)\n\n        self.debug_pub.publish(marker_array)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacObstacleAvoidanceNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"practical-lab--simulation",children:"Practical Lab / Simulation"}),"\n",(0,t.jsx)(e.h3,{id:"lab-exercise-1-isaac-navigation-installation-and-setup",children:"Lab Exercise 1: Isaac Navigation Installation and Setup"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Install Isaac ROS Navigation packages"}),"\n",(0,t.jsx)(e.li,{children:"Set up navigation dependencies (TF2, costmap_2d, etc.)"}),"\n",(0,t.jsx)(e.li,{children:"Configure navigation parameters for your robot"}),"\n",(0,t.jsx)(e.li,{children:"Test basic navigation with simple goals"}),"\n",(0,t.jsx)(e.li,{children:"Verify proper integration with perception systems"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-exercise-2-global-path-planning",children:"Lab Exercise 2: Global Path Planning"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement the Isaac navigation core node"}),"\n",(0,t.jsx)(e.li,{children:"Test A* pathfinding with different map configurations"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate path planning performance and quality"}),"\n",(0,t.jsx)(e.li,{children:"Tune path smoothing parameters for optimal results"}),"\n",(0,t.jsx)(e.li,{children:"Test navigation in various environments"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-exercise-3-visual-slam-integration",children:"Lab Exercise 3: Visual SLAM Integration"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Set up stereo camera system for visual SLAM"}),"\n",(0,t.jsx)(e.li,{children:"Implement the Isaac Visual SLAM node"}),"\n",(0,t.jsx)(e.li,{children:"Test pose estimation accuracy"}),"\n",(0,t.jsx)(e.li,{children:"Integrate with IMU data for improved orientation"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate SLAM performance in different lighting conditions"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-exercise-4-dynamic-obstacle-avoidance",children:"Lab Exercise 4: Dynamic Obstacle Avoidance"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement the Isaac obstacle avoidance node"}),"\n",(0,t.jsx)(e.li,{children:"Test with static and dynamic obstacles"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate avoidance behavior and safety"}),"\n",(0,t.jsx)(e.li,{children:"Tune parameters for different robot speeds"}),"\n",(0,t.jsx)(e.li,{children:"Test integration with global navigation planner"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-exercise-5-performance-optimization",children:"Lab Exercise 5: Performance Optimization"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Profile navigation nodes for computational bottlenecks"}),"\n",(0,t.jsx)(e.li,{children:"Optimize algorithms for real-time performance"}),"\n",(0,t.jsx)(e.li,{children:"Test navigation on different NVIDIA hardware platforms"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate power consumption vs. performance trade-offs"}),"\n",(0,t.jsx)(e.li,{children:"Document optimal configurations for different scenarios"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-exercise-6-real-world-validation",children:"Lab Exercise 6: Real-world Validation"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Deploy navigation system on physical robot"}),"\n",(0,t.jsx)(e.li,{children:"Test navigation performance in real environments"}),"\n",(0,t.jsx)(e.li,{children:"Compare simulation vs. reality performance"}),"\n",(0,t.jsx)(e.li,{children:"Identify and address reality gap issues"}),"\n",(0,t.jsx)(e.li,{children:"Validate safety and reliability in real scenarios"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"real-world-mapping",children:"Real-World Mapping"}),"\n",(0,t.jsx)(e.h3,{id:"industrial-applications",children:"Industrial Applications"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Warehouse Automation"}),": Isaac Navigation for AMR fleets"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Manufacturing"}),": Autonomous mobile robots for material transport"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Logistics"}),": Indoor navigation for package delivery robots"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"research-applications",children:"Research Applications"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Humanoid Robotics"}),": Navigation for human-like robots"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Search and Rescue"}),": Autonomous navigation in challenging environments"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Planetary Exploration"}),": Navigation for space robotics missions"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"key-success-factors",children:"Key Success Factors"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"GPU Acceleration"}),": Leveraging CUDA for real-time path planning"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sensor Fusion"}),": Integrating multiple sensor modalities for robust navigation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety"}),": Ensuring safe operation around obstacles and humans"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Reliability"}),": Consistent performance in varied environments"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Scalability"}),": Supporting multiple robots in the same environment"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"Chapter 3 has covered Isaac Navigation, NVIDIA's GPU-accelerated navigation stack for AI-powered mobile robots. We've explored Isaac Navigation's architecture, which leverages CUDA and TensorRT for high-performance path planning, localization, and obstacle avoidance. The examples demonstrated practical implementations of navigation core functionality, visual SLAM integration, and dynamic obstacle avoidance. The hands-on lab exercises provide experience with Isaac Navigation installation, path planning, SLAM integration, and real-world validation. This foundation enables the development of robust, high-performance navigation systems essential for autonomous mobile robots in Physical AI applications."})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>o,x:()=>r});var i=a(6540);const t={},s=i.createContext(t);function o(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:o(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);