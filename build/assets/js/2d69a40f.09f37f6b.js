"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[187],{8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>r});var a=i(6540);const t={},o=a.createContext(t);function s(n){const e=a.useContext(o);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),a.createElement(o.Provider,{value:e},n.children)}},9291:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"modules/module-03-isaac/index","title":"Module 3 - NVIDIA Isaac","description":"Learning Objectives","source":"@site/docs/modules/module-03-isaac/index.mdx","sourceDirName":"modules/module-03-isaac","slug":"/modules/module-03-isaac/","permalink":"/docs/modules/module-03-isaac/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-03-isaac/index.mdx","tags":[],"version":"current","frontMatter":{"title":"Module 3 - NVIDIA Isaac","sidebar_label":"Overview"},"sidebar":"tutorialSidebar","previous":{"title":"Unity Simulation","permalink":"/docs/modules/module-02-gazebo-unity/unity-simulation"},"next":{"title":"Isaac Manipulation","permalink":"/docs/modules/module-03-isaac/isaac-manipulation"}}');var t=i(4848),o=i(8453);const s={title:"Module 3 - NVIDIA Isaac",sidebar_label:"Overview"},r="Module 3: NVIDIA Isaac",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Physical AI Concept",id:"physical-ai-concept",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Tools &amp; Software",id:"tools--software",level:2},{value:"Code / Configuration Examples",id:"code--configuration-examples",level:2},{value:"Isaac ROS Perception Pipeline",id:"isaac-ros-perception-pipeline",level:3},{value:"Isaac Navigation Pipeline",id:"isaac-navigation-pipeline",level:3},{value:"Isaac Manipulation Pipeline",id:"isaac-manipulation-pipeline",level:3},{value:"Practical Lab / Simulation",id:"practical-lab--simulation",level:2},{value:"Lab Exercise 1: Isaac ROS Installation and Setup",id:"lab-exercise-1-isaac-ros-installation-and-setup",level:3},{value:"Lab Exercise 2: Isaac Sim Integration",id:"lab-exercise-2-isaac-sim-integration",level:3},{value:"Lab Exercise 3: Perception Pipeline Implementation",id:"lab-exercise-3-perception-pipeline-implementation",level:3},{value:"Lab Exercise 4: Navigation System Integration",id:"lab-exercise-4-navigation-system-integration",level:3},{value:"Lab Exercise 5: Manipulation Control",id:"lab-exercise-5-manipulation-control",level:3},{value:"Lab Exercise 6: Complete Robotics Application",id:"lab-exercise-6-complete-robotics-application",level:3},{value:"Real-World Mapping",id:"real-world-mapping",level:2},{value:"Industrial Applications",id:"industrial-applications",level:3},{value:"Research Applications",id:"research-applications",level:3},{value:"Key Success Factors",id:"key-success-factors",level:3},{value:"Summary",id:"summary",level:2}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"module-3-nvidia-isaac",children:"Module 3: NVIDIA Isaac"})}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(e.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand the NVIDIA Isaac ecosystem for robotics development"}),"\n",(0,t.jsx)(e.li,{children:"Implement AI-powered robotics applications using Isaac Sim and Isaac ROS"}),"\n",(0,t.jsx)(e.li,{children:"Integrate perception, planning, and control systems using Isaac frameworks"}),"\n",(0,t.jsx)(e.li,{children:"Deploy AI models to NVIDIA hardware for real-time robotics applications"}),"\n",(0,t.jsx)(e.li,{children:"Utilize Isaac's simulation capabilities for robot training and validation"}),"\n",(0,t.jsx)(e.li,{children:"Apply Isaac tools for perception, navigation, and manipulation tasks"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"physical-ai-concept",children:"Physical AI Concept"}),"\n",(0,t.jsx)(e.p,{children:"NVIDIA Isaac represents a comprehensive platform for developing AI-powered physical systems, specifically designed to bridge the gap between artificial intelligence and physical robotics. The Isaac ecosystem provides the computational infrastructure, simulation environments, and software frameworks necessary to create intelligent robots that can perceive, reason, and act in the physical world. Isaac's strength lies in its tight integration with NVIDIA's GPU computing platform, enabling high-performance AI processing for real-time robotics applications."}),"\n",(0,t.jsx)(e.p,{children:"Key aspects of Isaac in Physical AI:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"AI Acceleration"}),": GPU-optimized processing for deep learning and computer vision"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Simulation-to-Reality"}),": Advanced simulation tools for training and validation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Perception Systems"}),": State-of-the-art computer vision and sensor processing"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Navigation & Manipulation"}),": AI-powered planning and control algorithms"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Hardware Integration"}),": Optimized deployment on NVIDIA robotics platforms"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"NVIDIA Isaac Architecture:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Isaac Applications                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502   Navigation    \u2502  \u2502   Manipulation  \u2502  \u2502   Perception    \u2502    \u2502\n\u2502  \u2502   \u2022 Path        \u2502  \u2502   \u2022 Grasping   \u2502  \u2502   \u2022 Object      \u2502    \u2502\n\u2502  \u2502     Planning    \u2502  \u2502   \u2022 Manipulation\u2502  \u2502     Detection   \u2502    \u2502\n\u2502  \u2502   \u2022 SLAM        \u2502  \u2502   \u2022 Force      \u2502  \u2502   \u2022 Segmentation\u2502    \u2502\n\u2502  \u2502   \u2022 Localization\u2502  \u2502     Control    \u2502  \u2502   \u2022 Tracking    \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502              \u2502                    \u2502                    \u2502           \u2502\n\u2502              \u25bc                    \u25bc                    \u25bc           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                  Isaac ROS (ROS 2)                          \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502\n\u2502  \u2502  \u2502  Perception \u2502  \u2502  Planning   \u2502  \u2502  Control    \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  Accelerators\u2502  \u2502  Accelerators\u2502  \u2502  Accelerators\u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Vision   \u2502  \u2502  \u2022 Trajectory\u2502  \u2502  \u2022 Motion   \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Pipeline  \u2502  \u2502    Planning  \u2502  \u2502    Control  \u2502        \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                     \u2502                            \u2502\n\u2502                                     \u25bc                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                   Isaac Core                                  \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502\n\u2502  \u2502  \u2502   Isaac     \u2502  \u2502   Isaac     \u2502  \u2502   Isaac     \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502   Sim       \u2502  \u2502   Apps      \u2502  \u2502   GEMs      \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Physics  \u2502  \u2502  \u2022 Navigation\u2502  \u2502  \u2022 Sensor   \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Rendering\u2502  \u2502  \u2022 Manipulation\u2502 \u2502    Wrappers \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Training \u2502  \u2502  \u2022 Perception\u2502  \u2502  \u2022 AI       \u2502        \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    Models   \u2502        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                     \u2502                            \u2502\n\u2502                                     \u25bc                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              NVIDIA GPU Computing                           \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502\n\u2502  \u2502  \u2502   CUDA      \u2502  \u2502   Tensor    \u2502  \u2502   RTX       \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502   Core      \u2502  \u2502   Core      \u2502  \u2502   Graphics  \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Parallel \u2502  \u2502  \u2022 AI       \u2502  \u2502  \u2022 Real-time\u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Processing\u2502 \u2502    Acceleration\u2502 \u2502    Rendering\u2502        \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                   Robot Hardware                            \u2502\n        \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n        \u2502  \u2502   NVIDIA        \u2502  \u2502   Isaac-        \u2502  \u2502   External  \u2502 \u2502\n        \u2502  \u2502   Jetson/       \u2502  \u2502   Compatible    \u2502  \u2502   Sensors   \u2502 \u2502\n        \u2502  \u2502   Drive         \u2502  \u2502   Platforms     \u2502  \u2502   & Actuators\u2502\u2502\n        \u2502  \u2502  \u2022 AGX Xavier   \u2502  \u2502  \u2022 Isaac ROS2   \u2502  \u2502  \u2022 Cameras  \u2502 \u2502\n        \u2502  \u2502  \u2022 Orin         \u2502  \u2502    Hardware     \u2502  \u2502  \u2022 LIDAR    \u2502 \u2502\n        \u2502  \u2502  \u2022 Clara AGX    \u2502  \u2502    Interface    \u2502  \u2502  \u2022 IMU      \u2502 \u2502\n        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(e.h2,{id:"tools--software",children:"Tools & Software"}),"\n",(0,t.jsx)(e.p,{children:"This module uses the following tools and software:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"NVIDIA Isaac Sim"})," - Advanced robotics simulation environment"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Isaac ROS"})," - GPU-accelerated ROS 2 packages for robotics"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Isaac Apps"})," - Pre-built robotics applications"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"NVIDIA Isaac GEMs"})," - GPU-accelerated hardware abstractions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"CUDA & TensorRT"})," - GPU computing and AI acceleration frameworks"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"NVIDIA Jetson"})," - Edge AI computing platforms for robotics"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"NVIDIA Drive"})," - Autonomous vehicle computing platform"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Deep Learning SDKs"})," - For training and deploying AI models"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"code--configuration-examples",children:"Code / Configuration Examples"}),"\n",(0,t.jsx)(e.h3,{id:"isaac-ros-perception-pipeline",children:"Isaac ROS Perception Pipeline"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\nfrom geometry_msgs.msg import PointStamped\nfrom std_msgs.msg import Header\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\nimport torch\nimport torchvision.transforms as transforms\n\nclass IsaacPerceptionNode(Node):\n    def __init__(self):\n        super().__init__('isaac_perception_node')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/rgb/image_rect_color', self.image_callback, 10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, '/camera/rgb/camera_info', self.camera_info_callback, 10\n        )\n        self.detection_pub = self.create_publisher(\n            PointStamped, '/perception/detected_object', 10\n        )\n\n        # Load pre-trained model (using Isaac-compatible model)\n        self.load_detection_model()\n\n        # Camera parameters\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n\n        self.get_logger().info('Isaac perception node initialized')\n\n    def load_detection_model(self):\n        \"\"\"Load a pre-trained object detection model optimized for Isaac\"\"\"\n        # In a real implementation, this would load a TensorRT-optimized model\n        # For this example, we'll use a placeholder\n        try:\n            # Example: Load a model optimized for Jetson\n            self.detection_model = torch.hub.load(\n                'ultralytics/yolov5',\n                'yolov5s',\n                pretrained=True\n            )\n            self.detection_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n            self.detection_model.eval()\n            self.get_logger().info('Detection model loaded successfully')\n        except Exception as e:\n            self.get_logger().error(f'Failed to load detection model: {e}')\n            self.detection_model = None\n\n    def camera_info_callback(self, msg):\n        \"\"\"Process camera calibration information\"\"\"\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.distortion_coeffs = np.array(msg.d)\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera image\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Run object detection using Isaac-optimized pipeline\n            detections = self.run_detection(cv_image)\n\n            # Process detections and publish results\n            if detections is not None:\n                for detection in detections:\n                    self.publish_detection(detection, msg.header)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def run_detection(self, image):\n        \"\"\"Run object detection on the input image\"\"\"\n        if self.detection_model is None:\n            return None\n\n        try:\n            # Preprocess image for the model\n            img_tensor = transforms.ToTensor()(image).unsqueeze(0)\n            img_tensor = img_tensor.to('cuda' if torch.cuda.is_available() else 'cpu')\n\n            # Run inference\n            with torch.no_grad():\n                results = self.detection_model(img_tensor)\n\n            # Process results (simplified)\n            # In a real implementation, this would extract bounding boxes, classes, etc.\n            detections = []\n            if hasattr(results, 'xyxy') and len(results.xyxy[0]) > 0:\n                for *xyxy, conf, cls in results.xyxy[0].tolist():\n                    if conf > 0.5:  # Confidence threshold\n                        detections.append({\n                            'bbox': xyxy,\n                            'confidence': conf,\n                            'class_id': int(cls)\n                        })\n\n            return detections\n        except Exception as e:\n            self.get_logger().error(f'Error in detection: {e}')\n            return None\n\n    def publish_detection(self, detection, header):\n        \"\"\"Publish detection result as a PointStamped message\"\"\"\n        point_msg = PointStamped()\n        point_msg.header = header\n        point_msg.header.frame_id = 'camera_rgb_optical_frame'\n\n        # Calculate center of bounding box\n        x1, y1, x2, y2 = detection['bbox']\n        center_x = int((x1 + x2) / 2)\n        center_y = int((y1 + y2) / 2)\n\n        # Convert pixel coordinates to 3D point (simplified)\n        # In a real implementation, this would use depth information\n        point_msg.point.x = float(center_x)\n        point_msg.point.y = float(center_y)\n        point_msg.point.z = 1.0  # Placeholder depth\n\n        self.detection_pub.publish(point_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacPerceptionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(e.h3,{id:"isaac-navigation-pipeline",children:"Isaac Navigation Pipeline"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav_msgs.msg import Odometry, Path\nfrom sensor_msgs.msg import LaserScan\nfrom tf2_ros import TransformException\nfrom tf2_ros.buffer import Buffer\nfrom tf2_ros.transform_listener import TransformListener\nimport numpy as np\nimport math\n\nclass IsaacNavigationNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_navigation_node\')\n\n        # TF2 setup for coordinate transformations\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Publishers and subscribers\n        self.odom_sub = self.create_subscription(\n            Odometry, \'/odom\', self.odom_callback, 10\n        )\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10\n        )\n        self.goal_sub = self.create_subscription(\n            PoseStamped, \'/move_base_simple/goal\', self.goal_callback, 10\n        )\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.path_pub = self.create_publisher(Path, \'/navigation/global_plan\', 10)\n\n        # Navigation state\n        self.current_pose = None\n        self.goal_pose = None\n        self.scan_data = None\n        self.navigation_active = False\n\n        # Navigation parameters\n        self.linear_vel = 0.5\n        self.angular_vel = 0.8\n        self.safe_distance = 0.5\n        self.arrival_threshold = 0.3\n\n        # Timer for navigation control\n        self.nav_timer = self.create_timer(0.1, self.navigation_control)\n\n        self.get_logger().info(\'Isaac navigation node initialized\')\n\n    def odom_callback(self, msg):\n        """Update current robot pose from odometry"""\n        self.current_pose = msg.pose.pose\n\n    def scan_callback(self, msg):\n        """Update laser scan data for obstacle detection"""\n        self.scan_data = msg\n\n    def goal_callback(self, msg):\n        """Set new navigation goal"""\n        self.goal_pose = msg.pose\n        self.navigation_active = True\n        self.get_logger().info(f\'New goal set: ({msg.pose.position.x}, {msg.pose.position.y})\')\n\n        # Generate simple path to goal (in a real system, this would use a path planner)\n        self.publish_simple_path()\n\n    def publish_simple_path(self):\n        """Publish a simple path to the goal (straight line)"""\n        if self.current_pose and self.goal_pose:\n            path_msg = Path()\n            path_msg.header.stamp = self.get_clock().now().to_msg()\n            path_msg.header.frame_id = \'map\'\n\n            # Create simple path with intermediate points\n            start_pos = self.current_pose.position\n            goal_pos = self.goal_pose.position\n\n            # Generate intermediate points\n            steps = 10\n            for i in range(steps + 1):\n                t = i / steps\n                intermediate_pose = PoseStamped()\n                intermediate_pose.header.frame_id = \'map\'\n                intermediate_pose.pose.position.x = start_pos.x + t * (goal_pos.x - start_pos.x)\n                intermediate_pose.pose.position.y = start_pos.y + t * (goal_pos.y - start_pos.y)\n                intermediate_pose.pose.position.z = start_pos.z + t * (goal_pos.z - start_pos.z)\n\n                path_msg.poses.append(intermediate_pose)\n\n            self.path_pub.publish(path_msg)\n\n    def navigation_control(self):\n        """Main navigation control loop"""\n        if not self.navigation_active or not self.current_pose or not self.goal_pose:\n            return\n\n        # Check for obstacles\n        if self.scan_data and self.is_path_blocked():\n            self.stop_robot()\n            self.get_logger().warn(\'Path blocked by obstacle, stopping\')\n            return\n\n        # Calculate direction to goal\n        current_pos = self.current_pose.position\n        goal_pos = self.goal_pose.position\n\n        dx = goal_pos.x - current_pos.x\n        dy = goal_pos.y - current_pos.y\n        distance_to_goal = math.sqrt(dx*dx + dy*dy)\n\n        # Check if reached goal\n        if distance_to_goal < self.arrival_threshold:\n            self.stop_robot()\n            self.navigation_active = False\n            self.get_logger().info(\'Reached goal\')\n            return\n\n        # Calculate heading to goal\n        goal_heading = math.atan2(dy, dx)\n\n        # Get current orientation\n        current_yaw = self.get_yaw_from_quaternion(self.current_pose.orientation)\n\n        # Calculate angular error\n        angle_error = self.normalize_angle(goal_heading - current_yaw)\n\n        # Create velocity command\n        cmd = Twist()\n\n        # Proportional controller for angular velocity\n        cmd.angular.z = max(-self.angular_vel, min(self.angular_vel, angle_error * 2.0))\n\n        # Move forward if roughly aligned with goal\n        if abs(angle_error) < 0.3:\n            cmd.linear.x = self.linear_vel\n\n        self.cmd_vel_pub.publish(cmd)\n\n    def is_path_blocked(self):\n        """Check if the path to goal is blocked by obstacles"""\n        if not self.scan_data:\n            return False\n\n        # Check laser scan for obstacles in front of robot\n        front_scan = self.scan_data.ranges[len(self.scan_data.ranges)//2 - 30 : len(self.scan_data.ranges)//2 + 30]\n        valid_ranges = [r for r in front_scan if not (math.isnan(r) or math.isinf(r))]\n\n        if valid_ranges:\n            min_distance = min(valid_ranges)\n            return min_distance < self.safe_distance\n\n        return False\n\n    def stop_robot(self):\n        """Stop robot movement"""\n        cmd = Twist()\n        self.cmd_vel_pub.publish(cmd)\n\n    def get_yaw_from_quaternion(self, quaternion):\n        """Extract yaw angle from quaternion"""\n        siny_cosp = 2 * (quaternion.w * quaternion.z + quaternion.x * quaternion.y)\n        cosy_cosp = 1 - 2 * (quaternion.y * quaternion.y + quaternion.z * quaternion.z)\n        return math.atan2(siny_cosp, cosy_cosp)\n\n    def normalize_angle(self, angle):\n        """Normalize angle to [-pi, pi] range"""\n        while angle > math.pi:\n            angle -= 2.0 * math.pi\n        while angle < -math.pi:\n            angle += 2.0 * math.pi\n        return angle\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacNavigationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"isaac-manipulation-pipeline",children:"Isaac Manipulation Pipeline"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Pose, Point, Vector3\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import Float64MultiArray\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nfrom builtin_interfaces.msg import Duration\nimport numpy as np\nimport math\nfrom scipy.spatial.transform import Rotation as R\n\nclass IsaacManipulationNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_manipulation_node\')\n\n        # Publishers and subscribers\n        self.joint_state_sub = self.create_subscription(\n            JointState, \'/joint_states\', self.joint_state_callback, 10\n        )\n        self.joint_traj_pub = self.create_publisher(\n            JointTrajectory, \'/joint_trajectory\', 10\n        )\n\n        # Manipulation state\n        self.current_joint_positions = {}\n        self.target_pose = None\n        self.manipulation_active = False\n\n        # Robot parameters (simplified 3-DOF arm)\n        self.link_lengths = [0.3, 0.3, 0.2]  # Link lengths for simple arm\n        self.joint_names = [\'joint_1\', \'joint_2\', \'joint_3\']\n\n        # Timer for manipulation control\n        self.manip_timer = self.create_timer(0.1, self.manipulation_control)\n\n        self.get_logger().info(\'Isaac manipulation node initialized\')\n\n    def joint_state_callback(self, msg):\n        """Update current joint positions"""\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.current_joint_positions[name] = msg.position[i]\n\n    def move_to_cartesian_pose(self, target_pose):\n        """Move end effector to specified Cartesian pose"""\n        # Perform inverse kinematics to get joint angles\n        joint_angles = self.inverse_kinematics(target_pose)\n\n        if joint_angles is not None:\n            self.target_joint_positions = joint_angles\n            self.manipulation_active = True\n            self.execute_joint_trajectory(joint_angles)\n            self.get_logger().info(f\'Moving to pose: {target_pose.position}\')\n\n    def inverse_kinematics(self, target_pose):\n        """Simple inverse kinematics for 3-DOF arm (simplified solution)"""\n        # Extract target position\n        x = target_pose.position.x\n        y = target_pose.position.y\n        z = target_pose.position.z\n\n        # Simplified inverse kinematics for a 3-DOF arm\n        # This is a basic implementation - real IK would be more complex\n        try:\n            # Calculate distance from base to target in XY plane\n            r = math.sqrt(x*x + y*y)\n\n            # Height\n            h = z\n\n            # Link lengths\n            l1, l2, l3 = self.link_lengths\n\n            # Check if target is reachable\n            max_reach = l1 + l2 + l3\n            if math.sqrt(r*r + h*h) > max_reach:\n                self.get_logger().warn(\'Target position is not reachable\')\n                return None\n\n            # Simplified 3-DOF IK solution\n            # Joint 1: rotation around Z-axis\n            joint1 = math.atan2(y, x)\n\n            # Calculate projection on the plane perpendicular to joint 1\n            # For simplicity, assume l1 is vertical, l2 and l3 are horizontal\n            # This is a very simplified model\n            target_xz_distance = math.sqrt(r*r + (h - l1)**2)\n\n            # Use law of cosines for remaining joints\n            cos_angle = (l2*l2 + l3*l3 - target_xz_distance*target_xz_distance) / (2*l2*l3)\n            if abs(cos_angle) > 1.0:\n                return None  # Not reachable\n\n            angle = math.acos(cos_angle)\n            joint3 = math.pi - angle  # Elbow-up solution\n\n            # Calculate joint2\n            angle2_intermediate = math.atan2(h - l1, r) if r != 0 else 0\n            cos_angle2 = (l2*l2 + target_xz_distance*target_xz_distance - l3*l3) / (2*l2*target_xz_distance)\n            if abs(cos_angle2) > 1.0:\n                return None\n\n            angle2 = math.acos(cos_angle2)\n            joint2 = angle2_intermediate + angle2\n\n            return [joint1, joint2, joint3]\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in inverse kinematics: {e}\')\n            return None\n\n    def execute_joint_trajectory(self, joint_positions):\n        """Execute joint trajectory to reach target positions"""\n        traj_msg = JointTrajectory()\n        traj_msg.joint_names = self.joint_names\n\n        point = JointTrajectoryPoint()\n        point.positions = joint_positions\n        point.velocities = [0.0] * len(joint_positions)  # Start and end with zero velocity\n        point.accelerations = [0.0] * len(joint_positions)\n        point.time_from_start = Duration(sec=2, nanosec=0)  # 2 seconds to reach target\n\n        traj_msg.points.append(point)\n\n        self.joint_traj_pub.publish(traj_msg)\n\n    def manipulation_control(self):\n        """Main manipulation control loop"""\n        if not self.manipulation_active:\n            return\n\n        # Check if we\'ve reached the target\n        if self.has_reached_target():\n            self.manipulation_active = False\n            self.get_logger().info(\'Reached target position\')\n\n    def has_reached_target(self):\n        """Check if joints have reached target positions"""\n        if not hasattr(self, \'target_joint_positions\'):\n            return False\n\n        tolerance = 0.05  # 5 degrees tolerance\n        for i, joint_name in enumerate(self.joint_names):\n            if joint_name in self.current_joint_positions:\n                current_pos = self.current_joint_positions[joint_name]\n                target_pos = self.target_joint_positions[i]\n                if abs(current_pos - target_pos) > tolerance:\n                    return False\n        return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacManipulationNode()\n\n    try:\n        # Example: Move to a specific position after initialization\n        node.move_to_cartesian_pose(Pose(\n            position=Point(x=0.4, y=0.0, z=0.3),\n            orientation=Point(x=0.0, y=0.0, z=0.0)  # Simplified\n        ))\n\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"practical-lab--simulation",children:"Practical Lab / Simulation"}),"\n",(0,t.jsx)(e.h3,{id:"lab-exercise-1-isaac-ros-installation-and-setup",children:"Lab Exercise 1: Isaac ROS Installation and Setup"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Set up NVIDIA Isaac ROS environment:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Install Isaac ROS packages"}),"\n",(0,t.jsx)(e.li,{children:"Configure CUDA and GPU acceleration"}),"\n",(0,t.jsx)(e.li,{children:"Set up Isaac-compatible hardware (Jetson or RTX)"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Verify installation with basic perception pipeline:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"# Test Isaac perception nodes\nros2 launch isaac_ros_apriltag isaac_ros_apriltag.launch.py\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Run basic perception demo to verify functionality"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-exercise-2-isaac-sim-integration",children:"Lab Exercise 2: Isaac Sim Integration"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Install NVIDIA Isaac Sim"}),"\n",(0,t.jsx)(e.li,{children:"Load a robot model into Isaac Sim"}),"\n",(0,t.jsx)(e.li,{children:"Configure sensors (cameras, LIDAR, IMU) on the robot"}),"\n",(0,t.jsx)(e.li,{children:"Connect Isaac Sim to ROS 2 network"}),"\n",(0,t.jsx)(e.li,{children:"Control the simulated robot using ROS 2 commands"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-exercise-3-perception-pipeline-implementation",children:"Lab Exercise 3: Perception Pipeline Implementation"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement the Isaac perception pipeline from the example"}),"\n",(0,t.jsx)(e.li,{children:"Test object detection with various objects"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate detection performance and accuracy"}),"\n",(0,t.jsx)(e.li,{children:"Optimize for real-time performance on edge hardware"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-exercise-4-navigation-system-integration",children:"Lab Exercise 4: Navigation System Integration"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement the Isaac navigation pipeline"}),"\n",(0,t.jsx)(e.li,{children:"Set up map-based navigation in simulation"}),"\n",(0,t.jsx)(e.li,{children:"Test obstacle avoidance and path planning"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate navigation performance metrics"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-exercise-5-manipulation-control",children:"Lab Exercise 5: Manipulation Control"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement the Isaac manipulation pipeline"}),"\n",(0,t.jsx)(e.li,{children:"Test inverse kinematics with different target poses"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate trajectory execution accuracy"}),"\n",(0,t.jsx)(e.li,{children:"Integrate with perception for pick-and-place tasks"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"lab-exercise-6-complete-robotics-application",children:"Lab Exercise 6: Complete Robotics Application"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Combine perception, navigation, and manipulation"}),"\n",(0,t.jsx)(e.li,{children:"Create a complete application (e.g., object fetching)"}),"\n",(0,t.jsx)(e.li,{children:"Test in Isaac Sim and optimize performance"}),"\n",(0,t.jsx)(e.li,{children:"Deploy to physical hardware if available"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"real-world-mapping",children:"Real-World Mapping"}),"\n",(0,t.jsx)(e.h3,{id:"industrial-applications",children:"Industrial Applications"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Warehouse Automation"}),": Isaac-powered robots for inventory management"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Manufacturing"}),": Assembly and quality control robots with Isaac perception"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Logistics"}),": Autonomous mobile robots with Isaac navigation systems"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"research-applications",children:"Research Applications"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Humanoid Robotics"}),": Isaac for perception and control of humanoid robots"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Service Robotics"}),": Isaac for navigation and manipulation in human environments"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Agriculture"}),": Isaac-powered robots for precision farming"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"key-success-factors",children:"Key Success Factors"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"GPU Acceleration"}),": Leveraging NVIDIA hardware for real-time AI processing"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Simulation"}),": Using Isaac Sim for safe and efficient development"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Integration"}),": Seamless integration between perception, planning, and control"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Performance"}),": Optimized for real-time robotics applications"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Scalability"}),": From research to production deployment"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"Module 3 introduces the NVIDIA Isaac ecosystem, a comprehensive platform for developing AI-powered robotics applications. We've explored Isaac's architecture, including Isaac Sim, Isaac ROS, and Isaac Apps, and provided practical examples of perception, navigation, and manipulation pipelines. The hands-on lab exercises guide you through setting up Isaac, implementing core robotics functions, and creating complete applications. This module provides the foundation for leveraging NVIDIA's AI acceleration capabilities in Physical AI and humanoid robotics applications."})]})}function d(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(p,{...n})}):p(n)}}}]);