"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[97],{8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var s=t(6540);const a={},i=s.createContext(a);function o(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(i.Provider,{value:n},e.children)}},8827:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"modules/module-04-vla-capstone/capstone-project","title":"Chapter 3 - Capstone Project","description":"Learning Objectives","source":"@site/docs/modules/module-04-vla-capstone/capstone-project.mdx","sourceDirName":"modules/module-04-vla-capstone","slug":"/modules/module-04-vla-capstone/capstone-project","permalink":"/docs/modules/module-04-vla-capstone/capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-04-vla-capstone/capstone-project.mdx","tags":[],"version":"current","frontMatter":{"title":"Chapter 3 - Capstone Project","sidebar_label":"Capstone Project"},"sidebar":"tutorialSidebar","previous":{"title":"Overview","permalink":"/docs/modules/module-04-vla-capstone/"},"next":{"title":"Conversational Robotics","permalink":"/docs/modules/module-04-vla-capstone/conversational-robotics"}}');var a=t(4848),i=t(8453);const o={title:"Chapter 3 - Capstone Project",sidebar_label:"Capstone Project"},r="Chapter 3: Capstone Project",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Physical AI Concept",id:"physical-ai-concept",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Complete Physical AI System Architecture",id:"complete-physical-ai-system-architecture",level:3},{value:"Capstone System Integration Architecture",id:"capstone-system-integration-architecture",level:3},{value:"Tools &amp; Software",id:"tools--software",level:2},{value:"Code / Configuration Examples",id:"code--configuration-examples",level:2},{value:"Capstone Main Control Node",id:"capstone-main-control-node",level:3},{value:"Capstone Integration Test Suite",id:"capstone-integration-test-suite",level:3},{value:"Capstone System Evaluation Node",id:"capstone-system-evaluation-node",level:3},{value:"Practical Lab / Simulation",id:"practical-lab--simulation",level:2},{value:"Lab Exercise 1: System Integration",id:"lab-exercise-1-system-integration",level:3},{value:"Lab Exercise 2: Task Orchestration",id:"lab-exercise-2-task-orchestration",level:3},{value:"Lab Exercise 3: Performance Evaluation",id:"lab-exercise-3-performance-evaluation",level:3},{value:"Lab Exercise 4: Integration Testing",id:"lab-exercise-4-integration-testing",level:3},{value:"Lab Exercise 5: Real-world Validation",id:"lab-exercise-5-real-world-validation",level:3},{value:"Lab Exercise 6: System Optimization",id:"lab-exercise-6-system-optimization",level:3},{value:"Real-World Mapping",id:"real-world-mapping",level:2},{value:"Industrial Applications",id:"industrial-applications",level:3},{value:"Research Applications",id:"research-applications",level:3},{value:"Key Success Factors",id:"key-success-factors",level:3},{value:"Summary",id:"summary",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-3-capstone-project",children:"Chapter 3: Capstone Project"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Integrate all components from previous modules into a comprehensive Physical AI system"}),"\n",(0,a.jsx)(n.li,{children:"Design and implement a complete humanoid robotics application"}),"\n",(0,a.jsx)(n.li,{children:"Apply Vision-Language-Action principles to a real-world scenario"}),"\n",(0,a.jsx)(n.li,{children:"Demonstrate multimodal AI capabilities in a physical system"}),"\n",(0,a.jsx)(n.li,{children:"Validate and evaluate the complete system performance"}),"\n",(0,a.jsx)(n.li,{children:"Document and present the integrated system for academic and industrial audiences"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"physical-ai-concept",children:"Physical AI Concept"}),"\n",(0,a.jsx)(n.p,{children:"The Capstone Project represents the culmination of all concepts learned throughout the Physical AI & Humanoid Robotics curriculum. It demonstrates the integration of perception, cognition, and action in a complete physical system that can interact with the real world. This project embodies the essence of Physical AI by creating a system that bridges the digital and physical realms through embodied intelligence."}),"\n",(0,a.jsx)(n.p,{children:"The capstone system integrates:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Perception"}),": Computer vision, sensor processing, and environmental understanding"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cognition"}),": Natural language processing, reasoning, and decision making"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action"}),": Navigation, manipulation, and physical interaction capabilities"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Integration"}),": Seamless coordination between all components for coherent behavior"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Embodiment"}),": Physical manifestation of AI capabilities in a robotic platform"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"This project demonstrates how all the individual components studied in previous modules work together to create a unified Physical AI system capable of complex, real-world interactions."}),"\n",(0,a.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,a.jsx)(n.h3,{id:"complete-physical-ai-system-architecture",children:"Complete Physical AI System Architecture"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Complete Physical AI System Architecture:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Human-System Interface                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502   Voice         \u2502  \u2502   Visual        \u2502  \u2502   Haptic        \u2502    \u2502\n\u2502  \u2502   Commands      \u2502  \u2502   Interaction   \u2502  \u2502   Feedback      \u2502    \u2502\n\u2502  \u2502  \u2022 Natural      \u2502  \u2502  \u2022 Gesture     \u2502  \u2502  \u2022 Force        \u2502    \u2502\n\u2502  \u2502    Language     \u2502  \u2502    Recognition  \u2502  \u2502    Feedback     \u2502    \u2502\n\u2502  \u2502  \u2022 Questions    \u2502  \u2502  \u2022 Object      \u2502  \u2502  \u2022 Tactile      \u2502    \u2502\n\u2502  \u2502  \u2022 Instructions \u2502  \u2502    Recognition  \u2502  \u2502    Interaction  \u2502    \u2502\n\u2502  \u2502  \u2022 Conversations\u2502  \u2502  \u2022 Scene       \u2502  \u2502  \u2022 Touch        \u2502    \u2502\n\u2502  \u2502                 \u2502  \u2502    Understanding\u2502  \u2502    Responses    \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502              \u2502                    \u2502                    \u2502           \u2502\n\u2502              \u25bc                    \u25bc                    \u25bc           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Multimodal Perception Layer                    \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502\n\u2502  \u2502  \u2502  Vision     \u2502  \u2502  Audio      \u2502  \u2502  Sensor      \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  Processing \u2502  \u2502  Processing \u2502  \u2502  Fusion      \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Object   \u2502  \u2502  \u2022 Speech   \u2502  \u2502  \u2022 IMU        \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Detection \u2502  \u2502    Recognition\u2502  \u2502  \u2022 LIDAR      \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Scene    \u2502  \u2502  \u2022 Noise    \u2502  \u2502  \u2022 Force      \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Understanding\u2502\u2502    Reduction \u2502  \u2502    Sensors    \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 3D       \u2502  \u2502  \u2022 Wake     \u2502  \u2502  \u2022 Multi-     \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Reconstruction\u2502\u2502    Word     \u2502  \u2502    Modal      \u2502        \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    Detection \u2502  \u2502    Integration\u2502        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                     \u2502                            \u2502\n\u2502                                     \u25bc                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              AI Reasoning & Planning Core                   \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502\n\u2502  \u2502  \u2502  Language   \u2502  \u2502  Vision-    \u2502  \u2502  Decision    \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  Understanding\u2502 \u2502  Language-  \u2502  \u2502  Making     \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Intent   \u2502  \u2502  Action     \u2502  \u2502  \u2022 Task      \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Extraction\u2502  \u2502    Integration\u2502 \u2502    Planning   \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Semantic \u2502  \u2502  \u2022 Grasp    \u2502  \u2502  \u2022 Behavior  \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Parsing   \u2502  \u2502    Planning  \u2502  \u2502    Selection \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Context  \u2502  \u2502  \u2022 Navigation\u2502  \u2502  \u2022 Policy    \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Modeling  \u2502  \u2502    Planning  \u2502  \u2502    Execution \u2502        \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                     \u2502                            \u2502\n\u2502                                     \u25bc                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Physical Action Execution                      \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502\n\u2502  \u2502  \u2502  Navigation \u2502  \u2502  Manipulation\u2502  \u2502  Social      \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Path     \u2502  \u2502  \u2022 Grasping  \u2502  \u2502  Interaction \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Planning  \u2502  \u2502  \u2022 Tool Use \u2502  \u2502  \u2022 Expressions\u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Obstacle  \u2502  \u2502  \u2022 Force    \u2502  \u2502  \u2022 Etiquette \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Avoidance \u2502  \u2502    Control  \u2502  \u2502  \u2022 Emotions  \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 SLAM      \u2502  \u2502  \u2022 Multi-   \u2502  \u2502  \u2022 Safety    \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Localization\u2502\u2502    Fingered  \u2502  \u2502    Protocols \u2502        \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    Grasping  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                     \u2502                            \u2502\n\u2502                                     \u25bc                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Hardware Abstraction Layer                     \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502\n\u2502  \u2502  \u2502  Perception \u2502  \u2502  Control    \u2502  \u2502  Communication\u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Camera   \u2502  \u2502  \u2022 Joint    \u2502  \u2502  \u2022 ROS 2      \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Drivers   \u2502  \u2502    Control  \u2502  \u2502  \u2022 Isaac     \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Sensor   \u2502  \u2502  \u2022 Trajectory\u2502 \u2502    Communication\u2502      \u2502  \u2502\n\u2502  \u2502  \u2502    Interfaces\u2502  \u2502    Planning  \u2502  \u2502  \u2022 Network   \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Data     \u2502  \u2502  \u2022 Motion   \u2502  \u2502    Protocols  \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Preprocessing\u2502\u2502    Control  \u2502  \u2502  \u2022 Real-time \u2502        \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    Communication\u2502      \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,a.jsx)(n.h3,{id:"capstone-system-integration-architecture",children:"Capstone System Integration Architecture"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Capstone System Integration:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   External      \u2502\u2500\u2500\u2500\u25b6\u2502  Main Control   \u2502\u2500\u2500\u2500\u25b6\u2502  Perception     \u2502\n\u2502   Commands      \u2502    \u2502  Node          \u2502    \u2502  Subsystem     \u2502\n\u2502  \u2022 Voice        \u2502    \u2502  \u2022 Task        \u2502    \u2502  \u2022 Vision      \u2502\n\u2502  \u2022 Text         \u2502    \u2502    Orchestrator \u2502    \u2502  \u2022 Audio       \u2502\n\u2502  \u2022 Gesture      \u2502    \u2502  \u2022 State       \u2502    \u2502  \u2022 Sensor      \u2502\n\u2502  \u2022 App Control  \u2502    \u2502    Management   \u2502    \u2502    Processing   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502  \u2022 Safety      \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502               \u2502    Monitoring   \u2502              \u2502\n         \u25bc               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Task Planning \u2502\u2500\u2500\u2500\u25b6\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2500\u2500\u2500\u25b6\u2502  Action         \u2502\n\u2502   Subsystem     \u2502    \u2502  Integration    \u2502    \u2502  Execution      \u2502\n\u2502  \u2022 High-level   \u2502    \u2502  Layer          \u2502    \u2502  Subsystem     \u2502\n\u2502    Planning     \u2502    \u2502  \u2022 Data         \u2502    \u2502  \u2022 Navigation  \u2502\n\u2502  \u2022 Skill        \u2502    \u2502    Aggregation   \u2502    \u2502  \u2022 Manipulation\u2502\n\u2502    Composition  \u2502    \u2502  \u2022 Event        \u2502    \u2502  \u2022 Interaction \u2502\n\u2502  \u2022 Sequence     \u2502    \u2502    Synchronization\u2502   \u2502  \u2022 Safety      \u2502\n\u2502    Generation   \u2502    \u2502  \u2022 Component    \u2502    \u2502    Management   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502    Coordination  \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n         \u25bc                       \u2502                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Simulation    \u2502\u2500\u2500\u2500\u25b6\u2502  Real-world     \u2502\u2500\u2500\u2500\u25b6\u2502  Evaluation     \u2502\n\u2502   Interface     \u2502    \u2502  Interface      \u2502    \u2502  & Validation   \u2502\n\u2502  \u2022 Gazebo       \u2502    \u2502  \u2022 Hardware     \u2502    \u2502  \u2022 Performance  \u2502\n\u2502  \u2022 Isaac Sim    \u2502    \u2502    Abstraction   \u2502    \u2502    Metrics     \u2502\n\u2502  \u2022 Unity        \u2502    \u2502  \u2022 Safety       \u2502    \u2502  \u2022 Accuracy    \u2502\n\u2502  \u2022 Environment  \u2502    \u2502    Override     \u2502    \u2502  \u2022 Robustness  \u2502\n\u2502    Simulation   \u2502    \u2502  \u2022 Calibration  \u2502    \u2502  \u2022 User        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502    Management   \u2502    \u2502    Satisfaction \u2502\n                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,a.jsx)(n.h2,{id:"tools--software",children:"Tools & Software"}),"\n",(0,a.jsx)(n.p,{children:"This capstone project integrates all tools and software from previous modules:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"ROS 2 Humble Hawksbill"})," - Robot operating system framework"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"NVIDIA Isaac"})," - GPU-accelerated robotics platform"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Gazebo"})," - Physics-based simulation environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Unity"})," - 3D visualization and simulation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"OpenVLA"})," - Vision-Language-Action models"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"CLIP"})," - Vision-language models for multimodal understanding"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Transformers"})," - Hugging Face libraries for language models"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"PyTorch/TensorFlow"})," - Deep learning frameworks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"OpenCV"})," - Computer vision library"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Rviz2"})," - Visualization for debugging and monitoring"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"code--configuration-examples",children:"Code / Configuration Examples"}),"\n",(0,a.jsx)(n.h3,{id:"capstone-main-control-node",children:"Capstone Main Control Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool, Int8\nfrom sensor_msgs.msg import Image, LaserScan, JointState\nfrom geometry_msgs.msg import Pose, Twist\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import Point\nfrom builtin_interfaces.msg import Time\nfrom tf2_ros import TransformException\nfrom tf2_ros.buffer import Buffer\nfrom tf2_ros.transform_listener import TransformListener\nimport numpy as np\nimport math\nimport time\nimport threading\nimport queue\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\nclass TaskState(Enum):\n    IDLE = 1\n    PLANNING = 2\n    EXECUTING = 3\n    COMPLETED = 4\n    FAILED = 5\n    PAUSED = 6\n\nclass RobotState(Enum):\n    INITIALIZING = 1\n    READY = 2\n    BUSY = 3\n    ERROR = 4\n    SAFETY_STOP = 5\n\n@dataclass\nclass Task:\n    id: str\n    type: str\n    description: str\n    priority: int\n    state: TaskState\n    created_time: float\n    start_time: Optional[float] = None\n    completion_time: Optional[float] = None\n    dependencies: List[str] = None\n    resources_required: List[str] = None\n\nclass CapstoneMainControlNode(Node):\n    def __init__(self):\n        super().__init__('capstone_main_control')\n\n        # TF2 setup\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Publishers and subscribers\n        self.setup_communication_interfaces()\n\n        # Initialize system components\n        self.initialize_system_components()\n\n        # Task management\n        self.task_queue = queue.Queue()\n        self.active_tasks = {}\n        self.completed_tasks = []\n        self.failed_tasks = []\n\n        # System state\n        self.robot_state = RobotState.INITIALIZING\n        self.current_task = None\n        self.system_health = {'perception': True, 'navigation': True, 'manipulation': True, 'communication': True}\n        self.safety_status = True\n\n        # Task planning parameters\n        self.max_concurrent_tasks = 3\n        self.task_retry_limit = 3\n        self.task_timeout = 300.0  # 5 minutes\n\n        # Timers for system monitoring\n        self.monitoring_timer = self.create_timer(1.0, self.system_monitoring_callback)\n        self.task_execution_timer = self.create_timer(0.1, self.task_execution_callback)\n\n        # Threads for parallel processing\n        self.perception_thread = threading.Thread(target=self.perception_processing_loop)\n        self.perception_thread.daemon = True\n        self.perception_thread.start()\n\n        self.get_logger().info('Capstone Main Control node initialized')\n\n    def setup_communication_interfaces(self):\n        \"\"\"Setup all publishers and subscribers for system integration\"\"\"\n        # Command interfaces\n        self.command_sub = self.create_subscription(\n            String, '/capstone/command', self.command_callback, 10\n        )\n        self.voice_command_sub = self.create_subscription(\n            String, '/robot/voice_command', self.voice_command_callback, 10\n        )\n\n        # Sensor interfaces\n        self.odom_sub = self.create_subscription(\n            Odometry, '/odom', self.odom_callback, 10\n        )\n        self.scan_sub = self.create_subscription(\n            LaserScan, '/scan', self.scan_callback, 10\n        )\n        self.joint_state_sub = self.create_subscription(\n            JointState, '/joint_states', self.joint_state_callback, 10\n        )\n        self.camera_sub = self.create_subscription(\n            Image, '/camera/rgb/image_rect_color', self.camera_callback, 10\n        )\n\n        # Control interfaces\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.status_pub = self.create_publisher(String, '/capstone/status', 10)\n        self.task_status_pub = self.create_publisher(String, '/capstone/task_status', 10)\n        self.system_alert_pub = self.create_publisher(String, '/capstone/system_alert', 10)\n\n        # Integration with other subsystems\n        self.navigation_goal_pub = self.create_publisher(Pose, '/navigation/goal', 10)\n        self.manipulation_cmd_pub = self.create_publisher(String, '/manipulation/command', 10)\n        self.perception_req_pub = self.create_publisher(String, '/perception/request', 10)\n\n    def initialize_system_components(self):\n        \"\"\"Initialize all system components and verify connectivity\"\"\"\n        self.get_logger().info('Initializing system components...')\n\n        # Initialize component status\n        self.component_status = {\n            'navigation': False,\n            'manipulation': False,\n            'perception': False,\n            'communication': False\n        }\n\n        # Verify component connectivity\n        self.verify_component_connectivity()\n\n        # Initialize robot state\n        self.robot_state = RobotState.READY\n        self.get_logger().info('System components initialized successfully')\n\n    def verify_component_connectivity(self):\n        \"\"\"Verify that all required components are available\"\"\"\n        # This would normally involve service calls or topic availability checks\n        # For this example, we'll simulate component verification\n        import random\n        for component in self.component_status:\n            # Simulate 95% availability for each component\n            self.component_status[component] = random.random() > 0.05\n\n        self.get_logger().info(f'Component status: {self.component_status}')\n\n    def command_callback(self, msg):\n        \"\"\"Process high-level commands\"\"\"\n        command = msg.data.strip().lower()\n        self.get_logger().info(f'Received command: {command}')\n\n        # Parse command and create appropriate task\n        task = self.parse_command_to_task(command)\n        if task:\n            self.task_queue.put(task)\n            self.get_logger().info(f'Queued task: {task.id} - {task.description}')\n\n    def voice_command_callback(self, msg):\n        \"\"\"Process voice commands from conversational system\"\"\"\n        voice_command = msg.data.strip()\n        self.get_logger().info(f'Received voice command: {voice_command}')\n\n        # Convert voice command to task\n        task = self.parse_voice_command_to_task(voice_command)\n        if task:\n            self.task_queue.put(task)\n            self.get_logger().info(f'Queued voice task: {task.id} - {task.description}')\n\n    def parse_command_to_task(self, command):\n        \"\"\"Parse command string into Task object\"\"\"\n        command_lower = command.lower()\n\n        if 'navigate' in command_lower or 'go to' in command_lower:\n            # Extract destination\n            destination = self.extract_destination_from_command(command)\n            return Task(\n                id=f\"nav_{int(time.time())}\",\n                type='navigation',\n                description=f'Navigate to {destination}',\n                priority=5,\n                state=TaskState.IDLE,\n                created_time=time.time(),\n                resources_required=['navigation', 'perception']\n            )\n\n        elif 'pick up' in command_lower or 'grasp' in command_lower:\n            # Extract object\n            obj = self.extract_object_from_command(command)\n            return Task(\n                id=f\"manip_{int(time.time())}\",\n                type='manipulation',\n                description=f'Pick up {obj}',\n                priority=6,\n                state=TaskState.IDLE,\n                created_time=time.time(),\n                resources_required=['manipulation', 'perception']\n            )\n\n        elif 'inspect' in command_lower or 'look at' in command_lower:\n            # Extract target\n            target = self.extract_target_from_command(command)\n            return Task(\n                id=f\"inspect_{int(time.time())}\",\n                type='perception',\n                description=f'Inspect {target}',\n                priority=4,\n                state=TaskState.IDLE,\n                created_time=time.time(),\n                resources_required=['perception']\n            )\n\n        else:\n            # Generic task for unrecognized commands\n            return Task(\n                id=f\"generic_{int(time.time())}\",\n                type='generic',\n                description=command,\n                priority=3,\n                state=TaskState.IDLE,\n                created_time=time.time(),\n                resources_required=['communication']\n            )\n\n    def parse_voice_command_to_task(self, command):\n        \"\"\"Parse voice command to Task object with enhanced NLP processing\"\"\"\n        # In a real implementation, this would use more sophisticated NLP\n        # For this example, we'll use the same parser\n        return self.parse_command_to_task(command)\n\n    def extract_destination_from_command(self, command):\n        \"\"\"Extract destination from navigation command\"\"\"\n        # Simple keyword extraction\n        keywords = ['kitchen', 'living room', 'bedroom', 'office', 'lab', 'workshop', 'entrance', 'exit']\n        for keyword in keywords:\n            if keyword in command.lower():\n                return keyword\n        return 'specified location'\n\n    def extract_object_from_command(self, command):\n        \"\"\"Extract object from manipulation command\"\"\"\n        # Simple object extraction\n        objects = ['cup', 'book', 'phone', 'laptop', 'bottle', 'box', 'ball', 'toy']\n        for obj in objects:\n            if obj in command.lower():\n                return obj\n        return 'target object'\n\n    def extract_target_from_command(self, command):\n        \"\"\"Extract target from inspection command\"\"\"\n        # Simple target extraction\n        targets = ['object', 'area', 'room', 'person', 'thing', 'item']\n        for target in targets:\n            if target in command.lower():\n                return target\n        return 'specified target'\n\n    def system_monitoring_callback(self):\n        \"\"\"Monitor system health and safety status\"\"\"\n        # Check component health\n        for component, status in self.component_status.items():\n            if not status:\n                self.get_logger().warn(f'{component} component is offline')\n\n        # Check task execution\n        active_count = sum(1 for task in self.active_tasks.values() if task.state == TaskState.EXECUTING)\n        if active_count > self.max_concurrent_tasks:\n            self.get_logger().warn(f'Too many concurrent tasks: {active_count}')\n\n        # Check safety systems\n        if not self.safety_status:\n            self.get_logger().error('Safety system override detected')\n            self.emergency_stop()\n\n        # Publish system status\n        status_msg = String()\n        status_msg.data = f\"State: {self.robot_state.name}, Active Tasks: {len(self.active_tasks)}, Health: {self.system_health}\"\n        self.status_pub.publish(status_msg)\n\n    def task_execution_callback(self):\n        \"\"\"Execute tasks from the queue\"\"\"\n        # Check for completed tasks\n        for task_id, task in list(self.active_tasks.items()):\n            if task.state == TaskState.COMPLETED or task.state == TaskState.FAILED:\n                if task.state == TaskState.COMPLETED:\n                    self.completed_tasks.append(task)\n                else:\n                    self.failed_tasks.append(task)\n                del self.active_tasks[task_id]\n\n        # Execute new tasks if possible\n        if not self.task_queue.empty() and len(self.active_tasks) < self.max_concurrent_tasks:\n            try:\n                task = self.task_queue.get_nowait()\n                if self.can_execute_task(task):\n                    self.execute_task(task)\n                else:\n                    # Put back in queue if resources not available\n                    self.task_queue.put(task)\n            except queue.Empty:\n                pass\n\n    def can_execute_task(self, task):\n        \"\"\"Check if task can be executed given current resources\"\"\"\n        if not self.safety_status:\n            return False\n\n        # Check if required resources are available\n        for resource in task.resources_required or []:\n            if resource in self.component_status and not self.component_status[resource]:\n                return False\n\n        # Check robot state\n        if self.robot_state != RobotState.READY:\n            return False\n\n        return True\n\n    def execute_task(self, task):\n        \"\"\"Execute a task\"\"\"\n        self.get_logger().info(f'Executing task: {task.id} - {task.description}')\n\n        # Update task state\n        task.state = TaskState.EXECUTING\n        task.start_time = time.time()\n        self.active_tasks[task.id] = task\n\n        # Publish task status\n        task_status_msg = String()\n        task_status_msg.data = f\"EXECUTING: {task.description}\"\n        self.task_status_pub.publish(task_status_msg)\n\n        # Execute based on task type\n        if task.type == 'navigation':\n            self.execute_navigation_task(task)\n        elif task.type == 'manipulation':\n            self.execute_manipulation_task(task)\n        elif task.type == 'perception':\n            self.execute_perception_task(task)\n        else:\n            self.execute_generic_task(task)\n\n    def execute_navigation_task(self, task):\n        \"\"\"Execute navigation task\"\"\"\n        self.get_logger().info(f'Executing navigation task: {task.description}')\n\n        # In a real implementation, this would interface with navigation stack\n        # For this example, we'll simulate navigation\n        import threading\n        threading.Timer(10.0, lambda: self.complete_task(task.id, TaskState.COMPLETED)).start()\n\n    def execute_manipulation_task(self, task):\n        \"\"\"Execute manipulation task\"\"\"\n        self.get_logger().info(f'Executing manipulation task: {task.description}')\n\n        # In a real implementation, this would interface with manipulation stack\n        # For this example, we'll simulate manipulation\n        import threading\n        threading.Timer(15.0, lambda: self.complete_task(task.id, TaskState.COMPLETED)).start()\n\n    def execute_perception_task(self, task):\n        \"\"\"Execute perception task\"\"\"\n        self.get_logger().info(f'Executing perception task: {task.description}')\n\n        # In a real implementation, this would interface with perception system\n        # For this example, we'll simulate perception\n        import threading\n        threading.Timer(5.0, lambda: self.complete_task(task.id, TaskState.COMPLETED)).start()\n\n    def execute_generic_task(self, task):\n        \"\"\"Execute generic task\"\"\"\n        self.get_logger().info(f'Executing generic task: {task.description}')\n\n        # For this example, we'll simulate generic task completion\n        import threading\n        threading.Timer(3.0, lambda: self.complete_task(task.id, TaskState.COMPLETED)).start()\n\n    def complete_task(self, task_id, state):\n        \"\"\"Complete a task with specified state\"\"\"\n        if task_id in self.active_tasks:\n            task = self.active_tasks[task_id]\n            task.state = state\n            task.completion_time = time.time()\n\n            # Log completion\n            if state == TaskState.COMPLETED:\n                self.get_logger().info(f'Task completed: {task_id}')\n            else:\n                self.get_logger().warn(f'Task failed: {task_id}')\n\n    def perception_processing_loop(self):\n        \"\"\"Background thread for perception processing\"\"\"\n        while rclpy.ok():\n            # Process perception data in background\n            time.sleep(0.1)  # Simulate processing delay\n\n    def odom_callback(self, msg):\n        \"\"\"Update robot pose from odometry\"\"\"\n        # Update internal state\n        pass\n\n    def scan_callback(self, msg):\n        \"\"\"Process laser scan data\"\"\"\n        # Update internal state\n        pass\n\n    def joint_state_callback(self, msg):\n        \"\"\"Process joint state data\"\"\"\n        # Update internal state\n        pass\n\n    def camera_callback(self, msg):\n        \"\"\"Process camera data\"\"\"\n        # Update internal state\n        pass\n\n    def emergency_stop(self):\n        \"\"\"Emergency stop procedure\"\"\"\n        self.get_logger().error('EMERGENCY STOP ACTIVATED')\n\n        # Stop all robot motion\n        stop_cmd = Twist()\n        self.cmd_vel_pub.publish(stop_cmd)\n\n        # Update robot state\n        self.robot_state = RobotState.SAFETY_STOP\n\n        # Publish alert\n        alert_msg = String()\n        alert_msg.data = \"EMERGENCY_STOP: All systems halted\"\n        self.system_alert_pub.publish(alert_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CapstoneMainControlNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down capstone system...')\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"capstone-integration-test-suite",children:"Capstone Integration Test Suite"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import unittest\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nimport time\n\nclass CapstoneIntegrationTest(unittest.TestCase):\n    def setUp(self):\n        """Set up test environment"""\n        rclpy.init()\n        self.test_node = Node(\'capstone_integration_test\')\n\n        # Publishers for testing\n        self.command_pub = self.test_node.create_publisher(String, \'/capstone/command\', 10)\n        self.navigation_goal_pub = self.test_node.create_publisher(Pose, \'/navigation/goal\', 10)\n\n        # Subscribers for verification\n        self.status_messages = []\n        self.task_status_messages = []\n\n        self.status_sub = self.test_node.create_subscription(\n            String, \'/capstone/status\', self.status_callback, 10\n        )\n        self.task_status_sub = self.test_node.create_subscription(\n            String, \'/capstone/task_status\', self.task_status_callback, 10\n        )\n\n    def status_callback(self, msg):\n        """Collect status messages for verification"""\n        self.status_messages.append(msg.data)\n\n    def task_status_callback(self, msg):\n        """Collect task status messages for verification"""\n        self.task_status_messages.append(msg.data)\n\n    def tearDown(self):\n        """Clean up after tests"""\n        self.test_node.destroy_node()\n        rclpy.shutdown()\n\n    def test_basic_navigation_task(self):\n        """Test basic navigation task execution"""\n        # Send navigation command\n        command_msg = String()\n        command_msg.data = "navigate to kitchen"\n        self.command_pub.publish(command_msg)\n\n        # Wait for system to process\n        time.sleep(2.0)\n\n        # Verify system received and processed command\n        self.assertTrue(len(self.task_status_messages) > 0)\n        self.assertTrue(any("EXECUTING" in msg for msg in self.task_status_messages))\n        self.assertTrue("State: READY" in self.status_messages[-1])\n\n    def test_system_monitoring(self):\n        """Test system monitoring functionality"""\n        # Wait for monitoring to run\n        time.sleep(2.0)\n\n        # Verify monitoring messages are published\n        self.assertTrue(len(self.status_messages) > 0)\n        self.assertIn("State:", self.status_messages[-1])\n        self.assertIn("Active Tasks:", self.status_messages[-1])\n\n    def test_multimodal_integration(self):\n        """Test integration of multiple modalities"""\n        # This would test the integration of vision, language, and action\n        # For this example, we\'ll verify the system can accept different command types\n        commands = [\n            "navigate to living room",\n            "pick up cup",\n            "inspect object"\n        ]\n\n        for cmd in commands:\n            command_msg = String()\n            command_msg.data = cmd\n            self.command_pub.publish(command_msg)\n            time.sleep(0.5)\n\n        # Verify all commands were processed\n        self.assertTrue(len(self.task_status_messages) >= len(commands))\n\nclass CapstonePerformanceTest(unittest.TestCase):\n    def setUp(self):\n        """Set up performance test environment"""\n        rclpy.init()\n        self.test_node = Node(\'capstone_performance_test\')\n\n        # Publishers for performance testing\n        self.command_pub = self.test_node.create_publisher(String, \'/capstone/command\', 10)\n        self.status_messages = []\n\n        self.status_sub = self.test_node.create_subscription(\n            String, \'/capstone/status\', self.status_callback, 10\n        )\n\n    def status_callback(self, msg):\n        """Collect status messages for performance analysis"""\n        self.status_messages.append((time.time(), msg.data))\n\n    def tearDown(self):\n        """Clean up after performance tests"""\n        self.test_node.destroy_node()\n        rclpy.shutdown()\n\n    def test_concurrent_task_handling(self):\n        """Test system\'s ability to handle concurrent tasks"""\n        start_time = time.time()\n\n        # Send multiple commands quickly\n        for i in range(5):\n            command_msg = String()\n            command_msg.data = f"navigate to location_{i}"\n            self.command_pub.publish(command_msg)\n            time.sleep(0.1)  # Small delay between commands\n\n        # Wait for processing\n        time.sleep(3.0)\n\n        elapsed_time = time.time() - start_time\n        print(f"Concurrent task handling time: {elapsed_time:.2f}s")\n\n        # Verify system handled multiple tasks\n        self.assertTrue(len(self.status_messages) > 0)\n\ndef run_integration_tests():\n    """Run all integration tests"""\n    print("Running Capstone Integration Tests...")\n\n    # Create test suite\n    loader = unittest.TestLoader()\n    suite = unittest.TestSuite()\n\n    # Add tests\n    suite.addTest(loader.loadTestsFromTestCase(CapstoneIntegrationTest))\n    suite.addTest(loader.loadTestsFromTestCase(CapstonePerformanceTest))\n\n    # Run tests\n    runner = unittest.TextTestRunner(verbosity=2)\n    result = runner.run(suite)\n\n    # Print summary\n    print(f"\\nIntegration Tests Result:")\n    print(f"Tests run: {result.testsRun}")\n    print(f"Failures: {len(result.failures)}")\n    print(f"Errors: {len(result.errors)}")\n    print(f"Success: {result.wasSuccessful()}")\n\n    return result.wasSuccessful()\n\nif __name__ == \'__main__\':\n    success = run_integration_tests()\n    exit(0 if success else 1)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"capstone-system-evaluation-node",children:"Capstone System Evaluation Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Float32, Int32\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PoseStamped\nfrom visualization_msgs.msg import MarkerArray\nimport numpy as np\nimport time\nfrom datetime import datetime\nimport json\nimport csv\n\nclass CapstoneSystemEvaluationNode(Node):\n    def __init__(self):\n        super().__init__('capstone_system_evaluation')\n\n        # Publishers\n        self.performance_metrics_pub = self.create_publisher(\n            String, '/capstone/performance_metrics', 10\n        )\n        self.accuracy_metrics_pub = self.create_publisher(\n            String, '/capstone/accuracy_metrics', 10\n        )\n        self.robustness_metrics_pub = self.create_publisher(\n            String, '/capstone/robustness_metrics', 10\n        )\n        self.visualization_pub = self.create_publisher(\n            MarkerArray, '/capstone/evaluation_visualization', 10\n        )\n\n        # Subscribers\n        self.task_status_sub = self.create_subscription(\n            String, '/capstone/task_status', self.task_status_callback, 10\n        )\n        self.system_status_sub = self.create_subscription(\n            String, '/capstone/status', self.system_status_callback, 10\n        )\n        self.command_sub = self.create_subscription(\n            String, '/capstone/command', self.command_callback, 10\n        )\n\n        # Evaluation state\n        self.evaluation_data = {\n            'task_success_rate': 0.0,\n            'average_completion_time': 0.0,\n            'system_uptime': 0.0,\n            'response_time': [],\n            'task_completions': [],\n            'failures': [],\n            'accuracy_metrics': {},\n            'robustness_metrics': {}\n        }\n\n        # Timing and measurement\n        self.command_timestamps = {}\n        self.task_start_times = {}\n        self.evaluation_start_time = time.time()\n\n        # Evaluation parameters\n        self.evaluation_window = 300.0  # 5 minutes\n        self.metrics_collection_interval = 10.0  # 10 seconds\n\n        # Timers\n        self.metrics_timer = self.create_timer(\n            self.metrics_collection_interval, self.collect_metrics\n        )\n        self.reporting_timer = self.create_timer(\n            60.0, self.generate_evaluation_report  # Report every minute\n        )\n\n        # CSV file for detailed logging\n        self.csv_filename = f\"capstone_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n        self.csv_fields = [\n            'timestamp', 'metric_type', 'metric_name', 'value', 'unit'\n        ]\n        self.init_csv_logging()\n\n        self.get_logger().info('Capstone System Evaluation node initialized')\n\n    def init_csv_logging(self):\n        \"\"\"Initialize CSV file for detailed metric logging\"\"\"\n        with open(self.csv_filename, 'w', newline='') as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=self.csv_fields)\n            writer.writeheader()\n            csvfile.flush()\n\n    def log_metric_to_csv(self, metric_type, metric_name, value, unit=''):\n        \"\"\"Log metric to CSV file\"\"\"\n        with open(self.csv_filename, 'a', newline='') as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=self.csv_fields)\n            writer.writerow({\n                'timestamp': datetime.now().isoformat(),\n                'metric_type': metric_type,\n                'metric_name': metric_name,\n                'value': value,\n                'unit': unit\n            })\n            csvfile.flush()\n\n    def task_status_callback(self, msg):\n        \"\"\"Process task status updates for evaluation\"\"\"\n        status_text = msg.data\n\n        # Parse task status\n        if \"EXECUTING:\" in status_text:\n            task_desc = status_text.replace(\"EXECUTING: \", \"\")\n            self.task_start_times[task_desc] = time.time()\n        elif \"COMPLETED\" in status_text or \"FAILED\" in status_text:\n            # Calculate completion time and update metrics\n            for task_desc, start_time in list(self.task_start_times.items()):\n                if task_desc in status_text:\n                    completion_time = time.time() - start_time\n                    self.evaluation_data['task_completions'].append(completion_time)\n\n                    # Log to CSV\n                    self.log_metric_to_csv(\n                        'task', 'completion_time', completion_time, 'seconds'\n                    )\n\n                    # Remove from tracking\n                    del self.task_start_times[task_desc]\n\n    def system_status_callback(self, msg):\n        \"\"\"Process system status updates for evaluation\"\"\"\n        # Extract system status information\n        status_parts = msg.data.split(', ')\n        for part in status_parts:\n            if 'State:' in part:\n                state = part.split(': ')[1]\n                if state == 'ERROR':\n                    self.evaluation_data['failures'].append(time.time())\n                    self.log_metric_to_csv(\n                        'system', 'error_occurred', 1, 'count'\n                    )\n\n    def command_callback(self, msg):\n        \"\"\"Track command response times\"\"\"\n        command = msg.data\n        self.command_timestamps[command] = time.time()\n\n        # Log command received\n        self.log_metric_to_csv(\n            'command', 'received', 1, 'count'\n        )\n\n    def collect_metrics(self):\n        \"\"\"Collect and calculate evaluation metrics\"\"\"\n        current_time = time.time()\n        evaluation_duration = current_time - self.evaluation_start_time\n\n        # Calculate task success rate\n        total_tasks = len(self.evaluation_data['task_completions']) + len(self.evaluation_data['failures'])\n        if total_tasks > 0:\n            success_rate = len(self.evaluation_data['task_completions']) / total_tasks\n            self.evaluation_data['task_success_rate'] = success_rate\n            self.log_metric_to_csv(\n                'performance', 'success_rate', success_rate, 'ratio'\n            )\n\n        # Calculate average completion time\n        if self.evaluation_data['task_completions']:\n            avg_time = np.mean(self.evaluation_data['task_completions'])\n            self.evaluation_data['average_completion_time'] = avg_time\n            self.log_metric_to_csv(\n                'performance', 'avg_completion_time', avg_time, 'seconds'\n            )\n\n        # Calculate system uptime\n        uptime = evaluation_duration - self.calculate_downtime()\n        self.evaluation_data['system_uptime'] = uptime / evaluation_duration if evaluation_duration > 0 else 0\n        self.log_metric_to_csv(\n            'reliability', 'uptime_ratio', self.evaluation_data['system_uptime'], 'ratio'\n        )\n\n        # Publish metrics\n        self.publish_current_metrics()\n\n    def calculate_downtime(self):\n        \"\"\"Calculate system downtime based on failures\"\"\"\n        # In a real system, this would track actual downtime\n        # For this example, we'll estimate based on failure frequency\n        if not self.evaluation_data['failures']:\n            return 0.0\n\n        # Simple calculation: assume each failure causes 10 seconds of downtime\n        return len(self.evaluation_data['failures']) * 10.0\n\n    def publish_current_metrics(self):\n        \"\"\"Publish current evaluation metrics\"\"\"\n        # Performance metrics\n        perf_msg = String()\n        perf_msg.data = json.dumps({\n            'task_success_rate': self.evaluation_data['task_success_rate'],\n            'average_completion_time': self.evaluation_data['average_completion_time'],\n            'system_uptime': self.evaluation_data['system_uptime'],\n            'total_tasks_completed': len(self.evaluation_data['task_completions']),\n            'total_failures': len(self.evaluation_data['failures'])\n        })\n        self.performance_metrics_pub.publish(perf_msg)\n\n        # Accuracy metrics (placeholder - in real system would be perception accuracy)\n        acc_msg = String()\n        acc_msg.data = json.dumps({\n            'object_detection_accuracy': 0.95,\n            'pose_estimation_accuracy': 0.92,\n            'command_understanding_accuracy': 0.88\n        })\n        self.accuracy_metrics_pub.publish(acc_msg)\n\n        # Robustness metrics\n        rob_msg = String()\n        rob_msg.data = json.dumps({\n            'failure_recovery_rate': 0.99,\n            'safe_operation_percentage': 0.995,\n            'graceful_degradation_score': 0.85\n        })\n        self.robustness_metrics_pub.publish(rob_msg)\n\n    def generate_evaluation_report(self):\n        \"\"\"Generate comprehensive evaluation report\"\"\"\n        current_time = time.time()\n        evaluation_duration = current_time - self.evaluation_start_time\n\n        report = {\n            'evaluation_period': evaluation_duration,\n            'timestamp': datetime.now().isoformat(),\n            'performance': {\n                'task_success_rate': self.evaluation_data['task_success_rate'],\n                'average_completion_time': self.evaluation_data['average_completion_time'],\n                'system_uptime': self.evaluation_data['system_uptime'],\n                'total_tasks_processed': len(self.evaluation_data['task_completions']) + len(self.evaluation_data['failures'])\n            },\n            'accuracy': {\n                'object_detection_accuracy': 0.95,\n                'pose_estimation_accuracy': 0.92,\n                'command_understanding_accuracy': 0.88\n            },\n            'robustness': {\n                'failure_recovery_rate': 0.99,\n                'safe_operation_percentage': 0.995,\n                'graceful_degradation_score': 0.85\n            },\n            'summary': self.generate_summary_evaluation()\n        }\n\n        self.get_logger().info(f\"Evaluation Report:\\n{json.dumps(report, indent=2)}\")\n\n    def generate_summary_evaluation(self):\n        \"\"\"Generate summary evaluation of system performance\"\"\"\n        # Calculate overall performance score\n        perf_score = self.evaluation_data['task_success_rate'] * 0.4 + \\\n                    (1.0 - min(1.0, self.evaluation_data['average_completion_time'] / 60.0)) * 0.3 + \\\n                    self.evaluation_data['system_uptime'] * 0.3\n\n        # Determine grade\n        if perf_score >= 0.9:\n            grade = \"Excellent\"\n        elif perf_score >= 0.8:\n            grade = \"Good\"\n        elif perf_score >= 0.7:\n            grade = \"Fair\"\n        else:\n            grade = \"Needs Improvement\"\n\n        return {\n            'overall_score': perf_score,\n            'grade': grade,\n            'strengths': self.identify_strengths(),\n            'improvements_needed': self.identify_improvements()\n        }\n\n    def identify_strengths(self):\n        \"\"\"Identify system strengths based on metrics\"\"\"\n        strengths = []\n        if self.evaluation_data['task_success_rate'] > 0.9:\n            strengths.append(\"High task completion success rate\")\n        if self.evaluation_data['system_uptime'] > 0.95:\n            strengths.append(\"Excellent system reliability\")\n        if self.evaluation_data['average_completion_time'] < 30.0:\n            strengths.append(\"Fast task execution\")\n        return strengths\n\n    def identify_improvements(self):\n        \"\"\"Identify areas needing improvement\"\"\"\n        improvements = []\n        if self.evaluation_data['task_success_rate'] < 0.8:\n            improvements.append(\"Task completion rate needs improvement\")\n        if self.evaluation_data['average_completion_time'] > 60.0:\n            improvements.append(\"Task execution time is too slow\")\n        if len(self.evaluation_data['failures']) > 0:\n            improvements.append(\"System stability needs improvement\")\n        return improvements\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CapstoneSystemEvaluationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down evaluation system...')\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"practical-lab--simulation",children:"Practical Lab / Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"lab-exercise-1-system-integration",children:"Lab Exercise 1: System Integration"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Integrate all modules (ROS 2, Simulation, NVIDIA Isaac, VLA) into one system"}),"\n",(0,a.jsx)(n.li,{children:"Test communication between all components"}),"\n",(0,a.jsx)(n.li,{children:"Validate data flow and synchronization"}),"\n",(0,a.jsx)(n.li,{children:"Identify and resolve integration issues"}),"\n",(0,a.jsx)(n.li,{children:"Document system architecture and interfaces"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lab-exercise-2-task-orchestration",children:"Lab Exercise 2: Task Orchestration"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Implement the Capstone Main Control Node"}),"\n",(0,a.jsx)(n.li,{children:"Test task planning and execution"}),"\n",(0,a.jsx)(n.li,{children:"Validate resource management and scheduling"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate system monitoring and safety features"}),"\n",(0,a.jsx)(n.li,{children:"Test with various task sequences and priorities"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lab-exercise-3-performance-evaluation",children:"Lab Exercise 3: Performance Evaluation"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Implement the Capstone System Evaluation Node"}),"\n",(0,a.jsx)(n.li,{children:"Run comprehensive system tests"}),"\n",(0,a.jsx)(n.li,{children:"Collect performance, accuracy, and robustness metrics"}),"\n",(0,a.jsx)(n.li,{children:"Analyze system behavior under various conditions"}),"\n",(0,a.jsx)(n.li,{children:"Generate evaluation reports and recommendations"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lab-exercise-4-integration-testing",children:"Lab Exercise 4: Integration Testing"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Run the Capstone Integration Test Suite"}),"\n",(0,a.jsx)(n.li,{children:"Test multimodal interaction scenarios"}),"\n",(0,a.jsx)(n.li,{children:"Validate safety and error handling"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate system reliability and fault tolerance"}),"\n",(0,a.jsx)(n.li,{children:"Document test results and coverage"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lab-exercise-5-real-world-validation",children:"Lab Exercise 5: Real-world Validation"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Deploy system on physical robot platform"}),"\n",(0,a.jsx)(n.li,{children:"Test in real-world environments"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate human-robot interaction quality"}),"\n",(0,a.jsx)(n.li,{children:"Assess system performance in unstructured settings"}),"\n",(0,a.jsx)(n.li,{children:"Document lessons learned and system limitations"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lab-exercise-6-system-optimization",children:"Lab Exercise 6: System Optimization"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Profile system for performance bottlenecks"}),"\n",(0,a.jsx)(n.li,{children:"Optimize resource utilization"}),"\n",(0,a.jsx)(n.li,{children:"Improve response times and throughput"}),"\n",(0,a.jsx)(n.li,{children:"Enhance system reliability and safety"}),"\n",(0,a.jsx)(n.li,{children:"Document optimal configurations and best practices"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"real-world-mapping",children:"Real-World Mapping"}),"\n",(0,a.jsx)(n.h3,{id:"industrial-applications",children:"Industrial Applications"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Manufacturing"}),": Integrated systems for flexible automation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Logistics"}),": Complete warehouse automation solutions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Healthcare"}),": Assistive robots for patient care"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"research-applications",children:"Research Applications"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Humanoid Robotics"}),": Complete humanoid robot systems"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Autonomous Systems"}),": Integrated perception-action systems"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Human-Robot Interaction"}),": Natural interaction research platforms"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"key-success-factors",children:"Key Success Factors"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"System Integration"}),": Seamless coordination between all components"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reliability"}),": Consistent performance in real-world conditions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Safety"}),": Safe operation in human environments"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Usability"}),": Intuitive interfaces for end users"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scalability"}),": Ability to adapt to different applications"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"Chapter 3 has completed the Physical AI & Humanoid Robotics textbook with a comprehensive Capstone Project that integrates all concepts learned throughout the course. We've demonstrated how to combine perception, cognition, and action into a unified Physical AI system that can interact with the real world. The capstone project showcases the integration of ROS 2, NVIDIA Isaac, Vision-Language-Action systems, and conversational robotics into a cohesive humanoid robot application. The examples provided practical implementations of system integration, task orchestration, and evaluation methodologies. The hands-on lab exercises offer experience with building, testing, and validating complete Physical AI systems. This concludes the textbook with a demonstration of how all individual components work together to create sophisticated, embodied AI systems capable of complex real-world interactions."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}}}]);