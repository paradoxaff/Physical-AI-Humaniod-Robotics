"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[712],{1600:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"modules/module-01-ros2/physical-ai-robotic-nervous-system","title":"Lesson 1 - Physical AI and the Robotic Nervous System","description":"Learning Objectives","source":"@site/docs/modules/module-01-ros2/physical-ai-robotic-nervous-system.mdx","sourceDirName":"modules/module-01-ros2","slug":"/modules/module-01-ros2/physical-ai-robotic-nervous-system","permalink":"/docs/modules/module-01-ros2/physical-ai-robotic-nervous-system","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-01-ros2/physical-ai-robotic-nervous-system.mdx","tags":[],"version":"current","frontMatter":{"title":"Lesson 1 - Physical AI and the Robotic Nervous System","sidebar_label":"Physical AI and the Robotic Nervous System"},"sidebar":"tutorialSidebar","previous":{"title":"Nodes, Topics, and Services","permalink":"/docs/modules/module-01-ros2/nodes-topics-services"},"next":{"title":"Python Agents with rclpy","permalink":"/docs/modules/module-01-ros2/python-agents-rclpy"}}');var i=o(4848),t=o(8453);const r={title:"Lesson 1 - Physical AI and the Robotic Nervous System",sidebar_label:"Physical AI and the Robotic Nervous System"},a="Lesson 1: Physical AI and the Robotic Nervous System",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Physical AI Concept",id:"physical-ai-concept",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"The Robotic Nervous System Architecture",id:"the-robotic-nervous-system-architecture",level:3},{value:"Communication Patterns in the Robotic Nervous System",id:"communication-patterns-in-the-robotic-nervous-system",level:3},{value:"Tools &amp; Software",id:"tools--software",level:2},{value:"Code / Configuration Examples",id:"code--configuration-examples",level:2},{value:"Implementing a Sensory Node (Camera)",id:"implementing-a-sensory-node-camera",level:3},{value:"Implementing a Processing Node (Perception)",id:"implementing-a-processing-node-perception",level:3},{value:"Implementing an Action Node (Motor Control)",id:"implementing-an-action-node-motor-control",level:3},{value:"Practical Lab / Simulation",id:"practical-lab--simulation",level:2},{value:"Lab Exercise 1: Robotic Nervous System Simulation",id:"lab-exercise-1-robotic-nervous-system-simulation",level:3},{value:"Lab Exercise 2: Communication Analysis",id:"lab-exercise-2-communication-analysis",level:3},{value:"Real-World Mapping",id:"real-world-mapping",level:2},{value:"Biological Inspiration",id:"biological-inspiration",level:3},{value:"Industrial Applications",id:"industrial-applications",level:3},{value:"Research Platforms",id:"research-platforms",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"lesson-1-physical-ai-and-the-robotic-nervous-system",children:"Lesson 1: Physical AI and the Robotic Nervous System"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Define Physical AI and its relationship to embodied intelligence"}),"\n",(0,i.jsx)(n.li,{children:'Explain how ROS 2 serves as the "nervous system" for robotic systems'}),"\n",(0,i.jsx)(n.li,{children:"Identify the key components of a robotic nervous system"}),"\n",(0,i.jsx)(n.li,{children:"Understand the communication patterns between sensors, controllers, and actuators"}),"\n",(0,i.jsx)(n.li,{children:"Appreciate the role of distributed architecture in Physical AI systems"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"physical-ai-concept",children:"Physical AI Concept"}),"\n",(0,i.jsx)(n.p,{children:'Physical AI represents the convergence of artificial intelligence with the physical world through embodied systems. Unlike traditional AI that operates on abstract data, Physical AI systems must perceive, reason, and act in real-time physical environments. The "nervous system" metaphor captures how information flows through a robot\u2014sensory inputs are processed, decisions are made, and motor commands are executed, all coordinated through a distributed communication architecture.'}),"\n",(0,i.jsx)(n.p,{children:"Physical AI systems must handle:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time constraints"}),": Decisions and actions must occur within time bounds"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Uncertainty"}),": Sensor data is noisy, environments are dynamic"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Embodiment"}),": Physical form affects capabilities and constraints"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Interaction"}),": Robots must safely and effectively engage with the physical world"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"the-robotic-nervous-system-architecture",children:"The Robotic Nervous System Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502              Central Brain                      \u2502\n                    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n                    \u2502  \u2502  Perception     \u2502  \u2502  Decision Making    \u2502  \u2502\n                    \u2502  \u2502  \u2022 Object       \u2502  \u2502  \u2022 Behavior         \u2502  \u2502\n                    \u2502  \u2502    Detection    \u2502  \u2502    Selection       \u2502  \u2502\n                    \u2502  \u2502  \u2022 SLAM         \u2502  \u2502  \u2022 Planning         \u2502  \u2502\n                    \u2502  \u2502  \u2022 State        \u2502  \u2502  \u2022 Learning         \u2502  \u2502\n                    \u2502  \u2502    Estimation   \u2502  \u2502    Models          \u2502  \u2502\n                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n                                      \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502           ROS 2 Communication Layer             \u2502\n                    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n                    \u2502  \u2502  QoS        \u2502  \u2502  Topics     \u2502  \u2502  Services\u2502 \u2502\n                    \u2502  \u2502  Policies   \u2502  \u2502  & Actions  \u2502  \u2502  & RPC  \u2502 \u2502\n                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                 \u2502                               \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n        \u2502    Sensory Organs       \u2502   \u2502   \u2502     Motor Organs        \u2502 \u2502\n        \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502   \u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 \u2502\n        \u2502  \u2502  Visual Sensors     \u2502\u2502   \u2502   \u2502  \u2502  Actuator Control   \u2502\u2502 \u2502\n        \u2502  \u2502  \u2022 Cameras          \u2502\u2502   \u2502   \u2502  \u2502  \u2022 Joint Motors     \u2502\u2502 \u2502\n        \u2502  \u2502  \u2022 Depth Sensors    \u2502\u2502   \u2502   \u2502  \u2502  \u2022 Grippers         \u2502\u2502 \u2502\n        \u2502  \u2502  \u2022 Thermal Cameras  \u2502\u2502   \u2502   \u2502  \u2502  \u2022 Lights           \u2502\u2502 \u2502\n        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502   \u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502 \u2502\n        \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502   \u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 \u2502\n        \u2502  \u2502  Physical Sensors   \u2502\u2502   \u2502   \u2502  \u2502  Locomotion         \u2502\u2502 \u2502\n        \u2502  \u2502  \u2022 IMU              \u2502\u2502   \u2502   \u2502  \u2502  \u2022 Wheels           \u2502\u2502 \u2502\n        \u2502  \u2502  \u2022 Force/Torque     \u2502\u2502   \u2502   \u2502  \u2502  \u2022 Legs             \u2502\u2502 \u2502\n        \u2502  \u2502  \u2022 Encoders         \u2502\u2502   \u2502   \u2502  \u2502  \u2022 Tracks           \u2502\u2502 \u2502\n        \u2502  \u2502  \u2022 GPS              \u2502\u2502   \u2502   \u2502  \u2502                     \u2502\u2502 \u2502\n        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502   \u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n                                      \u2502                               \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                   Physical World                        \u2502\n                    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n                    \u2502  \u2502  Environment: Objects, Surfaces, Dynamics, etc.     \u2502 \u2502\n                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h3,{id:"communication-patterns-in-the-robotic-nervous-system",children:"Communication Patterns in the Robotic Nervous System"}),"\n",(0,i.jsx)(n.p,{children:"ROS 2 implements several communication patterns that mirror biological nervous systems:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensory Pathways"}),": High-frequency sensor data flows from sensors to processing nodes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Motor Pathways"}),": Commands flow from decision-making nodes to actuators"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integrative Pathways"}),": Information is combined across multiple sensory modalities"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback Loops"}),": Sensor data provides feedback on the results of actions"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"tools--software",children:"Tools & Software"}),"\n",(0,i.jsx)(n.p,{children:"This lesson uses:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Humble Hawksbill"})," - Core communication framework"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"rclpy"})," - Python client library for node implementation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RViz2"})," - Visualization for understanding data flow"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"rqt"})," - GUI tools for monitoring communication"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gazebo"})," - Simulation environment for testing nervous system patterns"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"code--configuration-examples",children:"Code / Configuration Examples"}),"\n",(0,i.jsx)(n.h3,{id:"implementing-a-sensory-node-camera",children:"Implementing a Sensory Node (Camera)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport cv2\n\nclass CameraSensorNode(Node):\n    def __init__(self):\n        super().__init__('camera_sensor')\n        self.bridge = CvBridge()\n\n        # Publisher for camera images\n        self.image_pub = self.create_publisher(Image, '/sensory/visual/rgb', 10)\n        self.info_pub = self.create_publisher(CameraInfo, '/sensory/visual/camera_info', 10)\n\n        # Simulate camera capture at 30 Hz\n        self.timer = self.create_timer(1.0/30.0, self.capture_callback)\n\n        self.get_logger().info('Camera sensor node initialized')\n\n    def capture_callback(self):\n        # In a real implementation, this would capture from actual camera\n        # For simulation, we create a dummy image\n        dummy_image = self.create_dummy_image()\n        ros_image = self.bridge.cv2_to_imgmsg(dummy_image, encoding='bgr8')\n\n        # Publish image data\n        self.image_pub.publish(ros_image)\n\n        # Publish camera info\n        camera_info = self.create_camera_info()\n        self.info_pub.publish(camera_info)\n\n        self.get_logger().debug('Published camera data')\n\n    def create_dummy_image(self):\n        # Create a dummy image for simulation\n        img = 255 * np.ones((480, 640, 3), dtype=np.uint8)\n        # Add some visual features\n        cv2.rectangle(img, (100, 100), (200, 200), (0, 0, 255), 2)\n        cv2.putText(img, 'Physical AI', (120, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n        return img\n\n    def create_camera_info(self):\n        # Create camera info message\n        info = CameraInfo()\n        info.header.stamp = self.get_clock().now().to_msg()\n        info.header.frame_id = 'camera_frame'\n        info.width = 640\n        info.height = 480\n        info.k = [500.0, 0.0, 320.0,  # fx, 0, cx\n                  0.0, 500.0, 240.0,  # 0, fy, cy\n                  0.0, 0.0, 1.0]      # 0, 0, 1\n        return info\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CameraSensorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"implementing-a-processing-node-perception",children:"Implementing a Processing Node (Perception)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Point\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass PerceptionNode(Node):\n    def __init__(self):\n        super().__init__('perception_node')\n        self.bridge = CvBridge()\n\n        # Subscribe to camera data\n        self.image_sub = self.create_subscription(\n            Image,\n            '/sensory/visual/rgb',\n            self.image_callback,\n            10\n        )\n\n        # Publish detected objects\n        self.object_pub = self.create_publisher(Point, '/perception/objects/position', 10)\n\n        self.get_logger().info('Perception node initialized')\n\n    def image_callback(self, msg):\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Simple color-based object detection (red rectangle from camera example)\n            hsv = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV)\n            lower_red = np.array([0, 100, 100])\n            upper_red = np.array([10, 255, 255])\n            mask = cv2.inRange(hsv, lower_red, upper_red)\n\n            # Find contours\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            if contours:\n                # Find the largest contour\n                largest_contour = max(contours, key=cv2.contourArea)\n                if cv2.contourArea(largest_contour) > 100:  # Minimum area threshold\n                    # Calculate centroid\n                    M = cv2.moments(largest_contour)\n                    if M[\"m00\"] != 0:\n                        cx = int(M[\"m10\"] / M[\"m00\"])\n                        cy = int(M[\"m01\"] / M[\"m00\"])\n\n                        # Publish object position\n                        point = Point()\n                        point.x = float(cx)\n                        point.y = float(cy)\n                        point.z = 0.0  # Depth would come from stereo/depth camera\n\n                        self.object_pub.publish(point)\n                        self.get_logger().info(f'Detected object at ({cx}, {cy})')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PerceptionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"implementing-an-action-node-motor-control",children:"Implementing an Action Node (Motor Control)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Point\nfrom std_msgs.msg import String\nimport math\n\nclass MotorControlNode(Node):\n    def __init__(self):\n        super().__init__('motor_control')\n\n        # Subscribe to object positions from perception\n        self.object_sub = self.create_subscription(\n            Point,\n            '/perception/objects/position',\n            self.object_callback,\n            10\n        )\n\n        # Subscribe to high-level commands\n        self.command_sub = self.create_subscription(\n            String,\n            '/behavior/command',\n            self.command_callback,\n            10\n        )\n\n        # Publish motor commands\n        self.motor_pub = self.create_publisher(String, '/motor/commands', 10)\n\n        # State variables\n        self.current_object_pos = None\n        self.following_object = False\n\n        self.get_logger().info('Motor control node initialized')\n\n    def object_callback(self, msg):\n        self.current_object_pos = (msg.x, msg.y)\n\n        # If following object mode is active, move toward it\n        if self.following_object and self.current_object_pos:\n            self.move_to_object()\n\n    def command_callback(self, msg):\n        if msg.data == 'follow_object':\n            self.following_object = True\n            self.get_logger().info('Starting object following mode')\n        elif msg.data == 'stop':\n            self.following_object = False\n            self.stop_motors()\n            self.get_logger().info('Stopping motors')\n\n    def move_to_object(self):\n        if not self.current_object_pos:\n            return\n\n        target_x, target_y = self.current_object_pos\n        # In a real system, this would calculate motor commands based on\n        # robot kinematics and current position\n        command = f\"move_to({target_x}, {target_y})\"\n\n        motor_cmd = String()\n        motor_cmd.data = command\n        self.motor_pub.publish(motor_cmd)\n\n        self.get_logger().info(f'Sending motor command: {command}')\n\n    def stop_motors(self):\n        stop_cmd = String()\n        stop_cmd.data = \"stop\"\n        self.motor_pub.publish(stop_cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MotorControlNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"practical-lab--simulation",children:"Practical Lab / Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"lab-exercise-1-robotic-nervous-system-simulation",children:"Lab Exercise 1: Robotic Nervous System Simulation"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create three separate ROS 2 packages for each component:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sensory_nodes"})," - Contains camera and other sensor simulators"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"perception_nodes"})," - Contains processing and perception algorithms"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"motor_nodes"})," - Contains motor control and actuator interfaces"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Implement the three nodes as shown in the examples above"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create a launch file that starts all three nodes simultaneously:"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<launch>\n  <node pkg="sensory_nodes" exec="camera_sensor" name="camera_sensor"/>\n  <node pkg="perception_nodes" exec="perception_node" name="perception_node"/>\n  <node pkg="motor_nodes" exec="motor_control" name="motor_control"/>\n</launch>\n'})}),"\n",(0,i.jsxs)(n.ol,{start:"4",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Run the system and observe the data flow using ROS 2 tools:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"ros2 topic list"})," - View available topics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"ros2 topic echo /perception/objects/position"})," - Monitor object detections"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"ros2 topic echo /motor/commands"})," - Monitor motor commands"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Send commands to control the system:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"ros2 topic pub /behavior/command std_msgs/String \"data: 'follow_object'\""})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"ros2 topic pub /behavior/command std_msgs/String \"data: 'stop'\""})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"lab-exercise-2-communication-analysis",children:"Lab Exercise 2: Communication Analysis"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Use ",(0,i.jsx)(n.code,{children:"rqt_graph"})," to visualize the communication graph between nodes"]}),"\n",(0,i.jsxs)(n.li,{children:["Analyze the message rates on different topics using ",(0,i.jsx)(n.code,{children:"ros2 topic hz"})]}),"\n",(0,i.jsx)(n.li,{children:"Experiment with different QoS settings to understand their impact on communication"}),"\n",(0,i.jsxs)(n.li,{children:["Monitor system performance using ",(0,i.jsx)(n.code,{children:"ros2 doctor"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"real-world-mapping",children:"Real-World Mapping"}),"\n",(0,i.jsx)(n.h3,{id:"biological-inspiration",children:"Biological Inspiration"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensory Systems"}),": Like biological sensory organs, robot sensors provide input about the environment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integration Centers"}),": Similar to how the brain integrates multiple sensory inputs, ROS 2 nodes combine information from various sources"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Motor Control"}),": Just as the brain sends motor commands to muscles, ROS 2 sends commands to actuators"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback Loops"}),": Biological reflexes and ROS 2 control loops both provide rapid responses to environmental changes"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"industrial-applications",children:"Industrial Applications"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Autonomous Vehicles"}),": Perception systems process sensor data, planning systems make decisions, and control systems execute driving commands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Manufacturing Robots"}),": Vision systems guide robotic arms, with communication ensuring coordinated motion"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Agricultural Robots"}),": Sensor fusion combines GPS, cameras, and environmental sensors to guide autonomous tractors and harvesters"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"research-platforms",children:"Research Platforms"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Boston Dynamics Robots"}),": Distributed control architecture with sensor processing, planning, and actuation nodes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"NASA's Robonaut"}),": Multi-modal sensing and coordinated manipulation using distributed control"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Social Robots"}),": Integration of speech recognition, emotion detection, and motor control for human-robot interaction"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:'Lesson 1 has established the foundational concept of ROS 2 as the "nervous system" for Physical AI systems. We\'ve explored how the distributed architecture of ROS 2 enables complex robotic behaviors by connecting sensory perception, decision-making, and motor control in a coordinated manner. The code examples demonstrate practical implementations of sensor, perception, and motor control nodes that communicate through ROS 2 topics. The lab exercises provide hands-on experience with creating and connecting these components, mirroring the communication patterns found in biological nervous systems. This understanding is crucial for building more sophisticated Physical AI systems that can effectively perceive, reason, and act in the physical world.'})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>a});var s=o(6540);const i={},t=s.createContext(i);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);