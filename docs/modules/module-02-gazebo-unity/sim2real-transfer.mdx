---
title: Chapter 3 - Sim2Real Transfer
sidebar_label: Sim2Real Transfer
---

# Chapter 3: Sim2Real Transfer

## Learning Objectives

By the end of this chapter, you will be able to:
- Identify and analyze the reality gap between simulation and real-world environments
- Apply domain randomization techniques to improve transferability
- Implement system identification methods to bridge simulation-reality differences
- Design robust control strategies that work in both simulation and reality
- Evaluate transfer performance and identify key factors affecting success
- Apply Sim2Real transfer techniques to humanoid robot applications

## Physical AI Concept

Sim2Real (Simulation-to-Reality) transfer is the critical challenge of ensuring behaviors, policies, and controllers learned in simulation environments effectively transfer to real-world robotic systems. This concept is particularly important in Physical AI and humanoid robotics, where the cost, safety, and time constraints of real-world training make simulation-based learning essential, but the differences between simulated and real environments can prevent successful transfer.

The Sim2Real problem encompasses:
- **Dynamics Mismatch**: Differences in physical properties between simulation and reality
- **Perception Gap**: Variations in sensor data quality and characteristics
- **Actuation Differences**: Discrepancies in motor control and response
- **Environmental Factors**: Unmodeled elements like lighting, texture, and disturbances
- **Model Fidelity**: Trade-offs between simulation accuracy and computational efficiency

## System Architecture

### Sim2Real Transfer Architecture

```
Sim2Real Transfer Framework:
┌─────────────────────────────────────────────────────────────────────┐
│                    Simulation Environment                           │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐    │
│  │   High-Fidelity │  │   Domain        │  │   Curriculum    │    │
│  │   Simulator     │  │   Randomization │  │   Training      │    │
│  │  • Accurate     │  │  • Visual       │  │  • Progressive  │    │
│  │    Physics     │  │  • Dynamics     │  │    Difficulty   │    │
│  │  • Sensor      │  │  • Materials    │  │  • Transfer     │    │
│  │    Modeling    │  │  • Lighting     │  │    Validation   │    │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘    │
│              │                    │                    │           │
│              ▼                    ▼                    ▼           │
│  ┌─────────────────────────────────────────────────────────────┐  │
│  │              Reality Gap Analysis                           │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐        │  │
│  │  │  Dynamics   │  │  Perception │  │  Control    │        │  │
│  │  │  Modeling   │  │  Alignment  │  │  Robustness │        │  │
│  │  │  • Friction  │  │  • Noise    │  │  • Adaptive │        │  │
│  │  │  • Inertia   │  │  • Blur     │  │  • Recovery │        │  │
│  │  │  • Compliance│  │  • Distortion│ │  • Learning │        │  │
│  │  └─────────────┘  └─────────────┘  └─────────────┘        │  │
│  └─────────────────────────────────────────────────────────────┘  │
│                                     │                            │
│                                     ▼                            │
│  ┌─────────────────────────────────────────────────────────────┐  │
│  │              Transfer Validation                            │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐        │  │
│  │  │  Performance│  │  Adaptation │  │  Iterative   │        │  │
│  │  │  Comparison │  │  Mechanisms │  │  Refinement  │        │  │
│  │  │  • Metrics  │  │  • Online   │  │  • Model    │        │  │
│  │  │  • Success  │  │  Adaptation │  │  Improvement │        │  │
│  │  │  Rate       │  │  • Fine-    │  │  • Validation │        │  │
│  │  └─────────────┘  │  Tuning     │  └─────────────┘        │  │
│  └─────────────────────────────────┘─────────────────────────────┘
└─────────────────────────────────────────────────────────────────────┘
                     │
                     ▼
        ┌─────────────────────────────────────────────────────────────┐
        │                   Real Robot                                │
        │  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────┐ │
        │  │   Physical      │  │   Real Sensors  │  │  Control    │ │
        │  │   Hardware      │  │   & Actuators   │  │  Systems    │ │
        │  │  • Humanoid     │  │  • Cameras     │  │  • Balance  │ │
        │  │    Robot        │  │  • LIDAR       │  │  • Locomotion││
        │  │  • Motors       │  │  • IMU         │  │  • Manipulation││
        │  │  • Gears        │  │  • Force/Torque│  │  • Planning │ │
        │  └─────────────────┘  └─────────────────┘  └─────────────┘ │
        └─────────────────────────────────────────────────────────────┘
```

### Domain Randomization Pipeline

```
Domain Randomization Process:
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Base Model   │───▶│  Randomization  │───▶│   Policy        │
│   • Fixed       │    │  • Visual       │    │   Training      │
│   Parameters   │    │  • Physical     │    │  • Robust       │
│                │    │  • Environmental│    │    Policy       │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         ▼                       ▼                       ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Reality Gap   │───▶│  Generalization │───▶│   Real-World   │
│   Analysis      │    │  Assessment     │    │   Deployment   │
│  • Identify     │    │  • Performance │    │  • Successful  │
│    Mismatches   │    │    Across       │    │    Transfer    │
│  • Quantify     │    │    Domains     │    │  • Performance │
│    Differences  │    │  • Robustness  │    │    Metrics     │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

## Tools & Software

This chapter uses:
- **Gazebo** - Physics-based simulation for dynamics modeling
- **Unity** - Visual simulation for perception alignment
- **ROS 2** - Integration between simulation and reality
- **PyBullet** - Physics engine for rapid prototyping
- **OpenAI Gym** - Reinforcement learning environment interface
- **MuJoCo** - High-fidelity physics simulation
- **System Identification Tools** - For modeling reality gaps
- **Domain Randomization Libraries** - For transfer improvement

## Code / Configuration Examples

### System Identification for Dynamics Modeling
```python
import numpy as np
import rclpy
from rclpy.node import Node
from std_msgs.msg import Float64MultiArray
from sensor_msgs.msg import JointState
from geometry_msgs.msg import Twist
import scipy.optimize as opt
from scipy import signal

class SystemIdentification(Node):
    def __init__(self):
        super().__init__('system_identification')

        # Publishers and subscribers
        self.cmd_pub = self.create_publisher(Float64MultiArray, '/joint_commands', 10)
        self.joint_sub = self.create_subscription(JointState, '/joint_states', self.joint_callback, 10)

        # Data collection
        self.joint_positions = []
        self.joint_velocities = []
        self.joint_commands = []
        self.timestamps = []

        # Timer for data collection
        self.data_timer = self.create_timer(0.01, self.collect_data)  # 100 Hz
        self.experiment_timer = self.create_timer(10.0, self.run_identification_experiment)

        # Robot dynamics parameters to identify
        self.dynamics_params = {
            'mass': 10.0,  # Base estimate
            'friction_coeff': 0.1,
            'inertia': 0.5
        }

        self.get_logger().info('System identification node initialized')

    def joint_callback(self, msg):
        """Collect joint state data for system identification"""
        current_time = self.get_clock().now().nanoseconds / 1e9

        self.joint_positions.append(list(msg.position))
        self.joint_velocities.append(list(msg.velocity))
        self.timestamps.append(current_time)

    def collect_data(self):
        """Collect input-output data for system identification"""
        # Apply random excitation commands to explore dynamics
        if len(self.joint_positions) > 0:
            # Generate random joint commands for excitation
            random_commands = np.random.uniform(-0.5, 0.5, len(self.joint_positions[-1]))
            cmd_msg = Float64MultiArray()
            cmd_msg.data = random_commands.tolist()
            self.cmd_pub.publish(cmd_msg)

            self.joint_commands.append(random_commands.tolist())

    def run_identification_experiment(self):
        """Run system identification experiment"""
        if len(self.joint_positions) > 1000:  # Need sufficient data
            self.get_logger().info('Starting system identification...')

            # Prepare data for identification
            positions = np.array(self.joint_positions)
            velocities = np.array(self.joint_velocities)
            commands = np.array(self.joint_commands)

            # Estimate dynamics parameters using optimization
            result = opt.minimize(
                self.dynamics_cost_function,
                x0=list(self.dynamics_params.values()),
                args=(positions, velocities, commands),
                method='L-BFGS-B'
            )

            # Update identified parameters
            param_names = list(self.dynamics_params.keys())
            for i, name in enumerate(param_names):
                self.dynamics_params[name] = result.x[i]

            self.get_logger().info(f'Identified dynamics parameters: {self.dynamics_params}')

            # Reset data collection for next iteration
            self.joint_positions = []
            self.joint_velocities = []
            self.joint_commands = []
            self.timestamps = []

    def dynamics_cost_function(self, params, positions, velocities, commands):
        """Cost function for dynamics parameter estimation"""
        mass, friction_coeff, inertia = params

        # Simulate system with current parameters
        simulated_velocities = []
        for i in range(1, len(positions)):
            # Simple dynamics model: mass * acceleration + friction * velocity = command
            dt = self.timestamps[i] - self.timestamps[i-1] if i > 0 else 0.01

            # Estimate acceleration from velocity
            if i > 1:
                acceleration = (velocities[i] - velocities[i-1]) / dt
            else:
                acceleration = np.zeros_like(velocities[i])

            # Calculate predicted velocity based on dynamics
            friction_force = friction_coeff * velocities[i-1]
            net_force = commands[i-1] - friction_force
            predicted_acceleration = net_force / mass
            predicted_velocity = velocities[i-1] + predicted_acceleration * dt

            simulated_velocities.append(predicted_velocity)

        if len(simulated_velocities) > 0:
            simulated_velocities = np.array(simulated_velocities)
            actual_velocities = velocities[1:]  # Skip first since we start from index 1

            # Calculate error between actual and simulated
            error = np.mean((actual_velocities - simulated_velocities)**2)
            return error
        else:
            return float('inf')

def main(args=None):
    rclpy.init(args=args)
    node = SystemIdentification()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Domain Randomization Environment
```python
import random
import numpy as np
import gym
from gym import spaces
import pybullet as p
import pybullet_data

class DomainRandomizationEnv(gym.Env):
    """Gym environment with domain randomization for Sim2Real transfer"""

    def __init__(self):
        super(DomainRandomizationEnv, self).__init__()

        # Define action and observation spaces
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(4,), dtype=np.float32)
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(12,), dtype=np.float32)

        # Physics parameters to randomize
        self.param_ranges = {
            'gravity': [-10.5, -9.5],  # Gravity range
            'friction': [0.1, 0.9],    # Friction coefficient
            'restitution': [0.0, 0.5], # Bounce coefficient
            'mass_multiplier': [0.8, 1.2], # Mass scaling
        }

        # Visual parameters to randomize
        self.visual_ranges = {
            'light_intensity': [0.5, 2.0],
            'floor_color_var': [0.1, 0.9],
        }

        # Connect to PyBullet
        self.physics_client = p.connect(p.DIRECT)  # Use DIRECT for training, SHARED_MEMORY for GUI
        p.setAdditionalSearchPath(pybullet_data.getDataPath())

        # Initialize environment
        self.reset()

    def reset(self):
        """Reset environment with randomized parameters"""
        # Randomize physics parameters
        self.randomize_physics()

        # Randomize visual parameters
        self.randomize_visual()

        # Reset robot position and state
        self.reset_robot()

        # Return initial observation
        return self.get_observation()

    def randomize_physics(self):
        """Randomize physics parameters"""
        # Set random gravity
        gravity_range = self.param_ranges['gravity']
        gravity_z = random.uniform(gravity_range[0], gravity_range[1])
        p.setGravity(0, 0, gravity_z, physicsClientId=self.physics_client)

        # Randomize other physics parameters as needed
        # This is a simplified example - real implementation would be more complex
        pass

    def randomize_visual(self):
        """Randomize visual parameters"""
        # In a real implementation, this would randomize lighting, textures, etc.
        # For PyBullet, we can randomize floor color
        floor_color_range = self.visual_ranges['floor_color_var']
        color_val = random.uniform(floor_color_range[0], floor_color_range[1])
        p.changeVisualShape(-1, -1, rgbaColor=[color_val, color_val, color_val, 1])

    def reset_robot(self):
        """Reset robot to initial state"""
        # Load plane
        self.plane_id = p.loadURDF("plane.urdf")

        # Load a simple robot (replace with your robot URDF)
        self.robot_id = p.loadURDF("r2d2.urdf", [0, 0, 1])

        # Reset joint positions
        num_joints = p.getNumJoints(self.robot_id)
        for i in range(num_joints):
            p.resetJointState(self.robot_id, i, targetValue=0)

    def step(self, action):
        """Execute action in environment"""
        # Apply action to robot
        self.apply_action(action)

        # Step simulation
        p.stepSimulation(physicsClientId=self.physics_client)

        # Get observation
        obs = self.get_observation()

        # Calculate reward (simplified)
        reward = self.calculate_reward()

        # Check if episode is done
        done = self.is_done()

        # Info dictionary
        info = {}

        return obs, reward, done, info

    def apply_action(self, action):
        """Apply action to robot"""
        # In a real implementation, this would apply torques or positions to joints
        # This is a simplified example
        num_joints = p.getNumJoints(self.robot_id)
        for i in range(min(len(action), num_joints)):
            p.setJointMotorControl2(
                bodyIndex=self.robot_id,
                jointIndex=i,
                controlMode=p.VELOCITY_CONTROL,
                targetVelocity=action[i] * 10.0  # Scale action
            )

    def get_observation(self):
        """Get current observation"""
        # Get robot state (position, velocity, etc.)
        robot_pos, robot_orn = p.getBasePositionAndOrientation(self.robot_id)
        robot_vel, robot_angular_vel = p.getBaseVelocity(self.robot_id)

        # Get joint states
        joint_states = []
        num_joints = p.getNumJoints(self.robot_id)
        for i in range(num_joints):
            joint_state = p.getJointState(self.robot_id, i)
            joint_states.extend([joint_state[0], joint_state[1]])  # position, velocity

        # Combine all observations
        obs = np.concatenate([
            robot_pos,
            robot_orn,  # Using orientation as quaternion
            robot_vel,
            robot_angular_vel,
            joint_states[:4]  # Limit for space
        ])

        return obs.astype(np.float32)

    def calculate_reward(self):
        """Calculate reward for current state"""
        # Simplified reward - in practice this would be more complex
        robot_pos = p.getBasePositionAndOrientation(self.robot_id)[0]
        # Reward for moving forward (positive x direction)
        reward = robot_pos[0] * 0.1
        return reward

    def is_done(self):
        """Check if episode is done"""
        # Simplified termination condition
        robot_pos = p.getBasePositionAndOrientation(self.robot_id)[0]
        # End if robot falls too low
        return robot_pos[2] < 0.1

    def close(self):
        """Close the environment"""
        p.disconnect(physicsClientId=self.physics_client)
```

### Robust Control with Adaptation
```python
import numpy as np
import rclpy
from rclpy.node import Node
from std_msgs.msg import Float64MultiArray
from sensor_msgs.msg import JointState, Imu
from geometry_msgs.msg import Twist
import math

class RobustAdaptiveController(Node):
    def __init__(self):
        super().__init__('robust_adaptive_controller')

        # Publishers and subscribers
        self.cmd_pub = self.create_publisher(Float64MultiArray, '/joint_commands', 10)
        self.joint_sub = self.create_subscription(JointState, '/joint_states', self.joint_callback, 10)
        self.imu_sub = self.create_subscription(Imu, '/imu/data', self.imu_callback, 10)

        # Timer for control loop
        self.control_timer = self.create_timer(0.01, self.control_loop)  # 100 Hz

        # Robot state
        self.current_positions = {}
        self.current_velocities = {}
        self.imu_data = None

        # Adaptive control parameters
        self.adaptation_rate = 0.01
        self.estimated_params = {
            'mass': 10.0,
            'friction': 0.1,
            'gravity_compensation': 9.81
        }

        # Reference trajectory
        self.trajectory_time = 0.0
        self.trajectory_amplitude = 0.5
        self.trajectory_frequency = 0.5

        # Control gains
        self.kp = 100.0  # Proportional gain
        self.kd = 10.0   # Derivative gain

        self.get_logger().info('Robust adaptive controller initialized')

    def joint_callback(self, msg):
        """Process joint state data"""
        for i, name in enumerate(msg.name):
            if i < len(msg.position):
                self.current_positions[name] = msg.position[i]
            if i < len(msg.velocity):
                self.current_velocities[name] = msg.velocity[i]

    def imu_callback(self, msg):
        """Process IMU data for balance control"""
        self.imu_data = msg

    def control_loop(self):
        """Main adaptive control loop"""
        if not self.current_positions or not self.imu_data:
            return

        # Update trajectory time
        self.trajectory_time += 0.01  # 100 Hz

        # Get first joint for demonstration (in practice, you'd control all joints)
        joint_names = list(self.current_positions.keys())
        if not joint_names:
            return

        joint_name = joint_names[0]
        current_pos = self.current_positions[joint_name]
        current_vel = self.current_velocities.get(joint_name, 0.0)

        # Generate reference trajectory
        reference_pos = self.trajectory_amplitude * math.sin(
            2 * math.pi * self.trajectory_frequency * self.trajectory_time
        )
        reference_vel = self.trajectory_amplitude * 2 * math.pi * self.trajectory_frequency * math.cos(
            2 * math.pi * self.trajectory_frequency * self.trajectory_time
        )

        # Calculate tracking error
        pos_error = reference_pos - current_pos
        vel_error = reference_vel - current_vel

        # Adaptive parameter estimation (simplified)
        self.update_parameter_estimates(pos_error, vel_error, current_vel)

        # Calculate control command with adaptive parameters
        control_effort = (
            self.kp * pos_error +
            self.kd * vel_error +
            self.estimated_params['friction'] * current_vel +
            self.estimated_params['gravity_compensation'] * math.sin(current_pos)
        )

        # Apply control limits
        control_effort = max(-100.0, min(100.0, control_effort))

        # Publish command
        cmd_msg = Float64MultiArray()
        cmd_msg.data = [float(control_effort)]  # For the first joint
        self.cmd_pub.publish(cmd_msg)

        # Log control information
        self.get_logger().debug(
            f'Joint: {joint_name}, Pos: {current_pos:.3f}, Ref: {reference_pos:.3f}, '
            f'Error: {pos_error:.3f}, Control: {control_effort:.3f}, '
            f'Params: {self.estimated_params}'
        )

    def update_parameter_estimates(self, pos_error, vel_error, velocity):
        """Update estimated parameters using adaptive law"""
        # Simplified parameter adaptation
        # In practice, this would use more sophisticated algorithms

        # Adapt friction estimate based on velocity and error
        friction_correction = self.adaptation_rate * velocity * pos_error
        self.estimated_params['friction'] = max(0.01,
            self.estimated_params['friction'] + friction_correction)

        # Adapt mass estimate based on acceleration error
        # This is a simplified example - real adaptation would be more complex
        acceleration_error = pos_error * 10.0  # Rough estimate
        mass_correction = self.adaptation_rate * abs(acceleration_error) * 0.1
        self.estimated_params['mass'] = max(1.0,
            self.estimated_params['mass'] + mass_correction)

def main(args=None):
    rclpy.init(args=args)
    controller = RobustAdaptiveController()

    try:
        rclpy.spin(controller)
    except KeyboardInterrupt:
        pass
    finally:
        controller.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Practical Lab / Simulation

### Lab Exercise 1: Reality Gap Analysis

1. Set up a simple simulation environment (using Gazebo or PyBullet)
2. Create a physical model of the same robot
3. Execute identical commands in both environments
4. Compare the resulting trajectories and identify differences
5. Document the key factors causing the reality gap

### Lab Exercise 2: Domain Randomization Implementation

1. Implement domain randomization in your simulation environment
2. Randomize key parameters like:
   - Visual properties (lighting, textures, colors)
   - Physical properties (friction, mass, damping)
   - Environmental properties (gravity, wind)
3. Train a simple controller with domain randomization
4. Test the controller's robustness to parameter variations

### Lab Exercise 3: System Identification

1. Collect input-output data from your real robot or high-fidelity simulation
2. Implement system identification techniques to estimate dynamics parameters
3. Compare identified parameters with nominal values
4. Use identified parameters to improve simulation fidelity

### Lab Exercise 4: Adaptive Control Design

1. Implement an adaptive controller that adjusts to changing dynamics
2. Test the controller in simulation with varying parameters
3. Evaluate the controller's ability to maintain performance despite model uncertainty
4. Analyze the adaptation rate and stability properties

### Lab Exercise 5: Transfer Validation

1. Train a policy in simulation with domain randomization
2. Deploy the policy to a real robot or more accurate simulation
3. Measure performance degradation and identify key transfer barriers
4. Iterate on the simulation model to improve transfer performance

### Lab Exercise 6: Curriculum Learning for Transfer

1. Design a curriculum of increasingly complex tasks
2. Start with simplified simulation and gradually increase realism
3. Train policies at each level of the curriculum
4. Evaluate the effectiveness of curriculum learning for Sim2Real transfer

## Real-World Mapping

### Industrial Applications
- **Manufacturing**: Robots trained in simulation with domain randomization for real-world assembly tasks
- **Logistics**: Warehouse robots with Sim2Real transfer for navigation and manipulation
- **Agriculture**: Autonomous vehicles with simulation-trained perception systems

### Research Applications
- **Boston Dynamics**: Extensive use of simulation with domain randomization for quadruped locomotion
- **DeepMind**: Sim2Real transfer for dexterous manipulation tasks
- **OpenAI**: Domain randomization for robotic hand manipulation

### Key Success Factors
- **Parameter Coverage**: Ensuring randomization covers the range of real-world parameters
- **Validation**: Systematic testing of transfer performance
- **Iterative Refinement**: Continuous improvement of simulation models
- **Robust Control**: Controllers that can handle model uncertainty
- **Sensory Alignment**: Matching simulated and real sensor characteristics

## Summary

Chapter 3 has covered the critical challenge of Sim2Real transfer in Physical AI and humanoid robotics. We've explored the reality gap problem, domain randomization techniques, system identification methods, and adaptive control strategies. The examples demonstrated practical implementations of system identification, domain randomization environments, and robust adaptive controllers. The hands-on lab exercises provide experience with analyzing reality gaps, implementing transfer techniques, and validating performance. This knowledge is essential for developing humanoid robots that can effectively transfer skills learned in simulation to real-world applications.