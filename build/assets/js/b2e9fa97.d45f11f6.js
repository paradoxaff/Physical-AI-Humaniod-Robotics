"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[398],{5043:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"modules/module-04-vla-capstone/vision-language-action","title":"Chapter 1 - Vision-Language-Action","description":"Learning Objectives","source":"@site/docs/modules/module-04-vla-capstone/vision-language-action.mdx","sourceDirName":"modules/module-04-vla-capstone","slug":"/modules/module-04-vla-capstone/vision-language-action","permalink":"/docs/modules/module-04-vla-capstone/vision-language-action","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-04-vla-capstone/vision-language-action.mdx","tags":[],"version":"current","frontMatter":{"title":"Chapter 1 - Vision-Language-Action","sidebar_label":"Vision-Language-Action"},"sidebar":"tutorialSidebar","previous":{"title":"Conversational Robotics","permalink":"/docs/modules/module-04-vla-capstone/conversational-robotics"}}');var o=t(4848),a=t(8453);const s={title:"Chapter 1 - Vision-Language-Action",sidebar_label:"Vision-Language-Action"},r="Chapter 1: Vision-Language-Action",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Physical AI Concept",id:"physical-ai-concept",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Vision-Language-Action Architecture",id:"vision-language-action-architecture",level:3},{value:"VLA Processing Pipeline",id:"vla-processing-pipeline",level:3},{value:"Tools &amp; Software",id:"tools--software",level:2},{value:"Code / Configuration Examples",id:"code--configuration-examples",level:2},{value:"Vision-Language-Action Core Node",id:"vision-language-action-core-node",level:3},{value:"VLA Object Detection and Grounding Node",id:"vla-object-detection-and-grounding-node",level:3},{value:"VLA Action Planning Node",id:"vla-action-planning-node",level:3},{value:"Practical Lab / Simulation",id:"practical-lab--simulation",level:2},{value:"Lab Exercise 1: VLA Core Implementation",id:"lab-exercise-1-vla-core-implementation",level:3},{value:"Lab Exercise 2: Object Grounding and Attention",id:"lab-exercise-2-object-grounding-and-attention",level:3},{value:"Lab Exercise 3: Action Planning and Execution",id:"lab-exercise-3-action-planning-and-execution",level:3},{value:"Lab Exercise 4: Multimodal Integration",id:"lab-exercise-4-multimodal-integration",level:3},{value:"Lab Exercise 5: Human-Robot Interaction",id:"lab-exercise-5-human-robot-interaction",level:3},{value:"Lab Exercise 6: Performance Optimization",id:"lab-exercise-6-performance-optimization",level:3},{value:"Real-World Mapping",id:"real-world-mapping",level:2},{value:"Industrial Applications",id:"industrial-applications",level:3},{value:"Research Applications",id:"research-applications",level:3},{value:"Key Success Factors",id:"key-success-factors",level:3},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"chapter-1-vision-language-action",children:"Chapter 1: Vision-Language-Action"})}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Understand the fundamental principles of Vision-Language-Action (VLA) systems"}),"\n",(0,o.jsx)(e.li,{children:"Implement multimodal AI models that integrate visual perception and language understanding"}),"\n",(0,o.jsx)(e.li,{children:"Design action execution systems that respond to natural language commands"}),"\n",(0,o.jsx)(e.li,{children:"Integrate vision, language, and action components into cohesive systems"}),"\n",(0,o.jsx)(e.li,{children:"Evaluate VLA system performance in real-world scenarios"}),"\n",(0,o.jsx)(e.li,{children:"Apply VLA concepts to humanoid robotics applications"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"physical-ai-concept",children:"Physical AI Concept"}),"\n",(0,o.jsx)(e.p,{children:"Vision-Language-Action (VLA) represents the integration of three critical cognitive modalities that enable Physical AI systems to interact naturally with the physical world and humans. This integration allows robots to perceive their environment visually, understand natural language commands, and execute appropriate physical actions. VLA systems embody the essence of Physical AI by creating machines that can see, understand, and act in the physical world with human-like capabilities."}),"\n",(0,o.jsx)(e.p,{children:"The VLA framework encompasses:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Vision"}),": Computer vision systems that enable environmental perception and object recognition"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Language"}),": Natural language processing that allows understanding of commands and context"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action"}),": Physical manipulation and navigation capabilities for real-world interaction"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Integration"}),": Seamless coordination between all three modalities for coherent behavior"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"This integration is essential for Physical AI because it enables robots to operate in unstructured human environments where they must interpret natural language commands, perceive complex visual scenes, and execute precise physical actions to complete tasks."}),"\n",(0,o.jsx)(e.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,o.jsx)(e.h3,{id:"vision-language-action-architecture",children:"Vision-Language-Action Architecture"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"VLA System Architecture:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Human Input Layer                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502   Natural       \u2502  \u2502   Visual        \u2502  \u2502   Demonstrative \u2502    \u2502\n\u2502  \u2502   Language      \u2502  \u2502   Commands      \u2502  \u2502   Gestures      \u2502    \u2502\n\u2502  \u2502   \u2022 Spoken      \u2502  \u2502   \u2022 Pointing    \u2502  \u2502   \u2022 Demonstrations\u2502  \u2502\n\u2502  \u2502   \u2022 Written     \u2502  \u2502   \u2022 Showing     \u2502  \u2502   \u2022 Actions     \u2502    \u2502\n\u2502  \u2502   \u2022 Questions   \u2502  \u2502   \u2022 Context     \u2502  \u2502   \u2022 Tool Use    \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502              \u2502                    \u2502                    \u2502           \u2502\n\u2502              \u25bc                    \u25bc                    \u25bc           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Multimodal Perception Layer                    \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502\n\u2502  \u2502  \u2502  Vision     \u2502  \u2502  Language   \u2502  \u2502  Context    \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  Processing \u2502  \u2502  Processing \u2502  \u2502  Integration\u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Object   \u2502  \u2502  \u2022 Intent   \u2502  \u2502  \u2022 Scene    \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Detection \u2502  \u2502    Extraction\u2502  \u2502    Understanding\u2502    \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Scene    \u2502  \u2502  \u2022 Command  \u2502  \u2502  \u2022 Spatial   \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Understanding\u2502\u2502    Parsing  \u2502  \u2502    Reasoning  \u2502      \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 3D       \u2502  \u2502  \u2022 Semantic  \u2502  \u2502  \u2022 Temporal  \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Reconstruction\u2502\u2502    Analysis \u2502  \u2502    Context   \u2502       \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                     \u2502                            \u2502\n\u2502                                     \u25bc                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              AI Reasoning Engine                            \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502\n\u2502  \u2502  \u2502  Multimodal \u2502  \u2502  Planning   \u2502  \u2502  Learning   \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  Fusion     \u2502  \u2502  & Control  \u2502  \u2502  & Adaptation\u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Attention\u2502  \u2502  \u2022 Task     \u2502  \u2502  \u2022 Imitation \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Mechanisms\u2502 \u2502    Decomposition\u2502\u2502    Learning \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Cross-   \u2502  \u2502  \u2022 Behavior \u2502  \u2502  \u2022 Reinforcement\u2502     \u2502  \u2502\n\u2502  \u2502  \u2502    Modal    \u2502  \u2502    Selection\u2502  \u2502    Learning  \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Alignment\u2502  \u2502  \u2022 Policy   \u2502  \u2502  \u2022 Adaptation\u2502        \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    Execution\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                     \u2502                            \u2502\n\u2502                                     \u25bc                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Action Execution Layer                         \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502  \u2502\n\u2502  \u2502  \u2502  Navigation \u2502  \u2502  Manipulation\u2502  \u2502  Interaction\u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Path     \u2502  \u2502  \u2022 Grasping  \u2502  \u2502  \u2022 Social   \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Planning  \u2502  \u2502  \u2022 Tool Use \u2502  \u2502    Behaviors \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Obstacle  \u2502  \u2502  \u2022 Force    \u2502  \u2502  \u2022 Safety   \u2502        \u2502  \u2502\n\u2502  \u2502  \u2502    Avoidance \u2502  \u2502    Control  \u2502  \u2502    Protocols \u2502        \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(e.h3,{id:"vla-processing-pipeline",children:"VLA Processing Pipeline"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"VLA Processing Pipeline:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Visual Input  \u2502\u2500\u2500\u2500\u25b6\u2502  Language       \u2502\u2500\u2500\u2500\u25b6\u2502  Multimodal     \u2502\n\u2502   \u2022 RGB Images  \u2502    \u2502  Input          \u2502    \u2502  Understanding  \u2502\n\u2502   \u2022 Point Cloud \u2502    \u2502  \u2022 Natural      \u2502    \u2502  \u2022 Scene       \u2502\n\u2502   \u2022 Video       \u2502    \u2502    Language     \u2502    \u2502    Context     \u2502\n\u2502   \u2022 Depth       \u2502    \u2502  \u2022 Commands     \u2502    \u2502  \u2022 Intent      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502  \u2022 Questions    \u2502    \u2502    Interpretation\u2502\n         \u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502  \u2022 Action      \u2502\n         \u25bc                       \u2502              \u2502    Planning    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502   Feature       \u2502\u2500\u2500\u2500\u25b6\u2502  Cross-Modal    \u2502\u2500\u2500\u2500\u25b6\u2502  Action         \u2502\n\u2502   Extraction    \u2502    \u2502  Alignment      \u2502    \u2502  Generation     \u2502\n\u2502  \u2022 Visual       \u2502    \u2502  \u2022 Attention    \u2502    \u2502  \u2022 Navigation  \u2502\n\u2502    Features     \u2502    \u2502  \u2022 Semantic     \u2502    \u2502  \u2022 Manipulation\u2502\n\u2502  \u2022 Text         \u2502    \u2502    Matching     \u2502    \u2502  \u2022 Interaction \u2502\n\u2502    Embeddings   \u2502    \u2502  \u2022 Context      \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502    Fusion       \u2502              \u2502\n         \u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u25bc\n         \u25bc                       \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502  Physical       \u2502\n\u2502   Object        \u2502\u2500\u2500\u2500\u25b6\u2502  Command        \u2502\u2500\u2500\u2500\u25b6\u2502  Execution      \u2502\n\u2502   Detection &   \u2502    \u2502  Interpretation \u2502    \u2502  \u2022 Robot        \u2502\n\u2502   Recognition   \u2502    \u2502  \u2022 Intent       \u2502    \u2502    Commands     \u2502\n\u2502  \u2022 YOLO, etc.   \u2502    \u2502    Extraction   \u2502    \u2502  \u2022 Motor        \u2502\n\u2502  \u2022 Classification\u2502    \u2502  \u2022 Action      \u2502    \u2502    Control      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502    Mapping      \u2502    \u2502  \u2022 Safety       \u2502\n                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502    Validation    \u2502\n                                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(e.h2,{id:"tools--software",children:"Tools & Software"}),"\n",(0,o.jsx)(e.p,{children:"This chapter uses:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"OpenVLA"})," - Open-source Vision-Language-Action models"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"CLIP"})," - Vision-language models for multimodal understanding"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"GPT/LLM"})," - Large language models for natural language processing"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"OpenCLIP"})," - Open-source implementation of CLIP"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Hugging Face Transformers"})," - Library for transformer models"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"PyTorch"})," - Deep learning framework"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"OpenCV"})," - Computer vision library"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"ROS 2"})," - Robot operating system for integration"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"code--configuration-examples",children:"Code / Configuration Examples"}),"\n",(0,o.jsx)(e.h3,{id:"vision-language-action-core-node",children:"Vision-Language-Action Core Node"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import Pose, Point\nfrom std_msgs.msg import String, Float32MultiArray\nfrom vision_msgs.msg import Detection2DArray\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nimport torch\nimport open_clip\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image as PILImage\nimport math\n\nclass VLACoreNode(Node):\n    def __init__(self):\n        super().__init__('vla_core_node')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/rgb/image_rect_color', self.image_callback, 10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, '/camera/rgb/camera_info', self.camera_info_callback, 10\n        )\n        self.command_sub = self.create_subscription(\n            String, '/vla/command', self.command_callback, 10\n        )\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, '/object_detections', self.detection_callback, 10\n        )\n        self.action_pub = self.create_publisher(\n            String, '/vla/action_command', 10\n        )\n        self.response_pub = self.create_publisher(\n            String, '/vla/response', 10\n        )\n        self.heatmap_pub = self.create_publisher(\n            Image, '/vla/attention_heatmap', 10\n        )\n\n        # Initialize models\n        self.setup_vision_language_model()\n        self.setup_action_mapping()\n\n        # System state\n        self.current_image = None\n        self.current_image_cv = None\n        self.camera_matrix = None\n        self.object_detections = []\n        self.command_history = []\n        self.attention_weights = None\n\n        # VLA parameters\n        self.temperature = 0.7\n        self.confidence_threshold = 0.5\n\n        # Timer for VLA processing\n        self.vla_timer = self.create_timer(0.5, self.process_vla_cycle)\n\n        self.get_logger().info('VLA Core node initialized')\n\n    def setup_vision_language_model(self):\n        \"\"\"Initialize vision-language model for multimodal understanding\"\"\"\n        try:\n            # Load pre-trained CLIP model\n            self.model, self.preprocess, self.tokenizer = open_clip.create_model_and_transforms(\n                'ViT-B-32', pretrained='openai'\n            )\n            self.model.eval()\n\n            # Move to GPU if available\n            if torch.cuda.is_available():\n                self.model = self.model.cuda()\n\n            self.get_logger().info('Vision-Language model loaded successfully')\n        except Exception as e:\n            self.get_logger().error(f'Failed to load vision-language model: {e}')\n            self.model = None\n\n    def setup_action_mapping(self):\n        \"\"\"Setup mapping from language commands to robot actions\"\"\"\n        self.action_map = {\n            'navigation': {\n                'keywords': ['go to', 'move to', 'navigate to', 'approach', 'move toward'],\n                'action_type': 'navigate'\n            },\n            'manipulation': {\n                'keywords': ['pick up', 'grasp', 'take', 'lift', 'get', 'place', 'put', 'set down'],\n                'action_type': 'manipulate'\n            },\n            'inspection': {\n                'keywords': ['look at', 'examine', 'check', 'inspect', 'see', 'find', 'search for'],\n                'action_type': 'inspect'\n            },\n            'interaction': {\n                'keywords': ['follow', 'accompany', 'come to', 'meet', 'greet'],\n                'action_type': 'interact'\n            },\n            'stop': {\n                'keywords': ['stop', 'halt', 'pause', 'wait', 'cease'],\n                'action_type': 'stop'\n            }\n        }\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera image\"\"\"\n        try:\n            self.current_image_cv = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')\n            # Convert to PIL for model processing\n            pil_image = PILImage.fromarray(cv2.cvtColor(self.current_image_cv, cv2.COLOR_BGR2RGB))\n            self.current_image = self.preprocess(pil_image).unsqueeze(0)\n\n            if torch.cuda.is_available():\n                self.current_image = self.current_image.cuda()\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def camera_info_callback(self, msg):\n        \"\"\"Process camera calibration information\"\"\"\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n\n    def detection_callback(self, msg):\n        \"\"\"Process object detections\"\"\"\n        self.object_detections = msg.detections\n\n    def command_callback(self, msg):\n        \"\"\"Process natural language command\"\"\"\n        command = msg.data.lower().strip()\n        self.command_history.append({\n            'timestamp': self.get_clock().now().nanoseconds / 1e9,\n            'command': command\n        })\n\n        self.get_logger().info(f'Received command: {command}')\n\n    def process_vla_cycle(self):\n        \"\"\"Main VLA processing cycle\"\"\"\n        if self.current_image is None or not self.command_history:\n            return\n\n        # Get the most recent command\n        latest_command = self.command_history[-1]['command']\n\n        # Process command using VLA pipeline\n        action, confidence = self.process_command_with_context(latest_command)\n\n        if action and confidence > self.confidence_threshold:\n            # Publish action command\n            action_msg = String()\n            action_msg.data = action\n            self.action_pub.publish(action_msg)\n\n            # Publish response\n            response_msg = String()\n            response_msg.data = f\"Understood: {latest_command}. Executing: {action} (confidence: {confidence:.2f})\"\n            self.response_pub.publish(response_msg)\n\n            self.get_logger().info(f'VLA Cycle - Command: {latest_command}, Action: {action}, Confidence: {confidence:.2f}')\n\n    def process_command_with_context(self, command):\n        \"\"\"Process command with visual context using VLA model\"\"\"\n        if self.model is None or self.current_image is None:\n            return \"error: models not loaded\", 0.0\n\n        try:\n            # Tokenize the command\n            text = self.tokenizer([command])\n            if torch.cuda.is_available():\n                text = text.cuda()\n\n            # Get image and text features\n            with torch.no_grad():\n                image_features = self.model.encode_image(self.current_image)\n                text_features = self.model.encode_text(text)\n\n                # Normalize features\n                image_features /= image_features.norm(dim=-1, keepdim=True)\n                text_features /= text_features.norm(dim=-1, keepdim=True)\n\n                # Calculate similarity\n                similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n                confidence = similarity[0][0].item()\n\n            # Determine appropriate action based on command\n            action = self.interpret_command_and_generate_action(command)\n\n            return action, confidence\n\n        except Exception as e:\n            self.get_logger().error(f'Error in VLA processing: {e}')\n            return f\"error_processing: {str(e)}\", 0.0\n\n    def interpret_command_and_generate_action(self, command):\n        \"\"\"Interpret command and generate appropriate action\"\"\"\n        command_lower = command.lower()\n\n        # Determine action type based on keywords\n        for action_type, config in self.action_map.items():\n            for keyword in config['keywords']:\n                if keyword in command_lower:\n                    # Extract object reference if present\n                    object_ref = self.extract_object_reference(command)\n\n                    # Generate specific action command\n                    if action_type == 'navigate' and object_ref:\n                        return f\"NAVIGATE_TO_OBJECT:{object_ref}\"\n                    elif action_type == 'manipulate' and object_ref:\n                        return f\"MANIPULATE_OBJECT:{object_ref}\"\n                    elif action_type == 'inspect' and object_ref:\n                        return f\"INSPECT_OBJECT:{object_ref}\"\n                    elif action_type == 'interact' and object_ref:\n                        return f\"INTERACT_WITH:{object_ref}\"\n                    elif action_type == 'stop':\n                        return \"STOP_ROBOT\"\n                    else:\n                        return f\"{config['action_type'].upper()}_ACTION:DEFAULT\"\n\n        # If no specific action type found, return generic command\n        return f\"GENERIC_ACTION:{command}\"\n\n    def extract_object_reference(self, command):\n        \"\"\"Extract object reference from command using visual context\"\"\"\n        if not self.object_detections:\n            return \"unknown_object\"\n\n        command_lower = command.lower()\n\n        # Look for color descriptors\n        colors = ['red', 'blue', 'green', 'yellow', 'black', 'white', 'orange', 'purple', 'pink', 'gray']\n        for color in colors:\n            if color in command_lower:\n                return f\"{color}_object\"\n\n        # Look for object type descriptors\n        object_types = ['box', 'cup', 'bottle', 'ball', 'person', 'chair', 'table', 'book', 'phone', 'laptop']\n        for obj_type in object_types:\n            if obj_type in command_lower:\n                return f\"{obj_type}_object\"\n\n        # If no specific reference, return the first detected object\n        if self.object_detections:\n            # In a real implementation, this would use the detection results\n            # to identify the most relevant object based on spatial reasoning\n            return \"first_detected_object\"\n\n        return \"unknown_object\"\n\n    def generate_attention_heatmap(self):\n        \"\"\"Generate attention heatmap for visualization\"\"\"\n        if self.attention_weights is not None and self.current_image_cv is not None:\n            # Create heatmap from attention weights\n            heatmap = cv2.applyColorMap(\n                np.uint8(255 * self.attention_weights),\n                cv2.COLORMAP_JET\n            )\n\n            # Blend with original image\n            blended = cv2.addWeighted(self.current_image_cv, 0.7, heatmap, 0.3, 0)\n\n            # Publish as ROS image\n            heatmap_msg = self.bridge.cv2_to_imgmsg(blended, encoding='bgr8')\n            heatmap_msg.header.stamp = self.get_clock().now().to_msg()\n            heatmap_msg.header.frame_id = 'camera_rgb_optical_frame'\n            self.heatmap_pub.publish(heatmap_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VLACoreNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.h3,{id:"vla-object-detection-and-grounding-node",children:"VLA Object Detection and Grounding Node"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray, Detection2D, ObjectHypothesisWithPose\nfrom geometry_msgs.msg import Point\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nfrom ultralytics import YOLO\nfrom PIL import Image as PILImage\nimport torch\n\nclass VLAObjectGroundingNode(Node):\n    def __init__(self):\n        super().__init__('vla_object_grounding_node')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/rgb/image_rect_color', self.image_callback, 10\n        )\n        self.command_sub = self.create_subscription(\n            String, '/vla/command', self.command_callback, 10\n        )\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, '/vla/object_detections_3d', 10\n        )\n        self.segmented_pub = self.create_publisher(\n            Image, '/vla/segmented_image', 10\n        )\n\n        # Initialize YOLO model for object detection\n        self.setup_object_detection_model()\n\n        # Initialize CLIP model for grounding\n        self.setup_grounding_model()\n\n        # System state\n        self.current_image = None\n        self.current_command = None\n        self.camera_matrix = None\n        self.object_detections = []\n\n        # Timer for processing\n        self.process_timer = self.create_timer(0.5, self.process_image_and_command)\n\n        self.get_logger().info('VLA Object Grounding node initialized')\n\n    def setup_object_detection_model(self):\n        \"\"\"Initialize YOLO model for object detection\"\"\"\n        try:\n            # Load YOLOv8 model (or specify your model path)\n            self.detector = YOLO('yolov8n.pt')  # Use 'yolov8n-obb.pt' for oriented bounding boxes\n            self.get_logger().info('YOLO object detection model loaded')\n        except Exception as e:\n            self.get_logger().error(f'Failed to load YOLO model: {e}')\n            self.detector = None\n\n    def setup_grounding_model(self):\n        \"\"\"Initialize grounding model for connecting language to detected objects\"\"\"\n        try:\n            # In practice, this could be a specialized grounding model\n            # For this example, we'll use a simple approach\n            self.grounding_model = None\n            self.get_logger().info('Grounding model initialized')\n        except Exception as e:\n            self.get_logger().error(f'Failed to initialize grounding model: {e}')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera image\"\"\"\n        try:\n            self.current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def command_callback(self, msg):\n        \"\"\"Process command for object grounding\"\"\"\n        self.current_command = msg.data.lower()\n\n    def process_image_and_command(self):\n        \"\"\"Process image and command for object grounding\"\"\"\n        if self.current_image is None or self.current_command is None:\n            return\n\n        try:\n            # Run object detection\n            detections = self.run_object_detection(self.current_image)\n\n            # Ground objects to command\n            grounded_detections = self.ground_objects_to_command(detections, self.current_command)\n\n            # Publish 3D detections\n            self.publish_groundings(grounded_detections)\n\n            # Publish segmented image for visualization\n            self.publish_segmented_image(detections)\n\n        except Exception as e:\n            self.get_logger().error(f'Error in image and command processing: {e}')\n\n    def run_object_detection(self, image):\n        \"\"\"Run object detection on image\"\"\"\n        if self.detector is None:\n            return []\n\n        try:\n            # Run YOLO detection\n            results = self.detector(image)\n\n            detections = []\n            for result in results:\n                boxes = result.boxes\n                if boxes is not None:\n                    for box in boxes:\n                        # Extract bounding box coordinates\n                        x1, y1, x2, y2 = box.xyxy[0].tolist()\n                        conf = box.conf[0].item()\n                        cls = int(box.cls[0].item())\n\n                        detection = {\n                            'bbox': [int(x1), int(y1), int(x2), int(y2)],\n                            'confidence': conf,\n                            'class_id': cls,\n                            'class_name': self.detector.names[cls] if hasattr(self.detector, 'names') else f'Class_{cls}'\n                        }\n                        detections.append(detection)\n\n            return detections\n        except Exception as e:\n            self.get_logger().error(f'Error in object detection: {e}')\n            return []\n\n    def ground_objects_to_command(self, detections, command):\n        \"\"\"Ground detected objects to command using simple matching\"\"\"\n        if not detections or not command:\n            return []\n\n        grounded_detections = []\n\n        for detection in detections:\n            bbox = detection['bbox']\n            class_name = detection['class_name']\n            confidence = detection['confidence']\n\n            # Simple grounding: check if class name is in command\n            # In practice, use more sophisticated grounding techniques\n            relevance_score = self.calculate_relevance_score(class_name, command)\n\n            if relevance_score > 0.3:  # Threshold for relevance\n                grounded_detection = detection.copy()\n                grounded_detection['relevance_score'] = relevance_score\n                grounded_detection['is_target'] = relevance_score > 0.7\n\n                # Calculate 3D position if camera matrix is available\n                if self.camera_matrix is not None:\n                    x_center = (bbox[0] + bbox[2]) / 2\n                    y_center = (bbox[1] + bbox[3]) / 2\n                    # In practice, you'd need depth information for true 3D position\n                    grounded_detection['position_3d'] = [x_center, y_center, 1.0]  # Placeholder depth\n\n                grounded_detections.append(grounded_detection)\n\n        return grounded_detections\n\n    def calculate_relevance_score(self, class_name, command):\n        \"\"\"Calculate relevance score between object class and command\"\"\"\n        class_lower = class_name.lower()\n        command_lower = command.lower()\n\n        # Simple keyword matching\n        score = 0.0\n\n        # Direct match\n        if class_lower in command_lower:\n            score += 0.8\n\n        # Partial match\n        if any(word in command_lower for word in class_lower.split()):\n            score += 0.4\n\n        # Semantic relatedness (simple rules)\n        semantic_relations = {\n            'person': ['person', 'man', 'woman', 'human', 'guy', 'lady'],\n            'cup': ['cup', 'mug', 'glass', 'drink', 'water', 'coffee'],\n            'bottle': ['bottle', 'water', 'drink', 'container'],\n            'chair': ['chair', 'sit', 'seat', 'table'],\n            'book': ['book', 'read', 'page', 'text', 'novel'],\n            'phone': ['phone', 'call', 'mobile', 'device', 'text'],\n            'laptop': ['laptop', 'computer', 'work', 'screen', 'device'],\n            'table': ['table', 'place', 'put', 'on', 'surface'],\n            'box': ['box', 'container', 'pack', 'package', 'item'],\n            'ball': ['ball', 'throw', 'catch', 'play', 'round']\n        }\n\n        for semantic_class, keywords in semantic_relations.items():\n            if class_lower == semantic_class and any(keyword in command_lower for keyword in keywords):\n                score += 0.6\n                break\n\n        return min(1.0, score)\n\n    def publish_groundings(self, grounded_detections):\n        \"\"\"Publish grounded object detections\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header.stamp = self.get_clock().now().to_msg()\n        detection_array.header.frame_id = 'camera_rgb_optical_frame'\n\n        for detection in grounded_detections:\n            detection_2d = Detection2D()\n            detection_2d.header = detection_array.header\n\n            # Set bounding box\n            x1, y1, x2, y2 = detection['bbox']\n            detection_2d.bbox.size_x = x2 - x1\n            detection_2d.bbox.size_y = y2 - y1\n            detection_2d.bbox.center.x = x1 + (x2 - x1) / 2\n            detection_2d.bbox.center.y = y1 + (y2 - y1) / 2\n\n            # Set detection result with relevance score\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.hypothesis.class_id = detection['class_name']\n            hypothesis.hypothesis.score = detection['relevance_score']\n            detection_2d.results.append(hypothesis)\n\n            detection_array.detections.append(detection_2d)\n\n        self.detection_pub.publish(detection_array)\n\n    def publish_segmented_image(self, detections):\n        \"\"\"Publish segmented image with detection overlays\"\"\"\n        if self.current_image is None:\n            return\n\n        output_image = self.current_image.copy()\n\n        for detection in detections:\n            bbox = detection['bbox']\n            x1, y1, x2, y2 = bbox\n            confidence = detection['confidence']\n            class_name = detection['class_name']\n\n            # Draw bounding box\n            cv2.rectangle(output_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n            # Draw label\n            label = f\"{class_name}: {confidence:.2f}\"\n            cv2.putText(\n                output_image,\n                label,\n                (x1, y1 - 10),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.5,\n                (0, 255, 0),\n                1\n            )\n\n        # Publish segmented image\n        segmented_msg = self.bridge.cv2_to_imgmsg(output_image, encoding='bgr8')\n        segmented_msg.header.stamp = self.get_clock().now().to_msg()\n        segmented_msg.header.frame_id = 'camera_rgb_optical_frame'\n        self.segmented_pub.publish(segmented_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VLAObjectGroundingNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.h3,{id:"vla-action-planning-node",children:"VLA Action Planning Node"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Pose, Point, Vector3\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import JointState\nfrom nav_msgs.msg import Path\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nfrom builtin_interfaces.msg import Duration\nimport numpy as np\nimport math\nfrom scipy.spatial.transform import Rotation as R\n\nclass VLAActionPlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'vla_action_planning_node\')\n\n        # Publishers and subscribers\n        self.command_sub = self.create_subscription(\n            String, \'/vla/action_command\', self.command_callback, 10\n        )\n        self.joint_state_sub = self.create_subscription(\n            JointState, \'/joint_states\', self.joint_state_callback, 10\n        )\n        self.path_pub = self.create_publisher(\n            Path, \'/vla/planned_path\', 10\n        )\n        self.joint_traj_pub = self.create_publisher(\n            JointTrajectory, \'/joint_trajectory\', 10\n        )\n        self.action_status_pub = self.create_publisher(\n            Bool, \'/vla/action_status\', 10\n        )\n\n        # Robot state\n        self.current_joint_positions = {}\n        self.current_task = None\n        self.is_executing = False\n\n        # Robot parameters (simplified 6-DOF arm)\n        self.joint_names = [\'joint_1\', \'joint_2\', \'joint_3\', \'joint_4\', \'joint_5\', \'joint_6\']\n        self.joint_limits = {\n            \'min\': [-2.967, -1.832, -2.967, -3.141, -2.967, -1.309],\n            \'max\': [2.967, 1.832, 2.967, 0.0, 2.967, 2.094]\n        }\n\n        # Action planning parameters\n        self.navigation_speed = 0.2  # m/s\n        self.manipulation_speed = 0.1  # m/s\n        self.approach_distance = 0.1  # meters\n        self.grasp_distance = 0.05    # meters\n\n        # Timer for action execution\n        self.action_timer = self.create_timer(0.1, self.execute_action)\n\n        self.get_logger().info(\'VLA Action Planning node initialized\')\n\n    def command_callback(self, msg):\n        """Process action command from VLA system"""\n        command = msg.data\n        self.get_logger().info(f\'Received action command: {command}\')\n\n        # Parse command\n        if \':\' in command:\n            action_type, params = command.split(\':\', 1)\n        else:\n            action_type = command\n            params = ""\n\n        # Set current task based on command\n        self.current_task = {\n            \'type\': action_type.lower(),\n            \'params\': params,\n            \'status\': \'pending\'\n        }\n\n        # Plan and execute action\n        self.plan_and_execute_action()\n\n    def joint_state_callback(self, msg):\n        """Update current joint positions"""\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.current_joint_positions[name] = msg.position[i]\n\n    def plan_and_execute_action(self):\n        """Plan and begin execution of action"""\n        if not self.current_task or self.is_executing:\n            return\n\n        action_type = self.current_task[\'type\']\n        params = self.current_task[\'params\']\n\n        if action_type == \'navigate_to_object\':\n            self.plan_navigation_action(params)\n        elif action_type == \'manipulate_object\':\n            self.plan_manipulation_action(params)\n        elif action_type == \'inspect_object\':\n            self.plan_inspection_action(params)\n        elif action_type == \'stop_robot\':\n            self.execute_stop_action()\n        else:\n            self.get_logger().warn(f\'Unknown action type: {action_type}\')\n\n    def plan_navigation_action(self, params):\n        """Plan navigation to object"""\n        # In a real implementation, this would use a navigation stack\n        # For this example, we\'ll simulate navigation to a relative position\n\n        # Parse object reference and determine navigation goal\n        # This would typically come from the perception system\n        object_ref = params\n\n        # Simulate determining a goal position based on object location\n        # In practice, this would use object pose estimation\n        goal_x, goal_y = self.estimate_navigation_goal(object_ref)\n\n        # Create simple path (in practice, use proper path planning)\n        path = self.generate_simple_path(goal_x, goal_y)\n\n        # Publish path for visualization\n        self.publish_path(path)\n\n        # Execute navigation (simplified)\n        self.execute_navigation(goal_x, goal_y)\n\n    def plan_manipulation_action(self, params):\n        """Plan manipulation action"""\n        # Parse object reference\n        object_ref = params\n\n        # Simulate determining grasp pose based on object\n        grasp_pose = self.estimate_grasp_pose(object_ref)\n\n        # Plan manipulation trajectory\n        trajectory = self.plan_manipulation_trajectory(grasp_pose)\n\n        # Execute manipulation\n        self.execute_manipulation(trajectory)\n\n    def plan_inspection_action(self, params):\n        """Plan inspection action"""\n        # Parse object reference\n        object_ref = params\n\n        # Plan inspection trajectory (orbit around object, etc.)\n        trajectory = self.plan_inspection_trajectory(object_ref)\n\n        # Execute inspection\n        self.execute_inspection(trajectory)\n\n    def execute_navigation(self, goal_x, goal_y):\n        """Execute navigation action"""\n        self.get_logger().info(f\'Navigating to ({goal_x}, {goal_y})\')\n\n        # In a real implementation, this would interface with navigation stack\n        # For simulation, we\'ll just mark as complete after a delay\n        self.current_task[\'status\'] = \'executing\'\n        self.is_executing = True\n\n        # Simulate navigation completion\n        import threading\n        threading.Timer(3.0, self.mark_navigation_complete).start()\n\n    def execute_manipulation(self, trajectory):\n        """Execute manipulation action"""\n        self.get_logger().info(\'Executing manipulation action\')\n\n        # Publish joint trajectory\n        if trajectory:\n            self.joint_traj_pub.publish(trajectory)\n\n        self.current_task[\'status\'] = \'executing\'\n        self.is_executing = True\n\n        # Simulate manipulation completion\n        import threading\n        threading.Timer(5.0, self.mark_manipulation_complete).start()\n\n    def execute_inspection(self, trajectory):\n        """Execute inspection action"""\n        self.get_logger().info(\'Executing inspection action\')\n\n        # Publish inspection trajectory\n        if trajectory:\n            self.joint_traj_pub.publish(trajectory)\n\n        self.current_task[\'status\'] = \'executing\'\n        self.is_executing = True\n\n        # Simulate inspection completion\n        import threading\n        threading.Timer(4.0, self.mark_inspection_complete).start()\n\n    def execute_stop_action(self):\n        """Execute stop action"""\n        self.get_logger().info(\'Stopping robot\')\n\n        # Stop all robot motion\n        self.stop_robot_motion()\n\n        self.current_task[\'status\'] = \'completed\'\n        self.is_executing = False\n\n    def estimate_navigation_goal(self, object_ref):\n        """Estimate navigation goal based on object reference"""\n        # In a real implementation, this would use object pose estimation\n        # For simulation, return a fixed offset from current position\n        # Assuming robot starts at (0, 0), navigate to a relative position\n        return 1.0, 1.0  # Example coordinates\n\n    def generate_simple_path(self, goal_x, goal_y):\n        """Generate simple path to goal (simplified implementation)"""\n        from nav_msgs.msg import Path\n        from geometry_msgs.msg import PoseStamped\n\n        path_msg = Path()\n        path_msg.header.stamp = self.get_clock().now().to_msg()\n        path_msg.header.frame_id = \'map\'\n\n        # Create a simple path with intermediate waypoints\n        current_x, current_y = 0.0, 0.0  # Starting position\n        steps = 10\n\n        for i in range(steps + 1):\n            t = i / steps\n            x = current_x + t * (goal_x - current_x)\n            y = current_y + t * (goal_y - current_y)\n\n            pose_stamped = PoseStamped()\n            pose_stamped.header = path_msg.header\n            pose_stamped.pose.position.x = x\n            pose_stamped.pose.position.y = y\n            pose_stamped.pose.position.z = 0.0\n            pose_stamped.pose.orientation.w = 1.0\n\n            path_msg.poses.append(pose_stamped)\n\n        return path_msg\n\n    def estimate_grasp_pose(self, object_ref):\n        """Estimate grasp pose for object"""\n        # In a real implementation, this would use grasp planning\n        # For simulation, return a simple grasp pose\n        grasp_pose = Pose()\n        grasp_pose.position.x = 0.5  # Example position\n        grasp_pose.position.y = 0.0\n        grasp_pose.position.z = 0.2  # Example height\n        grasp_pose.orientation.w = 1.0  # Identity orientation\n        return grasp_pose\n\n    def plan_manipulation_trajectory(self, grasp_pose):\n        """Plan manipulation trajectory to reach grasp pose"""\n        traj_msg = JointTrajectory()\n        traj_msg.joint_names = self.joint_names\n\n        # Calculate joint positions for grasp pose (simplified inverse kinematics)\n        # In practice, use MoveIt or other IK solver\n        joint_positions = self.calculate_ik_for_pose(grasp_pose)\n\n        if joint_positions:\n            # Create trajectory point\n            point = JointTrajectoryPoint()\n            point.positions = joint_positions\n            point.velocities = [0.0] * len(joint_positions)\n            point.accelerations = [0.0] * len(joint_positions)\n            point.time_from_start = Duration(sec=3, nanosec=0)  # 3 seconds\n\n            traj_msg.points.append(point)\n            return traj_msg\n\n        return None\n\n    def calculate_ik_for_pose(self, pose):\n        """Calculate inverse kinematics for desired end-effector pose (simplified)"""\n        # This is a very simplified IK solution\n        # In practice, use MoveIt or other proper IK solver\n        current_positions = []\n        for joint_name in self.joint_names:\n            if joint_name in self.current_joint_positions:\n                current_positions.append(self.current_joint_positions[joint_name])\n            else:\n                current_positions.append(0.0)\n\n        # Return current positions as approximation\n        # A real IK solver would compute proper joint angles\n        return current_positions\n\n    def plan_inspection_trajectory(self, object_ref):\n        """Plan inspection trajectory around object"""\n        traj_msg = JointTrajectory()\n        traj_msg.joint_names = self.joint_names\n\n        # Create multiple waypoints for inspection\n        for i in range(5):  # 5 inspection poses\n            point = JointTrajectoryPoint()\n            # Calculate different joint positions for various viewing angles\n            # (Simplified - in practice, plan specific inspection poses)\n            point.positions = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  # Placeholder\n            point.velocities = [0.0] * 6\n            point.accelerations = [0.0] * 6\n            point.time_from_start = Duration(sec=2 + i*2, nanosec=0)  # Staggered timing\n\n            traj_msg.points.append(point)\n\n        return traj_msg\n\n    def mark_navigation_complete(self):\n        """Mark navigation task as complete"""\n        if self.current_task and self.current_task[\'type\'] == \'navigate_to_object\':\n            self.current_task[\'status\'] = \'completed\'\n            self.is_executing = False\n            self.get_logger().info(\'Navigation task completed\')\n\n            # Publish completion status\n            status_msg = Bool()\n            status_msg.data = True\n            self.action_status_pub.publish(status_msg)\n\n    def mark_manipulation_complete(self):\n        """Mark manipulation task as complete"""\n        if self.current_task and self.current_task[\'type\'] == \'manipulate_object\':\n            self.current_task[\'status\'] = \'completed\'\n            self.is_executing = False\n            self.get_logger().info(\'Manipulation task completed\')\n\n            # Publish completion status\n            status_msg = Bool()\n            status_msg.data = True\n            self.action_status_pub.publish(status_msg)\n\n    def mark_inspection_complete(self):\n        """Mark inspection task as complete"""\n        if self.current_task and self.current_task[\'type\'] == \'inspect_object\':\n            self.current_task[\'status\'] = \'completed\'\n            self.is_executing = False\n            self.get_logger().info(\'Inspection task completed\')\n\n            # Publish completion status\n            status_msg = Bool()\n            status_msg.data = True\n            self.action_status_pub.publish(status_msg)\n\n    def stop_robot_motion(self):\n        """Stop all robot motion"""\n        # Publish zero-velocity commands to stop robot\n        stop_cmd = JointTrajectory()\n        stop_cmd.joint_names = self.joint_names\n\n        point = JointTrajectoryPoint()\n        point.positions = [0.0] * len(self.joint_names)\n        point.velocities = [0.0] * len(self.joint_names)\n        point.accelerations = [0.0] * len(self.joint_names)\n        point.time_from_start = Duration(sec=0, nanosec=100000000)  # 0.1 seconds\n\n        stop_cmd.points.append(point)\n        self.joint_traj_pub.publish(stop_cmd)\n\n    def execute_action(self):\n        """Execute action based on current task"""\n        # This method runs periodically to monitor action execution\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VLAActionPlanningNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"practical-lab--simulation",children:"Practical Lab / Simulation"}),"\n",(0,o.jsx)(e.h3,{id:"lab-exercise-1-vla-core-implementation",children:"Lab Exercise 1: VLA Core Implementation"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Implement the VLA Core Node with vision-language integration"}),"\n",(0,o.jsx)(e.li,{children:"Test with various natural language commands"}),"\n",(0,o.jsx)(e.li,{children:"Validate object detection and grounding capabilities"}),"\n",(0,o.jsx)(e.li,{children:"Evaluate confidence scoring and decision making"}),"\n",(0,o.jsx)(e.li,{children:"Optimize for real-time performance"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"lab-exercise-2-object-grounding-and-attention",children:"Lab Exercise 2: Object Grounding and Attention"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Implement the VLA Object Grounding Node"}),"\n",(0,o.jsx)(e.li,{children:"Test object detection with YOLO"}),"\n",(0,o.jsx)(e.li,{children:"Validate language-object grounding mechanisms"}),"\n",(0,o.jsx)(e.li,{children:"Evaluate attention visualization capabilities"}),"\n",(0,o.jsx)(e.li,{children:"Test with complex scenes and ambiguous commands"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"lab-exercise-3-action-planning-and-execution",children:"Lab Exercise 3: Action Planning and Execution"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Implement the VLA Action Planning Node"}),"\n",(0,o.jsx)(e.li,{children:"Test navigation action planning"}),"\n",(0,o.jsx)(e.li,{children:"Validate manipulation trajectory generation"}),"\n",(0,o.jsx)(e.li,{children:"Evaluate inspection and interaction capabilities"}),"\n",(0,o.jsx)(e.li,{children:"Test integration with robot hardware or simulation"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"lab-exercise-4-multimodal-integration",children:"Lab Exercise 4: Multimodal Integration"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Connect all VLA components together"}),"\n",(0,o.jsx)(e.li,{children:"Test end-to-end vision-language-action pipeline"}),"\n",(0,o.jsx)(e.li,{children:"Validate system robustness with various inputs"}),"\n",(0,o.jsx)(e.li,{children:"Evaluate response times and accuracy"}),"\n",(0,o.jsx)(e.li,{children:"Test error handling and recovery"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"lab-exercise-5-human-robot-interaction",children:"Lab Exercise 5: Human-Robot Interaction"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Test VLA system with human users"}),"\n",(0,o.jsx)(e.li,{children:"Evaluate natural language understanding"}),"\n",(0,o.jsx)(e.li,{children:"Validate action execution based on commands"}),"\n",(0,o.jsx)(e.li,{children:"Assess user experience and system usability"}),"\n",(0,o.jsx)(e.li,{children:"Document interaction patterns and improvements"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"lab-exercise-6-performance-optimization",children:"Lab Exercise 6: Performance Optimization"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Profile VLA system for computational bottlenecks"}),"\n",(0,o.jsx)(e.li,{children:"Optimize deep learning models for edge deployment"}),"\n",(0,o.jsx)(e.li,{children:"Test performance on different hardware platforms"}),"\n",(0,o.jsx)(e.li,{children:"Evaluate power consumption vs. performance trade-offs"}),"\n",(0,o.jsx)(e.li,{children:"Document optimal configurations for different scenarios"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"real-world-mapping",children:"Real-World Mapping"}),"\n",(0,o.jsx)(e.h3,{id:"industrial-applications",children:"Industrial Applications"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Manufacturing"}),": VLA systems for flexible assembly and quality inspection"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Logistics"}),": Autonomous systems for warehouse operations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Healthcare"}),": Assistive robots with natural language interfaces"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"research-applications",children:"Research Applications"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Humanoid Robotics"}),": Integrated perception, reasoning, and action"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Social Robotics"}),": Natural interaction for service applications"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Cognitive Robotics"}),": Advanced multimodal interfaces"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"key-success-factors",children:"Key Success Factors"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Multimodal Integration"}),": Seamless coordination between vision, language, and action"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Natural Interaction"}),": Intuitive interfaces for human-robot collaboration"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Robustness"}),": Reliable operation in unstructured environments"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Real-time Performance"}),": Fast response for interactive applications"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Safety"}),": Safe physical interaction with humans and objects"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"Chapter 1 has introduced Vision-Language-Action (VLA) systems, which represent the integration of visual perception, natural language understanding, and physical action in Physical AI. We've explored the VLA architecture, which connects these three critical modalities to enable robots to understand natural language commands, perceive their environment visually, and execute appropriate physical actions. The examples demonstrated practical implementations of VLA core functionality, object grounding, and action planning. The hands-on lab exercises provide experience with implementing and validating complete VLA systems. This foundation enables the development of sophisticated Physical AI systems that can interact naturally with humans and their environment through a combination of seeing, understanding, and acting."})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var i=t(6540);const o={},a=i.createContext(o);function s(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);